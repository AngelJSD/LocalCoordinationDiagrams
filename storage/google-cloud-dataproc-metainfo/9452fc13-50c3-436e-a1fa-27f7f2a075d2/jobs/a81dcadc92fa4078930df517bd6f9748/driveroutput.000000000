19/10/29 15:38:12 INFO org.spark_project.jetty.util.log: Logging initialized @3932ms
19/10/29 15:38:12 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
19/10/29 15:38:12 INFO org.spark_project.jetty.server.Server: Started @4073ms
19/10/29 15:38:12 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@992d646{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/10/29 15:38:13 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
19/10/29 15:38:14 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-9c8d-m/10.128.0.43:8032
19/10/29 15:38:14 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-9c8d-m/10.128.0.43:10200
19/10/29 15:38:17 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1572307850607_0006
19/10/29 15:38:25 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1
19/10/29 15:55:13 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container killed by YARN for exceeding memory limits. 3.0 GB of 3 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
19/10/29 15:55:13 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 2 on cluster-9c8d-w-0.us-central1-f.c.lustrous-drake-255300.internal: Container killed by YARN for exceeding memory limits. 3.0 GB of 3 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
19/10/29 15:55:13 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 1.0 (TID 7, cluster-9c8d-w-0.us-central1-f.c.lustrous-drake-255300.internal, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 3.0 GB of 3 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
19/10/29 15:55:13 WARN org.apache.spark.network.server.TransportChannelHandler: Exception in connection from /10.128.0.44:39574
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1106)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:343)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
