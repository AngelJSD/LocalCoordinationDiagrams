+ run_with_logger --tag google-dataproc-startup
+ local tag=
+ local pid=827
+ [[ --tag == \-\-\t\a\g ]]
+ tag=google-dataproc-startup
+ shift 2
+ exec
++ logger -s -t 'google-dataproc-startup[827]'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=()
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=()
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + cd /tmp
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ INSTALL_GCS_CONNECTOR=1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ ENABLE_HDFS=1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ ENABLE_NFS_GCS_FILE_CACHE=1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ GCS_ADMIN=gcsadmin
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ GCS_CACHE_CLEANER_LOG_DIRECTORY=/var/log/hadoop-hdfs
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,DRFA
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,RFA
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ NUM_WORKERS=10
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ WORKERS=()
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ CORES_PER_MAP_TASK=1.0
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ CORES_PER_APP_MASTER=2.0
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ HDFS_DATA_DIRS_PERM=700
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ COMMON_JAVA_OPTS=("-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance")
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ DATAPROC_VERSION_KEY=1_3
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ declare -A DATAPROC_TO_GCS_CONNECTOR_VERSION
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ DATAPROC_TO_GCS_CONNECTOR_VERSION=(["1_0"]="1.6.10-hadoop2" ["1_1"]="1.6.10-hadoop2" ["1_2"]="1.6.10-hadoop2" ["1_3"]="hadoop2-1.9.17" ["1_4"]="hadoop2-1.9.17" ["1_5"]="hadoop2-1.9.17" ["2_0"]="hadoop3-1.9.17")
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ GCS_CONNECTOR_VERSION=hadoop2-1.9.17
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ GCS_CONNECTOR_JAR=https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-1.9.17.jar
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ /usr/share/google/get_metadata_value attributes/dataproc_gcs_connector_url
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ true
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ GCS_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ [[ ! -z '' ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ declare -A DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ DATAPROC_TO_BQ_CONNECTOR_VERSION=(["1_0"]="0.10.11-hadoop2" ["1_1"]="0.10.11-hadoop2" ["1_2"]="0.10.11-hadoop2")
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ BIGQUERY_CONNECTOR_VERSION=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ [[ -n '' ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ /usr/share/google/get_metadata_value attributes/dataproc_bq_connector_url
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ true
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ BIGQUERY_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ [[ ! -z '' ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ unset DATAPROC_VERSION_KEY GCS_CONNECTOR_VERSION BIGQUERY_CONNECTOR_VERSION DATAPROC_TO_GCS_CONNECTOR_VERSION DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ dpkg -s hive
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ HIVE_VERSION=2.3.5
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ dpkg -s spark-core
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ SPARK_VERSION=2.3.3
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ readonly APT_SENTINEL=apt.lastupdate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ APT_SENTINEL=apt.lastupdate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + source /usr/local/share/google/dataproc/bdutil/cluster_properties.sh
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + source /usr/local/share/google/dataproc/bdutil/components/startup-script-components.sh
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ readonly COMPONENTS_DIR=/var/lib/google/dataproc/components
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ COMPONENTS_DIR=/var/lib/google/dataproc/components
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + trap logstacktrace ERR
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + loginfo 'Starting Dataproc startup script'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + echo 'Starting Dataproc startup script'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: Starting Dataproc startup script
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ hostname -s
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + MY_HOSTNAME=cluster-5cb0-m
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ hostname -f
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ dnsdomainname
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + DOMAIN=us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ echo cluster-5cb0-m
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ sed -r 's/-([mw](-[0-9]*)?)$//'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + PREFIX=cluster-5cb0
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + merge_java_properties /tmp/cluster/properties/dataproc.properties /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + local dest=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/dataproc.properties ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + cat /tmp/cluster/properties/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: Merged /tmp/cluster/properties/dataproc.properties.
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ local property_name=dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_name=dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_value=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ echo ''
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ local property_value=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ echo ''
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + OPTIONAL_COMPONENTS_VALUE=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + is_version_at_least 1.3 1.4
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + local ver2=1.4
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + local log=/tmp/tmp.pmy510GqPE
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + err_code=1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.pmy510GqPE
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + rm -f /tmp/tmp.pmy510GqPE
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + DATAPROC_MASTER=cluster-5cb0-m
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master-additional
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + DATAPROC_MASTER_ADDITIONAL=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + MASTER_COUNT=1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + ((  1 > 1  ))
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + is_component_selected kafka-server
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + local component=kafka-server
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ local property_name=dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_name=dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_value=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ echo ''
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ local property_value=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ echo ''
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + local activated_components=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + [[ '' == *kafka-server* ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + COMPONENTS_TO_ACTIVATE=(${OPTIONAL_COMPONENTS_VALUE})
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + KERBEROS_ENABLED=false
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + is_component_selected kerberos
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + local component=kerberos
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ local property_name=dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_name=dataproc.components.activate
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_value=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ echo ''
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ local property_value=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ echo ''
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + local activated_components=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + [[ '' == *kerberos* ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ /usr/share/google/get_metadata_value ../project/project-id
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + PROJECT=lustrous-drake-255300
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ /usr/share/google/get_metadata_value attributes/dataproc-bucket
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + CONFIGBUCKET=dataproc-ed3c3d29-fb10-47bb-aca7-dcc358c68973-us-central1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ /usr/share/google/get_metadata_value attributes/dataproc-role
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + ROLE=Master
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-name
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + CLUSTER_NAME=cluster-5cb0
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-uuid
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + CLUSTER_UUID=2421c644-b38c-4e6e-971a-135cc992f432
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ /usr/share/google/get_metadata_value attributes/dataproc-worker-count
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + WORKER_COUNT=3
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + HBASE_CONF_DIR=/etc/hbase/conf
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + PIG_CONF_DIR=/etc/pig/conf
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + SPARK_CONF_DIR=/etc/spark/conf
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + ZOOKEEPER_CONF_DIR=/etc/zookeeper/conf
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + HADOOP_2_PORTS=(50010 50020 50070 50090)
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + ((  1 > 1  ))
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + HDFS_ROOT_URI=hdfs://cluster-5cb0-m
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + hostname=cluster-5cb0-m
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + for i in "${!MASTER_HOSTNAMES[@]}"
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + [[ cluster-5cb0-m == \c\l\u\s\t\e\r\-\5\c\b\0\-\m ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + MASTER_INDEX=0
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + break
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + ((  3 == 0  ))
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + ((  1 > 1  ))
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + PACKAGES_TO_UNINSTALL=(${DATAPROC_MASTER_HA_SERVICES} ${DATAPROC_WORKER_SERVICES})
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + SERVICES=(${DATAPROC_MASTER_SERVICES} ${DATAPROC_MASTER_EXCLUSIVE_SERVICES} ${DATAPROC_MASTER_STANDALONE_SERVICES})
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + loginfo 'Generating helper scripts'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + echo 'Generating helper scripts'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: Generating helper scripts
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + sudo -u hdfs hdfs namenode -genclusterid
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ cat /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ (( i=0 ))
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ (( i<1 ))
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ echo MASTER_HOSTNAME_0=cluster-5cb0-m
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ (( i++  ))
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ (( i<1 ))
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ cat /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ cat /usr/local/share/google/dataproc/bdutil/setup_master_nfs.sh /usr/local/share/google/dataproc/bdutil/setup_client_nfs.sh
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ cat /usr/local/share/google/dataproc/bdutil/configure_hadoop.sh /usr/local/share/google/dataproc/bdutil/configure_mysql.sh /usr/local/share/google/dataproc/bdutil/configure_hive.sh /usr/local/share/google/dataproc/bdutil/configure_hdfs.sh /usr/local/share/google/dataproc/bdutil/configure_connectors.sh /usr/local/share/google/dataproc/bdutil/configure_spark.sh /usr/local/share/google/dataproc/bdutil/configure_tez.sh /usr/local/share/google/dataproc/bdutil/configure_zookeeper.sh
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + cp -r /usr/local/share/google/dataproc/bdutil/conf/bq-mapred-template.xml /usr/local/share/google/dataproc/bdutil/conf/capacity-scheduler-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-template.xml /usr/local/share/google/dataproc/bdutil/conf/distcp-template.xml /usr/local/share/google/dataproc/bdutil/conf/gcs-core-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-template.xml /usr/local/share/google/dataproc/bdutil/conf/hive-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hive-template.xml /usr/local/share/google/dataproc/bdutil/conf/knox /usr/local/share/google/dataproc/bdutil/conf/mapred-template.xml /usr/local/share/google/dataproc/b
<13>Oct 13 23:16:28 google-dataproc-startup[827]: dutil/conf/yarn-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-template.xml /tmp
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + cp /usr/local/share/google/dataproc/bdutil/configure_mrv2_mem.py /tmp
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + chmod +x configure_mrv2_mem.py
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + loginfo 'Running helper scripts'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + echo 'Running helper scripts'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: Running helper scripts
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ get_dataproc_property dataproc.localssd.mount.enable
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ local property_name=dataproc.localssd.mount.enable
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.localssd.mount.enable
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_name=dataproc.localssd.mount.enable
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ grep '^dataproc.localssd.mount.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ local property_value=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: +++ echo ''
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ local property_value=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: ++ echo ''
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + MOUNT_DISKS_ENABLED=
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + [[ '' == \f\a\l\s\e ]]
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + DATAPROC_MOUNT_SERVICE_FILE=/usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + chmod +x /usr/local/share/google/dataproc/bdutil/mount_disks.sh
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + chmod 644 /usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + systemctl enable google-dataproc-disk-mount
<13>Oct 13 23:16:28 google-dataproc-startup[827]: Created symlink /etc/systemd/system/multi-user.target.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 23:16:28 google-dataproc-startup[827]: Created symlink /etc/systemd/system/hadoop-hdfs-namenode.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 23:16:28 google-dataproc-startup[827]: Created symlink /etc/systemd/system/hadoop-hdfs-datanode.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 23:16:28 google-dataproc-startup[827]: Created symlink /etc/systemd/system/hadoop-yarn-resourcemanager.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 23:16:28 google-dataproc-startup[827]: Created symlink /etc/systemd/system/hadoop-yarn-nodemanager.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 23:16:28 google-dataproc-startup[827]: + systemctl start google-dataproc-disk-mount
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + in_array nfs-kernel-server DATAPROC_MASTER_EXCLUSIVE_SERVICES
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + local value=nfs-kernel-server
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + local -n values=DATAPROC_MASTER_EXCLUSIVE_SERVICES
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + [[ !  hadoop-mapreduce-historyserver spark-history-server hive-webhcat-server jupyter knox proxy-agent zeppelin hadoop-yarn-timelineserver mariadb-server  =~  nfs-kernel-server  ]]
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + bash configuration_script.sh
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ INSTALL_GCS_CONNECTOR=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ ENABLE_HDFS=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ ENABLE_NFS_GCS_FILE_CACHE=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ GCS_ADMIN=gcsadmin
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ GCS_CACHE_CLEANER_LOG_DIRECTORY=/var/log/hadoop-hdfs
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,DRFA
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,RFA
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ NUM_WORKERS=10
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ WORKERS=()
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ CORES_PER_MAP_TASK=1.0
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ CORES_PER_APP_MASTER=2.0
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ HDFS_DATA_DIRS_PERM=700
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ COMMON_JAVA_OPTS=("-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance")
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ DATAPROC_VERSION_KEY=1_3
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ declare -A DATAPROC_TO_GCS_CONNECTOR_VERSION
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ DATAPROC_TO_GCS_CONNECTOR_VERSION=(["1_0"]="1.6.10-hadoop2" ["1_1"]="1.6.10-hadoop2" ["1_2"]="1.6.10-hadoop2" ["1_3"]="hadoop2-1.9.17" ["1_4"]="hadoop2-1.9.17" ["1_5"]="hadoop2-1.9.17" ["2_0"]="hadoop3-1.9.17")
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ GCS_CONNECTOR_VERSION=hadoop2-1.9.17
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ GCS_CONNECTOR_JAR=https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-1.9.17.jar
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ /usr/share/google/get_metadata_value attributes/dataproc_gcs_connector_url
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ true
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ GCS_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ [[ ! -z '' ]]
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ declare -A DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ DATAPROC_TO_BQ_CONNECTOR_VERSION=(["1_0"]="0.10.11-hadoop2" ["1_1"]="0.10.11-hadoop2" ["1_2"]="0.10.11-hadoop2")
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ BIGQUERY_CONNECTOR_VERSION=
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ [[ -n '' ]]
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ /usr/share/google/get_metadata_value attributes/dataproc_bq_connector_url
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ true
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ BIGQUERY_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ [[ ! -z '' ]]
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ unset DATAPROC_VERSION_KEY GCS_CONNECTOR_VERSION BIGQUERY_CONNECTOR_VERSION DATAPROC_TO_GCS_CONNECTOR_VERSION DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ dpkg -s hive
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ HIVE_VERSION=2.3.5
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ dpkg -s spark-core
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ SPARK_VERSION=2.3.3
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ CLUSTER_NAME=cluster-5cb0
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ CLUSTER_UUID=2421c644-b38c-4e6e-971a-135cc992f432
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ CONFIGBUCKET=dataproc-ed3c3d29-fb10-47bb-aca7-dcc358c68973-us-central1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ HDFS_ROOT_URI=hdfs://cluster-5cb0-m
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ MASTER_HOSTNAME_0=cluster-5cb0-m
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ MASTER_HOSTNAMES=(cluster-5cb0-m)
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ NUM_MASTERS=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ NUM_WORKERS=3
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ PREFIX=cluster-5cb0
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ PROJECT=lustrous-drake-255300
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ ROLE=Master
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ set +a
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ readonly APT_SENTINEL=apt.lastupdate
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ APT_SENTINEL=apt.lastupdate
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + set -e
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + loginfo 'Running configure_hadoop.sh'
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + echo 'Running configure_hadoop.sh'
<13>Oct 13 23:16:29 google-dataproc-startup[827]: Running configure_hadoop.sh
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + export HADOOP_TMP_DIR=/hadoop/tmp
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + HADOOP_TMP_DIR=/hadoop/tmp
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + mkdir -p /hadoop/tmp
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + export DEFAULT_NUM_MAPS=30
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + DEFAULT_NUM_MAPS=30
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + export DEFAULT_NUM_REDUCES=12
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + DEFAULT_NUM_REDUCES=12
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ grep -c processor /proc/cpuinfo
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + export NUM_CORES=2
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + NUM_CORES=2
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ python -c 'print int(2 //     1.0)'
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + export MAP_SLOTS=2
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + MAP_SLOTS=2
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ python -c 'print int(2 //     2.0)'
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + export REDUCE_SLOTS=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + REDUCE_SLOTS=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ free -m
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ awk '/^Mem:/{print $2}'
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + TOTAL_MEM=7483
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ python -c 'print int(7483 *     0.4)'
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + HADOOP_MR_MASTER_MEM_MB=2993
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + [[ -x configure_mrv2_mem.py ]]
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ mktemp /tmp/mrv2_XXX_tmp_env.sh
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + TEMP_ENV_FILE=/tmp/mrv2_WQE_tmp_env.sh
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + ./configure_mrv2_mem.py --output_file /tmp/mrv2_WQE_tmp_env.sh --total_memory 7483 --available_memory_ratio 0.8 --total_cores 2 --cores_per_map 1.0 --cores_per_reduce 2.0 --cores_per_app_master 2.0
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + source /tmp/mrv2_WQE_tmp_env.sh
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export YARN_MIN_MEM_MB=512
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ YARN_MIN_MEM_MB=512
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export YARN_MAX_MEM_MB=5632
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ YARN_MAX_MEM_MB=5632
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export NODEMANAGER_MEM_MB=5632
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ NODEMANAGER_MEM_MB=5632
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export APP_MASTER_MEM_MB=5632
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ APP_MASTER_MEM_MB=5632
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export MAP_MEM_MB=2560
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ MAP_MEM_MB=2560
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ CORES_PER_MAP_ROUNDED=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export REDUCE_MEM_MB=5632
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ REDUCE_MEM_MB=5632
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export CORES_PER_REDUCE_ROUNDED=2
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ CORES_PER_REDUCE_ROUNDED=2
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ export REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ python -c 'print int(7483 / 4)'
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + HADOOP_CLIENT_MEM_MB=1870
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + is_version_at_least 1.3 1.4
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + local ver2=1.4
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + local log=/tmp/tmp.UFpcbtF6oU
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + err_code=1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.UFpcbtF6oU
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + rm -f /tmp/tmp.UFpcbtF6oU
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + DATA_DIRS=($(get_data_dirs))
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ get_data_dirs
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ mount_points=($(find /mnt/[0-9]*/ -maxdepth 0 || true))
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Oct 13 23:16:29 google-dataproc-startup[827]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ true
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ local mount_points
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ ((  0  ))
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ echo /
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ return
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + mkdir -p /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + chgrp hadoop -L -R /hadoop /hadoop/tmp /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + chmod g+rwx -R /hadoop /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + chmod 777 -R /hadoop/tmp
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + export MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + export NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + YARN_ENV_FILE=/etc/hadoop/conf/yarn-env.sh
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + [[ -f /etc/hadoop/conf/yarn-env.sh ]]
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ get_dataproc_property simplified.scaling.enable
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ local property_name=simplified.scaling.enable
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ local property_name=simplified.scaling.enable
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ local property_value=
<13>Oct 13 23:16:29 google-dataproc-startup[827]: +++ echo ''
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ local property_value=
<13>Oct 13 23:16:29 google-dataproc-startup[827]: ++ echo ''
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + [[ '' == \t\r\u\e ]]
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + touch /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + chown root:hadoop /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + [[ 1 -gt 1 ]]
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + CORE_TEMPLATE=core-template.xml
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + YARN_TEMPLATE=yarn-template.xml
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file core-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:29 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file mapred-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file yarn-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file capacity-scheduler-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file distcp-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + is_version_at_least 1.3 1.4
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local ver2=1.4
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local log=/tmp/tmp.572P3BgiKA
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + err_code=1
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.572P3BgiKA
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + rm -f /tmp/tmp.572P3BgiKA
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value false --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + ZK_QUORUM=cluster-5cb0-m:2181,:2181,:2181
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + [[ 1 -gt 1 ]]
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + is_version_at_least 1.3 1.2
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local ver2=1.2
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local log=/tmp/tmp.geUun1jQuB
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.2
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + err_code=0
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.geUun1jQuB
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + rm -f /tmp/tmp.geUun1jQuB
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + return 0
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.file.close.num-committed-allowed --value 1 --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + is_version_at_least 1.3 1.3
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local ver2=1.3
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local log=/tmp/tmp.ETra4GfPbx
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + err_code=0
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.ETra4GfPbx
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + rm -f /tmp/tmp.ETra4GfPbx
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + return 0
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/core-site.xml --name hadoop.http.filter.initializers --value org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.webapp.cross-origin.enabled --value true --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.http-cross-origin.enabled --value true --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.enabled --value true --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.hostname --value cluster-5cb0-m --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.bind-host --value 0.0.0.0 --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.system-metrics-publisher.enabled --value true --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.generic-application-history.enabled --value true --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++ local property_name=am.primary_only
<13>Oct 13 23:16:30 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:16:30 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:30 google-dataproc-startup[827]: +++ local property_name=am.primary_only
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:30 google-dataproc-startup[827]: +++ local property_value=false
<13>Oct 13 23:16:30 google-dataproc-startup[827]: +++ echo false
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++ local property_value=false
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++ echo false
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++ /usr/share/google/get_metadata_value attributes/dataproc-datanode-enabled
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + DATAPROC_DATANODE_ENABLED=true
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + [[ '' == \t\r\u\e ]]
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + set -e -x
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + grep -lr bind-address /etc/mysql
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + xargs -n1 sed -i 's/^\(bind-address\)\s*=.*/\1 = 0.0.0.0/'
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + set -e -x
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + is_version_at_least 1.3 1.3
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local ver2=1.3
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local log=/tmp/tmp.wnsGfFTVWx
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + err_code=0
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.wnsGfFTVWx
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + rm -f /tmp/tmp.wnsGfFTVWx
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + return 0
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + is_version_at_least 1.3 1.4
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local ver2=1.4
<13>Oct 13 23:16:30 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + local log=/tmp/tmp.GfJVHL9xme
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + err_code=1
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.GfJVHL9xme
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + rm -f /tmp/tmp.GfJVHL9xme
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + [[ 1 -gt 1 ]]
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + METASTORE_URIS=thrift://cluster-5cb0-m:9083
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris --value thrift://cluster-5cb0-m:9083 --clobber
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + METADATA_STORE=jdbc:mysql://cluster-5cb0-m/metastore
<13>Oct 13 23:16:30 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name javax.jdo.option.ConnectionURL --value jdbc:mysql://cluster-5cb0-m/metastore --clobber
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + [[ 1 -gt 1 ]]
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + set -e
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + loginfo 'Running configure_hdfs.sh'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + echo 'Running configure_hdfs.sh'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: Running configure_hdfs.sh
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + HDFS_ADMIN=hdfs
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + export HDFS_NAME_DIR=/hadoop/dfs/name
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + HDFS_NAME_DIR=/hadoop/dfs/name
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + export HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + DATA_DIRS=($(get_data_dirs))
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ get_data_dirs
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ mount_points=($(find /mnt/[0-9]*/ -maxdepth 0 || true))
<13>Oct 13 23:16:31 google-dataproc-startup[827]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Oct 13 23:16:31 google-dataproc-startup[827]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Oct 13 23:16:31 google-dataproc-startup[827]: +++ true
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ local mount_points
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ ((  0  ))
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ echo /
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ return
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + mkdir -p /hadoop/dfs /hadoop/dfs/data
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + chown hdfs:hadoop -L -R /hadoop/dfs /hadoop/dfs/data
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + chmod 700 /hadoop/dfs/data
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ awk '/^Mem:/{print $2}'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ free -m
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + TOTAL_MEM=7483
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ python -c 'print int(7483 *     0.4 / 2)'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + NAMENODE_MEM_MB=1496
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SECONDARYNAMENODE_MEM_MB=1496
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + export HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + [[ 1 -gt 1 ]]
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + TEMPLATE=hdfs-template.xml
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file hdfs-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + ((  3 == 0  ))
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ get_java_property /tmp/cluster/properties/dataproc.properties simplified.scaling.enable
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ local property_file=/tmp/cluster/properties/dataproc.properties
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ local property_name=simplified.scaling.enable
<13>Oct 13 23:16:31 google-dataproc-startup[827]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: +++ tail -n 1
<13>Oct 13 23:16:31 google-dataproc-startup[827]: +++ cut -d = -f 2-
<13>Oct 13 23:16:31 google-dataproc-startup[827]: +++ grep '^simplified.scaling.enable=' /tmp/cluster/properties/dataproc.properties
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ local property_value=
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ echo ''
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + [[ '' == \t\r\u\e ]]
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + set -e
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + loginfo 'Running configure_connectors.sh'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + echo 'Running configure_connectors.sh'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: Running configure_connectors.sh
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + ((  1  ))
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + export GCS_METADATA_CACHE_TYPE=FILESYSTEM_BACKED
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + GCS_METADATA_CACHE_TYPE=FILESYSTEM_BACKED
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ get_nfs_mount_point
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ echo /hadoop_gcs_connector_metadata_cache
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + export GCS_FILE_CACHE_DIRECTORY=/hadoop_gcs_connector_metadata_cache
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + GCS_FILE_CACHE_DIRECTORY=/hadoop_gcs_connector_metadata_cache
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file gcs-core-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ hostname -s
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + [[ cluster-5cb0-m == \c\l\u\s\t\e\r\-\5\c\b\0\-\m ]]
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + [[ 1 -ne 0 ]]
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + setup_cache_cleaner
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + mkdir -p /usr/lib/hadoop/google
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local cleaner=/usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + make_cache_cleaner_script
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local gc_cleaner=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemCacheCleaner
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local etab=/var/lib/nfs/etab
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + chmod 755 /usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + make_cleaner_crontab /usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local cleaner=/usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file bq-mapred-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + set -e
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + set -o nounset
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + loginfo 'Running configure_spark.sh'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + echo 'Running configure_spark.sh'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: Running configure_spark.sh
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_EVENTLOG_DIR=hdfs://cluster-5cb0-m/user/spark/eventlog
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_TMPDIR=/hadoop/spark/tmp
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_WORKDIR=/hadoop/spark/work
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_LOG_DIR=/var/log/spark
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + mkdir -p /hadoop/spark/tmp /hadoop/spark/work /var/log/spark
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + chown spark:spark -R /hadoop/spark /var/log/spark
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + chmod 1777 -R /hadoop/spark /var/log/spark
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + is_version_at_least 2.3.3 2
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local ver1=2.3.3.0.0.0.0
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local ver2=2
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local log=/tmp/tmp.1tbpzXMbhv
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + dpkg --compare-versions 2.3.3.0.0.0.0 '>=' 2
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + err_code=0
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.1tbpzXMbhv
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + rm -f /tmp/tmp.1tbpzXMbhv
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + return 0
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + RPC_SIZE_KEY=spark.rpc.message.maxSize
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + NUM_INITIAL_EXECUTORS_KEY=spark.executor.instances
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_YARN_DIR=/usr/lib/spark/yarn
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARKR_LIB_DIR=/usr/lib/spark/R/lib
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + [[ -f /usr/lib/spark/R/lib/sparkr.zip ]]
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ free -m
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ awk '/^Mem:/{print $2}'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + TOTAL_MEM=7483
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ python -c 'print int(7483 * 0.15)'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_DAEMON_MEMORY=1122
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ python -c 'print int(7483 / 4)'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_DRIVER_MEM_MB=1870
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ python -c 'print int(1870 / 2)'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_DRIVER_MAX_RESULT_MB=935
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ head -1
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ find /tmp/mrv2_WQE_tmp_env.sh
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + YARN_MEMORY_ENV=/tmp/mrv2_WQE_tmp_env.sh
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + source /tmp/mrv2_WQE_tmp_env.sh
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export YARN_MIN_MEM_MB=512
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ YARN_MIN_MEM_MB=512
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export YARN_MAX_MEM_MB=5632
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ YARN_MAX_MEM_MB=5632
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export NODEMANAGER_MEM_MB=5632
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ NODEMANAGER_MEM_MB=5632
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export APP_MASTER_MEM_MB=5632
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ APP_MASTER_MEM_MB=5632
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export MAP_MEM_MB=2560
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ MAP_MEM_MB=2560
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ CORES_PER_MAP_ROUNDED=1
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export REDUCE_MEM_MB=5632
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ REDUCE_MEM_MB=5632
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export CORES_PER_REDUCE_ROUNDED=2
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ CORES_PER_REDUCE_ROUNDED=2
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ export REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ python
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ cat
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_EXECUTOR_MEMORY=2560
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ python -c 'print max(1,     2 / 2)'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_EXECUTOR_CORES=1
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ python -c 'print int(max(     2560 / 11, 384))'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_YARN_EXECUTOR_MEMORY_OVERHEAD=384
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + SPARK_EXECUTOR_MEMORY=2176
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + is_version_at_least 1.3 1.4
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local ver2=1.4
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local log=/tmp/tmp.XsyZfyBy28
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + err_code=1
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.XsyZfyBy28
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + rm -f /tmp/tmp.XsyZfyBy28
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + is_version_at_least 1.3 1.3
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local ver2=1.3
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local log=/tmp/tmp.gVPUqnvZ8D
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + err_code=0
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.gVPUqnvZ8D
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + rm -f /tmp/tmp.gVPUqnvZ8D
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + return 0
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + set -e
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + loginfo 'Running configure_tez.sh'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + echo 'Running configure_tez.sh'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: Running configure_tez.sh
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + readonly CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + is_version_at_least 1.3 1.3
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local ver2=1.3
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local log=/tmp/tmp.AzXUR4k3aE
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + err_code=0
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.AzXUR4k3aE
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + rm -f /tmp/tmp.AzXUR4k3aE
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + return 0
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ ls /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + configure_war /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local -r tez_war=/usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ mktemp -d
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local -r tmp_dir=/tmp/tmp.G9zwGrL9Xm
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + unzip -q /usr/lib/tez/tez-ui-0.9.2.war -d /tmp/tmp.G9zwGrL9Xm
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local -r tez_configs=/tmp/tmp.G9zwGrL9Xm/config/configs.env
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ cut -d ' ' -f 1
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ md5sum /tmp/tmp.G9zwGrL9Xm/config/configs.env
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + [[ 23fbfca8f7b8e142395c6bb4676427ae != \2\3\f\b\f\c\a\8\f\7\b\8\e\1\4\2\3\9\5\c\6\b\b\4\6\7\6\4\2\7\a\e ]]
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + [[ -f /tmp/tmp.G9zwGrL9Xm/config/configs.env ]]
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + sed -i 's#\(.*\)//timeline: "http://localhost:8188"\(.*\)#\1timeline: "http://cluster-5cb0-m:8188"\2#' /tmp/tmp.G9zwGrL9Xm/config/configs.env
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + sed -i 's#\(.*\)//rm: "http://localhost:8088"\(.*\)#\1rm: "http://cluster-5cb0-m:8088"\2#' /tmp/tmp.G9zwGrL9Xm/config/configs.env
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ local property_name=dataproc.components.activate
<13>Oct 13 23:16:31 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 23:16:31 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:31 google-dataproc-startup[827]: +++ local property_name=dataproc.components.activate
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:31 google-dataproc-startup[827]: +++ local property_value=
<13>Oct 13 23:16:31 google-dataproc-startup[827]: +++ echo ''
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ local property_value=
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ echo ''
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + local -r optional_components_value=
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + [[ '' == *\k\n\o\x* ]]
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + cd /tmp/tmp.G9zwGrL9Xm
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + zip -q /usr/lib/tez/tez-ui-0.9.2.war -r ./assets ./config ./fonts ./index.html ./META-INF ./WEB-INF
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + cd ..
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + rm -rf /tmp/tmp.G9zwGrL9Xm
<13>Oct 13 23:16:31 google-dataproc-startup[827]: ++ stat /usr/lib/tez/tez-common-0.9.2.jar --format=%Y
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + touch -d @1568819629 /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 23:16:31 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-names --value tez --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-on-disk-path.tez --value /usr/lib/tez/tez-ui-0.9.2.war --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-web-path.tez --value /tez-ui --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.history.logging.service.class --value org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.tez-ui.history-url.base --value http://cluster-5cb0-m:8188/tez-ui/ --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.am.node-blacklisting.enabled --value false --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + set -e
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + declare -r ZOOKEEPER_CONFIG=/etc/zookeeper/conf/zoo.cfg
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + declare -r ZOOKEEPER_DATA_DIR=/var/lib/zookeeper/
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i=0 ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i<1 ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo server.0=cluster-5cb0-m:2888:3888
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i++  ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i<1 ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo autopurge.purgeInterval=168
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ sed -e 's/.*-m-//'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ uname -n
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + declare -r MY_ID=cluster-5cb0-m
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo cluster-5cb0-m
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Populating initial cluster member list'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Populating initial cluster member list'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Populating initial cluster member list
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ get_dataproc_property dataproc.worker.custom.init.actions.mode
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local property_name=dataproc.worker.custom.init.actions.mode
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.worker.custom.init.actions.mode
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ local property_name=dataproc.worker.custom.init.actions.mode
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++++ grep '^dataproc.worker.custom.init.actions.mode=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ local property_value=
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ echo ''
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local property_value=
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ echo ''
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + WORKER_CUSTOM_INIT_ACTIONS_MODE=
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + WORKER_COUNT=3
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ get_dataproc_property simplified.scaling.enable
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local property_name=simplified.scaling.enable
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ local property_name=simplified.scaling.enable
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ local property_value=
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ echo ''
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local property_value=
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ echo ''
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ '' != \t\r\u\e ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + ((  3 == 0  ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ '' == \R\U\N\_\B\E\F\O\R\E\_\S\E\R\V\I\C\E\S ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + MEMBERSHIP_FILE=/etc/hadoop/conf/nodes_include
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i=0 ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i<3 ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo cluster-5cb0-w-0.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i++  ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i<3 ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo cluster-5cb0-w-1.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i++  ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i<3 ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo cluster-5cb0-w-2.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i++  ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i<3 ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merging user-specified cluster properties'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merging user-specified cluster properties'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merging user-specified cluster properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_xml_properties /tmp/cluster/properties/capacity-scheduler.xml /etc/hadoop/conf/capacity-scheduler.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/capacity-scheduler.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/hadoop/conf/capacity-scheduler.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/capacity-scheduler.xml ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file /tmp/cluster/properties/capacity-scheduler.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/capacity-scheduler.xml.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_xml_properties /tmp/cluster/properties/core.xml /etc/hadoop/conf/core-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/core.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/hadoop/conf/core-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/core.xml ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file /tmp/cluster/properties/core.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/core.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/core.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/core.xml.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_xml_properties /tmp/cluster/properties/distcp.xml /etc/hadoop/conf/distcp-default.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/distcp.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/hadoop/conf/distcp-default.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/distcp.xml ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file /tmp/cluster/properties/distcp.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/distcp.xml.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_xml_properties /tmp/cluster/properties/hdfs.xml /etc/hadoop/conf/hdfs-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/hdfs.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/hadoop/conf/hdfs-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/hdfs.xml ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /tmp/cluster/properties/hdfs.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/hdfs.xml.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_xml_properties /tmp/cluster/properties/mapred.xml /etc/hadoop/conf/mapred-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/mapred.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/hadoop/conf/mapred-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/mapred.xml ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file /tmp/cluster/properties/mapred.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/mapred.xml.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_xml_properties /tmp/cluster/properties/yarn.xml /etc/hadoop/conf/yarn-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/yarn.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/hadoop/conf/yarn-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/yarn.xml ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /tmp/cluster/properties/yarn.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/yarn.xml.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_xml_properties /tmp/cluster/properties/hive.xml /etc/hive/conf/hive-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/hive.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/hive/conf/hive-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/hive.xml ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/hive/conf/hive-site.xml --source_configuration_file /tmp/cluster/properties/hive.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/hive.xml.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_java_properties /tmp/cluster/properties/pig.properties /etc/pig/conf/pig.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/pig.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/pig/conf/pig.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/pig.properties ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + cat /tmp/cluster/properties/pig.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/pig.properties.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_java_properties /tmp/cluster/properties/spark.properties /etc/spark/conf/spark-defaults.conf
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/spark.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/spark/conf/spark-defaults.conf
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/spark.properties ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + cat /tmp/cluster/properties/spark.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/spark.properties.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_java_properties /tmp/cluster/properties/zookeeper.properties /etc/zookeeper/conf/zoo.cfg
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/zookeeper.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/zookeeper/conf/zoo.cfg
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/zookeeper.properties ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + cat /tmp/cluster/properties/zookeeper.properties
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/zookeeper.properties.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_sh_env_vars /tmp/cluster/properties/hadoop-env.sh /etc/hadoop/conf/hadoop-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/hadoop-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/hadoop/conf/hadoop-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/hadoop-env.sh ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + cat /tmp/cluster/properties/hadoop-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/hadoop-env.sh.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_sh_env_vars /tmp/cluster/properties/mapred-env.sh /etc/hadoop/conf/mapred-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/mapred-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/hadoop/conf/mapred-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/mapred-env.sh ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + cat /tmp/cluster/properties/mapred-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/mapred-env.sh.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_sh_env_vars /tmp/cluster/properties/spark-env.sh /etc/spark/conf/spark-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/spark-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/spark/conf/spark-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/spark-env.sh ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + cat /tmp/cluster/properties/spark-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/spark-env.sh.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_sh_env_vars /tmp/cluster/properties/yarn-env.sh /etc/hadoop/conf/yarn-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/yarn-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/hadoop/conf/yarn-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/yarn-env.sh ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + cat /tmp/cluster/properties/yarn-env.sh
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/yarn-env.sh.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ -f /etc/hbase/conf/hbase-site.xml ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ -f /etc/tez/conf/tez-site.xml ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + merge_xml_properties /tmp/cluster/properties/tez.xml /etc/tez/conf/tez-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local src=/tmp/cluster/properties/tez.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local dest=/etc/tez/conf/tez-site.xml
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ ! -f /tmp/cluster/properties/tez.xml ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + bdconfig merge_configurations --configuration_file /etc/tez/conf/tez-site.xml --source_configuration_file /tmp/cluster/properties/tez.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: Merged /tmp/cluster/properties/tez.xml.
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + ACTIVATABLE_COMPONENTS=(${DATAPROC_OPTIONAL_COMPONENTS})
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + DATAPROC_NON_DEBIAN_COMPONENTS=(${DATAPROC_NON_DEBIAN_COMPONENTS})
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + PACKAGES_TO_KEEP=("${SERVICES[@]}" ${DATAPROC_COMMON_PACKAGES})
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + DATAPROC_START_AFTER_HDFS_SERVICES=(${DATAPROC_START_AFTER_HDFS_SERVICES})
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + COMPONENTS_TO_ACTIVATE=($(intersection COMPONENTS_TO_ACTIVATE PACKAGES_TO_KEEP))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ intersection COMPONENTS_TO_ACTIVATE PACKAGES_TO_KEEP
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local -n values=COMPONENTS_TO_ACTIVATE
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local -n filter=PACKAGES_TO_KEEP
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ sort -u
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ printf '%s\n' hadoop-hdfs-namenode hadoop-yarn-resourcemanager hive-metastore hive-server2 zookeeper-server solr-server hadoop-mapreduce-historyserver spark-history-server hive-webhcat-server jupyter knox proxy-agent zeppelin hadoop-yarn-timelineserver mariadb-server hadoop-hdfs-secondarynamenode openjdk-8-jdk openjdk-8-dbg libjansi-java python-numpy libmysql-java hadoop-client hive pig spark-core spark-python spark-r autofs libhdfs0 libsnappy-dev libzstd-dev libatlas3-base libopenblas-base libapr1 vim git bash-completion spark-yarn-shuffle spark-datanucleus spark-extras hadoop-lzo python-setuptools anaconda druid kafka-server kerberos presto openssl tez hive-hcatalog
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ printf '%s\n'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ sort -u
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + NON_ACTIVATED_COMPONENTS=($(difference ACTIVATABLE_COMPONENTS COMPONENTS_TO_ACTIVATE))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ difference ACTIVATABLE_COMPONENTS COMPONENTS_TO_ACTIVATE
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local -n filter=COMPONENTS_TO_ACTIVATE
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ sort -u
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ sort -u
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ printf '%s\n'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ printf '%s\n' zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + PACKAGES_TO_UNINSTALL+=($(difference NON_ACTIVATED_COMPONENTS PACKAGES_TO_UNINSTALL))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ difference NON_ACTIVATED_COMPONENTS PACKAGES_TO_UNINSTALL
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local -n values=NON_ACTIVATED_COMPONENTS
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local -n filter=PACKAGES_TO_UNINSTALL
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ printf '%s\n' anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ sort -u
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ sort -u
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + NON_DEBIAN_COMPONENTS_TO_UNINSTALL=($(intersection PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ intersection PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local -n values=PACKAGES_TO_UNINSTALL
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local -n filter=DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ printf '%s\n' anaconda jupyter kerberos presto proxy-agent
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ sort -u
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ sort -u
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + DEBIAN_COMPONENTS_TO_UNINSTALL=($(difference PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ difference PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local -n values=PACKAGES_TO_UNINSTALL
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ local -n filter=DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 23:16:32 google-dataproc-startup[827]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ printf '%s\n' anaconda jupyter kerberos presto proxy-agent
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ sort -u
<13>Oct 13 23:16:32 google-dataproc-startup[827]: +++ sort -u
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + [[ false != \t\r\u\e ]]
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + DEBIAN_COMPONENTS_TO_UNINSTALL+=('krb5-kpropd' 'krb5-kdc' 'krb5-admin-server' 'krb5-user' 'krb5-config' 'xinetd')
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + uninstall_packages
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + run_with_retries set_selections
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local retry_backoff
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + cmd=("$@")
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local -a cmd
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + loginfo 'About to run '\''set_selections'\'' with retries...'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + echo 'About to run '\''set_selections'\'' with retries...'
<13>Oct 13 23:16:32 google-dataproc-startup[827]: About to run 'set_selections' with retries...
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + local update_succeeded=0
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i = 0 ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + (( i < 12 ))
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + set_selections
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + cat
<13>Oct 13 23:16:32 google-dataproc-startup[827]: + debconf-set-selections
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + update_succeeded=1
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + break
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + ((  1  ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + loginfo 'Uninstalling un-needed daemons'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Uninstalling un-needed daemons'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Uninstalling un-needed daemons
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag uninstall bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1420
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true] as pid 1420'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true] as pid 1420
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag uninstall-component-anaconda uninstall_component anaconda
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1421
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [uninstall_component anaconda] as pid 1421'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [uninstall_component anaconda] as pid 1421
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag uninstall-component-jupyter uninstall_component jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1422
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [uninstall_component jupyter] as pid 1422'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [uninstall_component jupyter] as pid 1422
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag uninstall-component-kerberos uninstall_component kerberos
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1423
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [uninstall_component kerberos] as pid 1423'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [uninstall_component kerberos] as pid 1423
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag uninstall-component-presto uninstall_component presto
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1424
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [uninstall_component presto] as pid 1424'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [uninstall_component presto] as pid 1424
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag uninstall-component-proxy-agent uninstall_component proxy-agent
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1425
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [uninstall_component proxy-agent] as pid 1425'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [uninstall_component proxy-agent] as pid 1425
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + is_version_at_least 1.3 1.3
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local ver2=1.3
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ mktemp
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag uninstall-component-presto uninstall_component presto
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1424
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=uninstall-component-presto
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag uninstall-component-jupyter uninstall_component jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1422
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=uninstall-component-jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag uninstall-component-anaconda uninstall_component anaconda
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1421
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=uninstall-component-anaconda
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'uninstall-component-presto[1424]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag uninstall-component-proxy-agent uninstall_component proxy-agent
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'uninstall-component-jupyter[1422]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1425
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=uninstall-component-proxy-agent
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag uninstall-component-kerberos uninstall_component kerberos
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'uninstall-component-anaconda[1421]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1423
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=uninstall-component-kerberos
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local log=/tmp/tmp.NXs2c0UTaj
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + uninstall_component presto
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + local component=presto
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + set -exo pipefail
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'uninstall-component-kerberos[1423]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag uninstall bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1420
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=uninstall
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + err_code=0
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + grep -C 10 -i warning /tmp/tmp.NXs2c0UTaj
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: + uninstall_component jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: + uninstall_component anaconda
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: + local component=anaconda
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: + set -exo pipefail
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: + local component=jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'uninstall-component-proxy-agent[1425]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/presto.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ readonly HTTP_PORT=8060
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ HTTP_PORT=8060
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ readonly PRESTO_BIN_PATH=/usr/bin/presto
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ PRESTO_BIN_PATH=/usr/bin/presto
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'uninstall[1420]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ readonly PRESTO_CLI_PATH=/opt/presto-cli
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ PRESTO_CLI_PATH=/opt/presto-cli
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ readonly PRESTO_DATA_DIRECTORY=/var/presto/data
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ PRESTO_DATA_DIRECTORY=/var/presto/data
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ readonly PRESTO_INSTALL_DIRECTORY=/opt/presto-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ PRESTO_INSTALL_DIRECTORY=/opt/presto-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ readonly INIT_SCRIPT=/usr/lib/systemd/system/presto.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ INIT_SCRIPT=/usr/lib/systemd/system/presto.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ readonly PRESTO_VERSION=0.215
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: ++ PRESTO_VERSION=0.215
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + rm -Rf /opt/presto-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + rm -f /tmp/tmp.NXs2c0UTaj
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: + set -euxo pipefail
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/jupyter.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ export WHEEL_DIR=/opt/dataproc/jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: + uninstall_component proxy-agent
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: + local component=proxy-agent
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: + set -exo pipefail
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ WHEEL_DIR=/opt/dataproc/jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ export JGSCM_WHEEL=/opt/dataproc/jupyter/jgscm
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ JGSCM_WHEEL=/opt/dataproc/jupyter/jgscm
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ export JGSCM_REQUIREMENTS=/opt/dataproc/jupyter/jgscm.requirements
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ JGSCM_REQUIREMENTS=/opt/dataproc/jupyter/jgscm.requirements
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ export JUPYTER_ETC_DIR=/etc/jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ JUPYTER_ETC_DIR=/etc/jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ export JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ export JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ export PYSPARK_KERNELSPEC=/opt/conda/anaconda/share/jupyter/kernels/pyspark/kernel.json
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ PYSPARK_KERNELSPEC=/opt/conda/anaconda/share/jupyter/kernels/pyspark/kernel.json
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ export JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ export JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: ++ JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/anaconda.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: + rm -Rf /etc/jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: ++ export ANACONDA_INSTALL_PATH=/opt/conda/anaconda
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: ++ ANACONDA_INSTALL_PATH=/opt/conda/anaconda
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: ++ export ANACONDA_BIN_DIR=/opt/conda/anaconda/bin
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: ++ ANACONDA_BIN_DIR=/opt/conda/anaconda/bin
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: ++ export PATH=/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: ++ PATH=/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-anaconda[1421]: + rm -Rf /opt/conda/anaconda
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/proxy-agent.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: ++ readonly PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: ++ PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: ++ readonly PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: ++ PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: ++ readonly PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: ++ PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: + rm -f /usr/bin/proxy-forwarding-agent
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall[1420]: + bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-kerberos[1423]: + uninstall_component kerberos
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-kerberos[1423]: + local component=kerberos
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-kerberos[1423]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-kerberos[1423]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-kerberos[1423]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-kerberos[1423]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-kerberos[1423]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-kerberos[1423]: + set -euxo pipefail
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-jupyter[1422]: + rm -Rf /opt/dataproc/jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + return 0
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-proxy-agent[1425]: + rm -f /usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ get_dataproc_property dataproc.monitoring.stackdriver.enable
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ local property_name=dataproc.monitoring.stackdriver.enable
<13>Oct 13 23:16:33 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.monitoring.stackdriver.enable
<13>Oct 13 23:16:33 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:33 google-dataproc-startup[827]: +++ local property_name=dataproc.monitoring.stackdriver.enable
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++++ grep '^dataproc.monitoring.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:33 google-dataproc-startup[827]: +++ local property_value=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: +++ echo false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ local property_value=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ echo false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + STACKDRIVER_MONITORING_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + loginfo 'Stackdriver monitoring disabled.'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Stackdriver monitoring disabled.'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Stackdriver monitoring disabled.
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + loginfo 'Replace dataproc plugin instance_name label with gce instance name.'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Replace dataproc plugin instance_name label with gce instance name.'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Replace dataproc plugin instance_name label with gce instance name.
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for PLUGIN_FILE in /opt/stackdriver/collectd/etc/collectd.d/dataproc*
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ hostname
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + sed -i 's/"label:instance_name".*$/"label:instance_name" "cluster-5cb0-m"/g' /opt/stackdriver/collectd/etc/collectd.d/dataproc_collectd_default-20170324-133642.conf
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + chmod +x /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + rm -Rf /var/presto/data
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + rm -f /usr/bin/presto
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + loginfo 'Running verify_setup.sh'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Running verify_setup.sh'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Running verify_setup.sh
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + rm -f /opt/presto-cli
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.warehouse.dir
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 uninstall-component-presto[1424]: + rm -f /usr/lib/systemd/system/presto.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + hive_warehouse_dir=None
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ None == \g\s\:\/\/* ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + loginfo 'Starting services'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Starting services'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Starting services
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array hadoop-hdfs-namenode ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=hadoop-hdfs-namenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-hdfs-namenode  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + case "${SERVICE}" in
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag setup-hadoop-hdfs-namenode setup_service hadoop-hdfs-namenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1482
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [setup_service hadoop-hdfs-namenode] as pid 1482'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [setup_service hadoop-hdfs-namenode] as pid 1482
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array hadoop-yarn-resourcemanager ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=hadoop-yarn-resourcemanager
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-yarn-resourcemanager  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + case "${SERVICE}" in
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1483
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [setup_service hadoop-yarn-resourcemanager] as pid 1483'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [setup_service hadoop-yarn-resourcemanager] as pid 1483
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array hive-metastore ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=hive-metastore
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-metastore  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + case "${SERVICE}" in
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag setup-hive-metastore setup_service hive-metastore
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1484
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [setup_service hive-metastore] as pid 1484'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [setup_service hive-metastore] as pid 1484
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array hive-server2 ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=hive-server2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-server2  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + case "${SERVICE}" in
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag setup-hive-server2 setup_service hive-server2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1485
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [setup_service hive-server2] as pid 1485'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [setup_service hive-server2] as pid 1485
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array zookeeper-server ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=zookeeper-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  zookeeper-server  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + continue
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array solr-server ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=solr-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  solr-server  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + continue
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array hadoop-mapreduce-historyserver ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=hadoop-mapreduce-historyserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-mapreduce-historyserver  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + case "${SERVICE}" in
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1486
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [setup_service hadoop-mapreduce-historyserver] as pid 1486'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [setup_service hadoop-mapreduce-historyserver] as pid 1486
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array spark-history-server ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=spark-history-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  spark-history-server  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + case "${SERVICE}" in
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag setup-spark-history-server setup_service spark-history-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1487
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [setup_service spark-history-server] as pid 1487'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [setup_service spark-history-server] as pid 1487
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array hive-webhcat-server ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=hive-webhcat-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-webhcat-server  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + continue
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array jupyter ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=jupyter
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  jupyter  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + continue
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array knox ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=knox
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  knox  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + continue
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array proxy-agent ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=proxy-agent
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  proxy-agent  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + continue
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array zeppelin ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=zeppelin
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  zeppelin  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + continue
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array hadoop-yarn-timelineserver ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=hadoop-yarn-timelineserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1483
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=setup-hadoop-yarn-resourcemanager
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-yarn-timelineserver  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + case "${SERVICE}" in
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag setup-hive-metastore setup_service hive-metastore
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1489
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag setup-spark-history-server setup_service spark-history-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1487
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1486
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag setup-hive-server2 setup_service hive-server2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1484
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1485
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=setup-hive-server2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=setup-spark-history-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=setup-hadoop-mapreduce-historyserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=setup-hive-metastore
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag setup-hadoop-hdfs-namenode setup_service hadoop-hdfs-namenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [setup_service hadoop-yarn-timelineserver] as pid 1489'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [setup_service hadoop-yarn-timelineserver] as pid 1489
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array mariadb-server ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=mariadb-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  mariadb-server  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + case "${SERVICE}" in
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag setup-mariadb setup_service mariadb
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1494
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [setup_service mariadb] as pid 1494'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [setup_service mariadb] as pid 1494
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + in_array hadoop-hdfs-secondarynamenode ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local value=hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-hdfs-secondarynamenode  ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + return 1
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + case "${SERVICE}" in
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_in_background --tag setup-hadoop-hdfs-secondarynamenode setup_service hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'setup-hadoop-yarn-resourcemanager[1483]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + PID=1495
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1482
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=setup-hadoop-hdfs-namenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'setup-hive-server2[1485]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'setup-hive-metastore[1484]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + setup_service hadoop-yarn-resourcemanager
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + export MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + local service=hadoop-yarn-resourcemanager
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + enable_service hadoop-yarn-resourcemanager
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + local service=hadoop-yarn-resourcemanager
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + local unit=hadoop-yarn-resourcemanager.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + run_with_retries systemctl enable hadoop-yarn-resourcemanager.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + local retry_backoff
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + cmd=("$@")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + local -a cmd
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + loginfo 'About to run '\''systemctl enable hadoop-yarn-resourcemanager.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + echo 'About to run '\''systemctl enable hadoop-yarn-resourcemanager.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: About to run 'systemctl enable hadoop-yarn-resourcemanager.service' with retries...
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + local update_succeeded=0
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + (( i = 0 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + (( i < 12 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: + systemctl enable hadoop-yarn-resourcemanager.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: hadoop-yarn-resourcemanager.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-resourcemanager[1483]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-resourcemanager
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag setup-hadoop-hdfs-secondarynamenode setup_service hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1495
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=setup-hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag setup-mariadb setup_service mariadb
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1494
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=setup-mariadb
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'setup-spark-history-server[1487]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + run_with_logger --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + local pid=1489
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + tag=setup-hadoop-yarn-timelineserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'setup-hadoop-hdfs-namenode[1482]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'setup-hadoop-mapreduce-historyserver[1486]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + setup_service hive-server2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + export MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + local service=hive-server2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + [[ hive-server2 == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + enable_service hive-server2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + local service=hive-server2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + local unit=hive-server2.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + run_with_retries systemctl enable hive-server2.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + local retry_backoff
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + cmd=("$@")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + local -a cmd
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + loginfo 'About to run '\''systemctl enable hive-server2.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + echo 'About to run '\''systemctl enable hive-server2.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: About to run 'systemctl enable hive-server2.service' with retries...
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + local update_succeeded=0
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + (( i = 0 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + (( i < 12 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: + systemctl enable hive-server2.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: hive-server2.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-server2[1485]: Executing: /lib/systemd/systemd-sysv-install enable hive-server2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Started background process [setup_service hadoop-hdfs-secondarynamenode] as pid 1495'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Started background process [setup_service hadoop-hdfs-secondarynamenode] as pid 1495
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + loginfo 'Configuring optional components'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: + echo 'Configuring optional components'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: Configuring optional components
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'setup-hadoop-hdfs-secondarynamenode[1495]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + setup_service hive-metastore
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + export MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + local service=hive-metastore
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + [[ hive-metastore == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + wait_for_port cluster-5cb0-m 3306
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + local -r host=cluster-5cb0-m
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + local -r port=3306
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + loginfo 'Waiting for service to come up on host=cluster-5cb0-m port=3306.'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + echo 'Waiting for service to come up on host=cluster-5cb0-m port=3306.'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: Waiting for service to come up on host=cluster-5cb0-m port=3306.
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + retry_with_constant_backoff nc -v -z -w 0 cluster-5cb0-m 3306
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + local max_retry=300
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + cmd=("$@")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + local -a cmd
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + local update_succeeded=0
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: ++ seq 1 300
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + setup_service spark-history-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + export MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + local service=spark-history-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + [[ spark-history-server == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + enable_service spark-history-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + local service=spark-history-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + local unit=spark-history-server.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + run_with_retries systemctl enable spark-history-server.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + local retry_backoff
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + cmd=("$@")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + local -a cmd
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + loginfo 'About to run '\''systemctl enable spark-history-server.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + echo 'About to run '\''systemctl enable spark-history-server.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: About to run 'systemctl enable spark-history-server.service' with retries...
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + local update_succeeded=0
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + (( i = 0 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + (( i < 12 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: + systemctl enable spark-history-server.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: spark-history-server.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-spark-history-server[1487]: Executing: /lib/systemd/systemd-sysv-install enable spark-history-server
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'setup-mariadb[1494]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + setup_service hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + export MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + local service=hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + enable_service hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + local service=hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + local unit=hadoop-hdfs-secondarynamenode.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + run_with_retries systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + local retry_backoff
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + cmd=("$@")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + local -a cmd
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + loginfo 'About to run '\''systemctl enable hadoop-hdfs-secondarynamenode.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + echo 'About to run '\''systemctl enable hadoop-hdfs-secondarynamenode.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: About to run 'systemctl enable hadoop-hdfs-secondarynamenode.service' with retries...
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + local update_succeeded=0
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + (( i = 0 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + (( i < 12 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: + systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: hadoop-hdfs-secondarynamenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-secondarynamenode[1495]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + setup_service hadoop-hdfs-namenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + export MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + local service=hadoop-hdfs-namenode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + case "${MASTER_INDEX?}" in
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + loginfo 'Formatting NameNode'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + echo 'Formatting NameNode'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: Formatting NameNode
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + run_with_retries su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + local retry_backoff
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + cmd=("$@")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + local -a cmd
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + loginfo 'About to run '\''su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + echo 'About to run '\''su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: About to run 'su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive' with retries...
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + local update_succeeded=0
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + (( i = 0 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + (( i < 12 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-hdfs-namenode[1482]: + su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ logger -s -t 'setup-hadoop-yarn-timelineserver[1489]'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 3306
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + setup_service hadoop-mapreduce-historyserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + export MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + local service=hadoop-mapreduce-historyserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + [[ hadoop-mapreduce-historyserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + enable_service hadoop-mapreduce-historyserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + local service=hadoop-mapreduce-historyserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + local unit=hadoop-mapreduce-historyserver.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + run_with_retries systemctl enable hadoop-mapreduce-historyserver.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + local retry_backoff
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + cmd=("$@")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + local -a cmd
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + loginfo 'About to run '\''systemctl enable hadoop-mapreduce-historyserver.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + echo 'About to run '\''systemctl enable hadoop-mapreduce-historyserver.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: About to run 'systemctl enable hadoop-mapreduce-historyserver.service' with retries...
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + local update_succeeded=0
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + (( i = 0 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + (( i < 12 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: + systemctl enable hadoop-mapreduce-historyserver.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: hadoop-mapreduce-historyserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-mapreduce-historyserver[1486]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-mapreduce-historyserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ get_dataproc_property dataproc.conscrypt.provider.enable
<13>Oct 13 23:16:33 google-dataproc-startup[827]: ++ local property_name=dataproc.conscrypt.provider.enable
<13>Oct 13 23:16:33 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.conscrypt.provider.enable
<13>Oct 13 23:16:33 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:33 google-dataproc-startup[827]: +++ local property_name=dataproc.conscrypt.provider.enable
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + setup_service mariadb
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + export MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + local service=mariadb
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + [[ mariadb == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + enable_service mariadb
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + local service=mariadb
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + local unit=mariadb.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + run_with_retries systemctl enable mariadb.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + local retry_backoff
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + cmd=("$@")
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + local -a cmd
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + loginfo 'About to run '\''systemctl enable mariadb.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + echo 'About to run '\''systemctl enable mariadb.service'\'' with retries...'
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: About to run 'systemctl enable mariadb.service' with retries...
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + local update_succeeded=0
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + (( i = 0 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + (( i < 12 ))
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + systemctl enable mariadb.service
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + setup_service hadoop-yarn-timelineserver
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + KERBEROS_ENABLED=false
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:16:33 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + export MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + local service=hadoop-yarn-timelineserver
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 1.'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 1.'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 1.
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + enable_service hadoop-yarn-timelineserver
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + local service=hadoop-yarn-timelineserver
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + local unit=hadoop-yarn-timelineserver.service
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + run_with_retries systemctl enable hadoop-yarn-timelineserver.service
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + local retry_backoff
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + cmd=("$@")
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + local -a cmd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + loginfo 'About to run '\''systemctl enable hadoop-yarn-timelineserver.service'\'' with retries...'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + echo 'About to run '\''systemctl enable hadoop-yarn-timelineserver.service'\'' with retries...'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: About to run 'systemctl enable hadoop-yarn-timelineserver.service' with retries...
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + local update_succeeded=0
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + (( i = 0 ))
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + (( i < 12 ))
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: + systemctl enable hadoop-yarn-timelineserver.service
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: hadoop-yarn-timelineserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-hadoop-yarn-timelineserver[1489]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-timelineserver
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: Created symlink /etc/systemd/system/mysql.service → /lib/systemd/system/mariadb.service.
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: Created symlink /etc/systemd/system/mysqld.service → /lib/systemd/system/mariadb.service.
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: Created symlink /etc/systemd/system/multi-user.target.wants/mariadb.service → /lib/systemd/system/mariadb.service.
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ grep '^dataproc.conscrypt.provider.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ local property_value=true
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ echo true
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ local property_value=true
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ echo true
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + CONSCRYPT_ENABLED=true
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + [[ true == \t\r\u\e ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt.jar /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/libconscrypt.jar
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni.so /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/libconscrypt_openjdk_jni.so
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + cp /usr/local/share/google/dataproc/java.security.conscrypt /etc/java-8-openjdk/security/java.security
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ get_dataproc_property dataproc.logging.stackdriver.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ local property_name=dataproc.logging.stackdriver.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ local property_name=dataproc.logging.stackdriver.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ tail -n 1
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ grep '^dataproc.logging.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ local property_value=
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ echo ''
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ local property_value=
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ echo ''
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + STACKDRIVER_LOGGING_ENABLED=
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + [[ '' == \f\a\l\s\e ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + loginfo 'Stackdriver enabled; enabling google-fluentd.'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + echo 'Stackdriver enabled; enabling google-fluentd.'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: Stackdriver enabled; enabling google-fluentd.
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + source /usr/local/share/google/dataproc/bdutil/configure_fluentd.sh
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ set -e
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ set -u
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ loginfo 'Running configure_fluentd.sh'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ echo 'Running configure_fluentd.sh'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: Running configure_fluentd.sh
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ FLUENTD_BASE_DIR=/etc/google-fluentd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ FLUENTD_CONF_DIR=/etc/google-fluentd/config.d
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ FLUENTD_PLUGIN_DIR=/etc/google-fluentd/plugin
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ cp /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/monitoring.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/out_google_cloud.rb /etc/google-fluentd/plugin
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + update_succeeded=1
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + break
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: + ((  1  ))
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-mariadb[1494]: ++ systemctl show mariadb.service -p Restart,RemainAfterExit
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ get_dataproc_property dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ local property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ local property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++++ cut -d = -f 2-
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++++ grep '^dataproc.logging.stackdriver.job.driver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++++ tail -n 1
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ local property_value=
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ echo ''
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ local property_value=
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ echo ''
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ JOB_DRIVER_LOGGING_ENABLED=
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ [[ '' == \t\r\u\e ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ get_dataproc_property dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ local property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ local property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++++ cut -d = -f 2-
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++++ grep '^dataproc.logging.stackdriver.job.yarn.container.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++++ tail -n 1
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ local property_value=
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++++ echo ''
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ local property_value=
<13>Oct 13 23:16:34 google-dataproc-startup[827]: +++ echo ''
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ CONTAINER_LOGGING_ENABLED=
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ [[ '' == \t\r\u\e ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + run_in_background --tag setup-google-fluentd setup_service google-fluentd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + PID=1600
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + echo 'Started background process [setup_service google-fluentd] as pid 1600'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: Started background process [setup_service google-fluentd] as pid 1600
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + wait_on_async_processes
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + loginfo 'Waiting on async proccesses'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + echo 'Waiting on async proccesses'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: Waiting on async proccesses
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + (( i = 0 ))
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + pid=1600
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + cmd='setup_service google-fluentd'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1600 cmd=[setup_service google-fluentd]'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + echo 'Waiting on pid=1600 cmd=[setup_service google-fluentd]'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: Waiting on pid=1600 cmd=[setup_service google-fluentd]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + wait 1600
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + run_with_logger --tag setup-google-fluentd setup_service google-fluentd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + local tag=
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + local pid=1600
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + tag=setup-google-fluentd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + shift 2
<13>Oct 13 23:16:34 google-dataproc-startup[827]: + exec
<13>Oct 13 23:16:34 google-dataproc-startup[827]: ++ logger -s -t 'setup-google-fluentd[1600]'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + setup_service google-fluentd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + KERBEROS_ENABLED=false
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + export MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + MY_FULL_HOSTNAME=cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + local service=google-fluentd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + [[ google-fluentd == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + enable_service google-fluentd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + local service=google-fluentd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + local unit=google-fluentd.service
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + run_with_retries systemctl enable google-fluentd.service
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + local retry_backoff
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + cmd=("$@")
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + local -a cmd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + loginfo 'About to run '\''systemctl enable google-fluentd.service'\'' with retries...'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + echo 'About to run '\''systemctl enable google-fluentd.service'\'' with retries...'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: About to run 'systemctl enable google-fluentd.service' with retries...
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + local update_succeeded=0
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + (( i = 0 ))
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + (( i < 12 ))
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: + systemctl enable google-fluentd.service
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: google-fluentd.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:33 setup-google-fluentd[1600]: Executing: /lib/systemd/systemd-sysv-install enable google-fluentd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 3306
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 2.'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 2.'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 2.
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + local 'props=Restart=on-abort
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: RemainAfterExit=no'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + [[ Restart=on-abort
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + in_array mariadb DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + local value=mariadb
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  mariadb  ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + return 1
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + [[ mariadb == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + run_with_retries systemctl start mariadb
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + local retry_backoff
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + cmd=("$@")
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + local -a cmd
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + loginfo 'About to run '\''systemctl start mariadb'\'' with retries...'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + echo 'About to run '\''systemctl start mariadb'\'' with retries...'
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: About to run 'systemctl start mariadb' with retries...
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + local update_succeeded=0
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + (( i = 0 ))
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + (( i < 12 ))
<13>Oct 13 23:16:34 google-dataproc-startup[827]: <13>Oct 13 23:16:34 setup-mariadb[1494]: + systemctl start mariadb
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 3306
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 3.'
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 3.'
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 3.
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hadoop-hdfs-secondarynamenode[1495]: + update_succeeded=1
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hadoop-hdfs-secondarynamenode[1495]: + break
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hadoop-hdfs-secondarynamenode[1495]: + ((  1  ))
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hadoop-hdfs-secondarynamenode[1495]: ++ systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hive-server2[1485]: + update_succeeded=1
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hive-server2[1485]: + break
<13>Oct 13 23:16:35 google-dataproc-startup[827]: <13>Oct 13 23:16:35 setup-hive-server2[1485]: + ((  1  ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: ++ systemctl show hive-server2.service -p Restart,RemainAfterExit
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 uninstall[1420]: Reading package lists...
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + update_succeeded=1
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + break
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + ((  1  ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: ++ systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + update_succeeded=1
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + break
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + ((  1  ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: ++ systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + update_succeeded=1
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + break
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + ((  1  ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: ++ systemctl show spark-history-server.service -p Restart,RemainAfterExit
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 uninstall[1420]: Building dependency tree...
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 uninstall[1420]: Reading state information...
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 3306
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 4.'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 4.'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 4.
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + update_succeeded=1
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + break
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + ((  1  ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + local 'props=Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: RemainAfterExit=no'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + local drop_in_dir=/etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + mkdir /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: ++ systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + in_array hadoop-hdfs-secondarynamenode DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + local value=hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-hdfs-secondarynamenode  ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + return 1
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + run_with_retries systemctl start hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + local retry_backoff
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + cmd=("$@")
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + local -a cmd
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + loginfo 'About to run '\''systemctl start hadoop-hdfs-secondarynamenode'\'' with retries...'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + echo 'About to run '\''systemctl start hadoop-hdfs-secondarynamenode'\'' with retries...'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: About to run 'systemctl start hadoop-hdfs-secondarynamenode' with retries...
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + local update_succeeded=0
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + (( i = 0 ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + (( i < 12 ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-hdfs-secondarynamenode[1495]: + systemctl start hadoop-hdfs-secondarynamenode
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + update_succeeded=1
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + break
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + ((  1  ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + local 'props=Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: RemainAfterExit=no'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + local drop_in_dir=/etc/systemd/system/hive-server2.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + mkdir /etc/systemd/system/hive-server2.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: ++ systemctl show google-fluentd.service -p Restart,RemainAfterExit
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-server2.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + in_array hive-server2 DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + local value=hive-server2
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hive-server2  ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + return 1
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + [[ hive-server2 == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hive-server2[1485]: + return
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + local 'props=Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: RemainAfterExit=no'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + local drop_in_dir=/etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + mkdir /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + local 'props=Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: RemainAfterExit=no'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + local drop_in_dir=/etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + mkdir /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + local 'props=Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: RemainAfterExit=no'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + local drop_in_dir=/etc/systemd/system/spark-history-server.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + mkdir /etc/systemd/system/spark-history-server.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + local 'props=Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: RemainAfterExit=no'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + local drop_in_dir=/etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + mkdir /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + in_array hadoop-yarn-timelineserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + local value=hadoop-yarn-timelineserver
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-yarn-timelineserver  ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + return 1
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + run_with_retries systemctl start hadoop-yarn-timelineserver
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + local retry_backoff
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + cmd=("$@")
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + local -a cmd
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + loginfo 'About to run '\''systemctl start hadoop-yarn-timelineserver'\'' with retries...'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + echo 'About to run '\''systemctl start hadoop-yarn-timelineserver'\'' with retries...'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: About to run 'systemctl start hadoop-yarn-timelineserver' with retries...
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + local update_succeeded=0
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + (( i = 0 ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + (( i < 12 ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: + systemctl start hadoop-yarn-timelineserver
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + local 'props=Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: RemainAfterExit=yes'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + [[ Restart=no
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + in_array google-fluentd DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + local value=google-fluentd
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  google-fluentd  ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + return 1
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + [[ google-fluentd == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + run_with_retries systemctl start google-fluentd
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + local retry_backoff
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + cmd=("$@")
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + local -a cmd
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + loginfo 'About to run '\''systemctl start google-fluentd'\'' with retries...'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + echo 'About to run '\''systemctl start google-fluentd'\'' with retries...'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: About to run 'systemctl start google-fluentd' with retries...
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + local update_succeeded=0
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + (( i = 0 ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + (( i < 12 ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-google-fluentd[1600]: + systemctl start google-fluentd
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + in_array hadoop-yarn-resourcemanager DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + local value=hadoop-yarn-resourcemanager
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-yarn-resourcemanager  ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + return 1
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + run_with_retries systemctl start hadoop-yarn-resourcemanager
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + local retry_backoff
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + cmd=("$@")
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + local -a cmd
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + loginfo 'About to run '\''systemctl start hadoop-yarn-resourcemanager'\'' with retries...'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + echo 'About to run '\''systemctl start hadoop-yarn-resourcemanager'\'' with retries...'
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: About to run 'systemctl start hadoop-yarn-resourcemanager' with retries...
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + local update_succeeded=0
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + (( i = 0 ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + (( i < 12 ))
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: + systemctl start hadoop-yarn-resourcemanager
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/spark-history-server.service.d
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + in_array hadoop-mapreduce-historyserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + local value=hadoop-mapreduce-historyserver
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-mapreduce-historyserver  ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-mapreduce-historyserver[1486]: + return
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + in_array spark-history-server DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + local value=spark-history-server
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  spark-history-server  ]]
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-spark-history-server[1487]: + return
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-timelineserver[1489]: Warning: hadoop-yarn-timelineserver.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 23:16:36 google-dataproc-startup[827]: <13>Oct 13 23:16:36 setup-hadoop-yarn-resourcemanager[1483]: Warning: hadoop-yarn-resourcemanager.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]: The following packages will be REMOVED:
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   bind9-host* druid* fonts-font-awesome* fonts-mathjax* geoip-database*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   hadoop-hdfs-datanode* hadoop-hdfs-journalnode* hadoop-hdfs-zkfc*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   hadoop-yarn-nodemanager* hive-webhcat* hive-webhcat-server*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   javascript-common* kafka* kafka-server* knox* krb5-admin-server*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   krb5-config* krb5-kdc* krb5-kpropd* krb5-user* libbind9-140* libc-ares2*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   libdns162* libev4* libfile-copy-recursive-perl* libgeoip1* libgssrpc4*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   libhttp-parser2.8* libisc160* libisccc140* libisccfg140* libjs-bootstrap*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   libjs-d3* libjs-es5-shim* libjs-highlight.js* libjs-jquery*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   libjs-jquery-datatables* libjs-jquery-metadata* libjs-jquery-selectize.js*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   libjs-jquery-tablesorter* libjs-jquery-ui* libjs-json* libjs-mathjax*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   libjs-microplugin.js* libjs-modernizr* libjs-prettify* libjs-sifter.js*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   libjs-twitter-bootstrap* libjs-twitter-bootstrap-datepicker*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   libkadm5clnt-mit11* libkadm5srv-mit11* libkdb5-8* liblua5.1-0*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   libluajit-5.1-2* libluajit-5.1-common* liblwres141* libuv1* libverto-libev1*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   libverto1* libyaml-0-2* littler* node-highlight.js* node-normalize.css*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   nodejs* nodejs-doc* pandoc* pandoc-data* r-cran-assertthat*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-base64enc* r-cran-bindr* r-cran-bindrcpp* r-cran-bit* r-cran-bit64*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-cli* r-cran-colorspace* r-cran-crayon* r-cran-data.table* r-cran-dbi*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-digest* r-cran-dplyr* r-cran-evaluate* r-cran-fansi* r-cran-filehash*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-ggplot2* r-cran-glue* r-cran-googlevis* r-cran-gtable* r-cran-hexbin*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-highr* r-cran-hms* r-cran-htmltools* r-cran-htmlwidgets*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-httpuv* r-cran-jsonlite* r-cran-knitr* r-cran-labeling* r-cran-later*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-lazyeval* r-cran-littler* r-cran-magrittr* r-cran-mapproj*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-maps* r-cran-markdown* r-cran-memoise* r-cran-mime* r-cran-munsell*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-pillar* r-cran-pkgconfig* r-cran-pkgkitten* r-cran-plyr* r-cran-png*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-praise* r-cran-promises* r-cran-purrr* r-cran-r6*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-rcolorbrewer* r-cran-rcpp* r-cran-reshape2* r-cran-rlang*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-rmarkdown* r-cran-rsqlite* r-cran-scales* r-cran-shiny*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-sourcetools* r-cran-sp* r-cran-stringi* r-cran-stringr*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-testit* r-cran-testthat* r-cran-tibble* r-cran-tidyselect*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-tikzdevice* r-cran-tinytex* r-cran-utf8* r-cran-viridislite*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   r-cran-withr* r-cran-xfun* r-cran-xml2* r-cran-xtable* r-cran-yaml* solr*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 uninstall[1420]:   solr-server* update-inetd* xinetd* zeppelin* zookeeper-server*
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 3306
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 5.'
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 5.'
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 5.
<13>Oct 13 23:16:37 google-dataproc-startup[827]: <13>Oct 13 23:16:37 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:38 google-dataproc-startup[827]: <13>Oct 13 23:16:38 uninstall[1420]: 0 upgraded, 0 newly installed, 146 to remove and 1 not upgraded.
<13>Oct 13 23:16:38 google-dataproc-startup[827]: <13>Oct 13 23:16:38 uninstall[1420]: After this operation, 2,006 MB disk space will be freed.
<13>Oct 13 23:16:38 google-dataproc-startup[827]: <13>Oct 13 23:16:38 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:38 google-dataproc-startup[827]: <13>Oct 13 23:16:38 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 3306
<13>Oct 13 23:16:38 google-dataproc-startup[827]: <13>Oct 13 23:16:38 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:16:38 google-dataproc-startup[827]: <13>Oct 13 23:16:38 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 6.'
<13>Oct 13 23:16:38 google-dataproc-startup[827]: <13>Oct 13 23:16:38 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 6.'
<13>Oct 13 23:16:38 google-dataproc-startup[827]: <13>Oct 13 23:16:38 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 6.
<13>Oct 13 23:16:38 google-dataproc-startup[827]: <13>Oct 13 23:16:38 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:39 google-dataproc-startup[827]: <13>Oct 13 23:16:39 uninstall[1420]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 119619 files and directories currently installed.)
<13>Oct 13 23:16:39 google-dataproc-startup[827]: <13>Oct 13 23:16:39 uninstall[1420]: Removing krb5-admin-server (1.15-1+deb9u1) ...
<13>Oct 13 23:16:39 google-dataproc-startup[827]: <13>Oct 13 23:16:39 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:39 google-dataproc-startup[827]: <13>Oct 13 23:16:39 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 3306
<13>Oct 13 23:16:39 google-dataproc-startup[827]: <13>Oct 13 23:16:39 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:16:39 google-dataproc-startup[827]: <13>Oct 13 23:16:39 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 7.'
<13>Oct 13 23:16:39 google-dataproc-startup[827]: <13>Oct 13 23:16:39 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 7.'
<13>Oct 13 23:16:39 google-dataproc-startup[827]: <13>Oct 13 23:16:39 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 3306 failed. Retry attempt: 7.
<13>Oct 13 23:16:39 google-dataproc-startup[827]: <13>Oct 13 23:16:39 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: + update_succeeded=1
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: + break
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: + ((  1  ))
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: + [[ mariadb == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: ++ local property_name=am.primary_only
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: +++ local property_name=am.primary_only
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: ++++ tail -n 1
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: +++ local property_value=false
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: +++ echo false
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: ++ local property_value=false
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: ++ echo false
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-mariadb[1494]: + [[ mariadb == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 3306
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: Connection to cluster-5cb0-m 3306 port [tcp/mysql] succeeded!
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + update_succeeded=1
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 3306 succeeded.'
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 3306 succeeded.'
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 3306 succeeded.
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + break
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + ((  1  ))
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + loginfo 'Service up on host=cluster-5cb0-m port=3306.'
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + echo 'Service up on host=cluster-5cb0-m port=3306.'
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: Service up on host=cluster-5cb0-m port=3306.
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + enable_service hive-metastore
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + local service=hive-metastore
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + local unit=hive-metastore.service
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + run_with_retries systemctl enable hive-metastore.service
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + local retry_backoff
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + cmd=("$@")
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + local -a cmd
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + loginfo 'About to run '\''systemctl enable hive-metastore.service'\'' with retries...'
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + echo 'About to run '\''systemctl enable hive-metastore.service'\'' with retries...'
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: About to run 'systemctl enable hive-metastore.service' with retries...
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + local update_succeeded=0
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + (( i = 0 ))
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + (( i < 12 ))
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: + systemctl enable hive-metastore.service
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: hive-metastore.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:16:40 google-dataproc-startup[827]: <13>Oct 13 23:16:40 setup-hive-metastore[1484]: Executing: /lib/systemd/systemd-sysv-install enable hive-metastore
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:41 INFO namenode.NameNode: STARTUP_MSG: 
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: /************************************************************
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: STARTUP_MSG: Starting NameNode
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: STARTUP_MSG:   host = cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal/10.128.0.20
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: STARTUP_MSG:   args = [-format, -nonInteractive]
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: STARTUP_MSG:   version = 2.9.2
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/hadoop-lzo-0.4.20.jar:/usr/lib/hadoop/lib/httpclient-4.5.2.jar:/usr/lib/hadoop/lib/gcs-connector-hadoop2-1.9.17.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/gcs-connector.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.1
<13>Oct 13 23:16:41 google-dataproc-startup[827]: 3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jetty-6.1.26.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0
<13>Oct 13 23:16:41 google-dataproc-startup[827]: .4.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/u
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: sr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/httpcore-4.4.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/li
<13>Oct 13 23:16:41 google-dataproc-startup[827]: b/jetty-sslengine-6.1.26.jar:/usr/lib/hadoop/lib/m
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: ockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.9.2.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-common-2.9.2-tests.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-2.9.2.jar:/usr/lib/hadoop/.//hadoop-nfs-2.9.2.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth-2.9.2.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib
<13>Oct 13 23:16:41 google-dataproc-startup[827]: /hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoo
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: p-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2
<13>Oct 13 23:16:41 google-dataproc-startup[827]: .7.8.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: .jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.9.2-tests.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/l
<13>Oct 13 23:16:41 google-dataproc-startup[827]: ib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: /hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/api-as
<13>Oct 13 23:16:41 google-dataproc-startup[827]: n1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/comm
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: ons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hado
<13>Oct 13 23:16:41 google-dataproc-startup[827]: op-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/ha
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: doop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/ht
<13>Oct 13 23:16:41 google-dataproc-startup[827]: tpcore-4.4.4.jar:/usr/lib/hadoop-yarn/lib/jetty-ut
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: il-6.1.26.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.9.2.jar:/usr
<13>Oct 13 23:16:41 google-dataproc-startup[827]: /lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/li
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: b/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-y
<13>Oct 13 23:16:41 google-dataproc-startup[827]: arn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-ya
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: rn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.9.2.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce
<13>Oct 13 23:16:41 google-dataproc-startup[827]: /lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapredu
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: ce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//l
<13>Oct 13 23:16:41 google-dataproc-startup[827]: eveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/./
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: /jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ar
<13>Oct 13 23:16:41 google-dataproc-startup[827]: chive-logs.jar:/usr/lib/hadoop-mapreduce/.//log4j-
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: 1.2.17.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//h
<13>Oct 13 23:16:41 google-dataproc-startup[827]: adoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hado
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: op-archives-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.199.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2
<13>Oct 13 23:16:41 google-dataproc-startup[827]: .0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//mssql-jd
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: bc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client
<13>Oct 13 23:16:41 google-dataproc-startup[827]: -hs-plugins-2.9.2.jar:/usr/lib/hadoop-mapreduce/./
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: /hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.9.2-tests.jar:/usr
<13>Oct 13 23:16:41 google-dataproc-startup[827]: /lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: /lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.9.2.jar:/usr/lib/hadoop-ma
<13>Oct 13 23:16:41 google-dataproc-startup[827]: preduce/.//json-20170516.jar:/usr/lib/hadoop-mapre
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: duce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.5.j
<13>Oct 13 23:16:41 google-dataproc-startup[827]: ar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: .jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sd
<13>Oct 13 23:16:41 google-dataproc-startup[827]: k-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ar
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: chive-logs-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.13.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: STARTUP_MSG:   build = https://bigdataoss-internal.googlesource.com/third_party/apache/hadoop -r 849ee9eda72c7e8b1eb9fc5a830432c887914111; compiled by 'bigtop' on 2019-09-18T10:58Z
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: STARTUP_MSG:   java = 1.8.0_222
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: ************************************************************/
<13>Oct 13 23:16:41 google-dataproc-startup[827]: <13>Oct 13 23:16:41 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:41 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
<13>Oct 13 23:16:42 google-dataproc-startup[827]: <13>Oct 13 23:16:42 uninstall[1420]: Removing krb5-kpropd (1.15-1+deb9u1) ...
<13>Oct 13 23:16:42 google-dataproc-startup[827]: <13>Oct 13 23:16:42 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:42 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-hdfs-namenode[1482]: 2019-10-13T23:16:43.259+0000: 5.455: [GC (Allocation Failure) 2019-10-13T23:16:43.259+0000: 5.455: [ParNew: 32320K->3968K(36288K), 0.0404740 secs] 32320K->5328K(116864K), 0.0405689 secs] [Times: user=0.02 sys=0.00, real=0.04 secs] 
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + update_succeeded=1
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + break
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + ((  1  ))
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: ++ systemctl show hive-metastore.service -p Restart,RemainAfterExit
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + local 'props=Restart=no
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: RemainAfterExit=no'
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + [[ Restart=no
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + [[ Restart=no
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + local drop_in_dir=/etc/systemd/system/hive-metastore.service.d
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + mkdir /etc/systemd/system/hive-metastore.service.d
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-metastore.service.d
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + in_array hive-metastore DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + local value=hive-metastore
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hive-metastore  ]]
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + return 1
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + [[ hive-metastore == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + run_with_retries systemctl start hive-metastore
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + local retry_backoff
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + cmd=("$@")
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + local -a cmd
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + loginfo 'About to run '\''systemctl start hive-metastore'\'' with retries...'
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + echo 'About to run '\''systemctl start hive-metastore'\'' with retries...'
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: About to run 'systemctl start hive-metastore' with retries...
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + local update_succeeded=0
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + (( i = 0 ))
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + (( i < 12 ))
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: + systemctl start hive-metastore
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hive-metastore[1484]: Warning: hive-metastore.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: + update_succeeded=1
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: + break
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: + ((  1  ))
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: ++ local property_name=am.primary_only
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-resourcemanager[1483]: + update_succeeded=1
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-resourcemanager[1483]: + break
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-resourcemanager[1483]: + ((  1  ))
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-resourcemanager[1483]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-resourcemanager[1483]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: +++ local property_name=am.primary_only
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-resourcemanager[1483]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-resourcemanager[1483]: ++ local property_name=am.primary_only
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-resourcemanager[1483]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-resourcemanager[1483]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-resourcemanager[1483]: +++ local property_name=am.primary_only
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:43 google-dataproc-startup[827]: <13>Oct 13 23:16:43 setup-hadoop-yarn-timelineserver[1489]: ++++ tail -n 1
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-timelineserver[1489]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-timelineserver[1489]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-timelineserver[1489]: +++ local property_value=false
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-timelineserver[1489]: +++ echo false
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-timelineserver[1489]: ++ local property_value=false
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-timelineserver[1489]: ++ echo false
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-timelineserver[1489]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: ++++ tail -n 1
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: +++ local property_value=false
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: +++ echo false
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: ++ local property_value=false
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: ++ echo false
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: + [[ 0 -eq 0 ]]
<13>Oct 13 23:16:44 google-dataproc-startup[827]: <13>Oct 13 23:16:44 setup-hadoop-yarn-resourcemanager[1483]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:16:45 google-dataproc-startup[827]: <13>Oct 13 23:16:45 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:45 WARN common.Util: Path /hadoop/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
<13>Oct 13 23:16:45 google-dataproc-startup[827]: <13>Oct 13 23:16:45 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:45 WARN common.Util: Path /hadoop/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
<13>Oct 13 23:16:45 google-dataproc-startup[827]: <13>Oct 13 23:16:45 setup-hadoop-hdfs-namenode[1482]: Formatting using clusterid: CID-5c3f0994-9b70-437d-ae1a-aa22a30449bc
<13>Oct 13 23:16:46 google-dataproc-startup[827]: <13>Oct 13 23:16:46 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:46 INFO namenode.FSEditLog: Edit logging is async:true
<13>Oct 13 23:16:46 google-dataproc-startup[827]: <13>Oct 13 23:16:46 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:46 INFO namenode.FSNamesystem: KeyProvider: null
<13>Oct 13 23:16:46 google-dataproc-startup[827]: <13>Oct 13 23:16:46 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:46 INFO namenode.FSNamesystem: fsLock is fair: true
<13>Oct 13 23:16:46 google-dataproc-startup[827]: <13>Oct 13 23:16:46 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:46 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
<13>Oct 13 23:16:46 google-dataproc-startup[827]: <13>Oct 13 23:16:46 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:46 INFO namenode.FSNamesystem: fsOwner             = hdfs (auth:SIMPLE)
<13>Oct 13 23:16:46 google-dataproc-startup[827]: <13>Oct 13 23:16:46 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:46 INFO namenode.FSNamesystem: supergroup          = hadoop
<13>Oct 13 23:16:46 google-dataproc-startup[827]: <13>Oct 13 23:16:46 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:46 INFO namenode.FSNamesystem: isPermissionEnabled = false
<13>Oct 13 23:16:46 google-dataproc-startup[827]: <13>Oct 13 23:16:46 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:46 INFO namenode.FSNamesystem: HA Enabled: false
<13>Oct 13 23:16:46 google-dataproc-startup[827]: <13>Oct 13 23:16:46 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:46 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
<13>Oct 13 23:16:46 google-dataproc-startup[827]: <13>Oct 13 23:16:46 setup-hadoop-hdfs-namenode[1482]: 2019-10-13T23:16:46.784+0000: 8.980: [GC (Allocation Failure) 2019-10-13T23:16:46.784+0000: 8.980: [ParNew: 36288K->3967K(36288K), 0.1658386 secs] 37648K->8262K(116864K), 0.1659333 secs] [Times: user=0.05 sys=0.00, real=0.16 secs] 
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 uninstall[1420]: Removing krb5-kdc (1.15-1+deb9u1) ...
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + update_succeeded=1
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + break
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + ((  1  ))
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + [[ hive-metastore == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + wait_for_port cluster-5cb0-m 9083
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + local -r host=cluster-5cb0-m
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + local -r port=9083
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + loginfo 'Waiting for service to come up on host=cluster-5cb0-m port=9083.'
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + echo 'Waiting for service to come up on host=cluster-5cb0-m port=9083.'
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: Waiting for service to come up on host=cluster-5cb0-m port=9083.
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + retry_with_constant_backoff nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + local max_retry=300
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + cmd=("$@")
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + local -a cmd
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + local update_succeeded=0
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: ++ seq 1 300
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 1.'
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 1.'
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 1.
<13>Oct 13 23:16:47 google-dataproc-startup[827]: <13>Oct 13 23:16:47 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:48 google-dataproc-startup[827]: <13>Oct 13 23:16:48 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:48 google-dataproc-startup[827]: <13>Oct 13 23:16:48 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:48 google-dataproc-startup[827]: <13>Oct 13 23:16:48 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:48 google-dataproc-startup[827]: <13>Oct 13 23:16:48 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 2.'
<13>Oct 13 23:16:48 google-dataproc-startup[827]: <13>Oct 13 23:16:48 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 2.'
<13>Oct 13 23:16:48 google-dataproc-startup[827]: <13>Oct 13 23:16:48 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 2.
<13>Oct 13 23:16:48 google-dataproc-startup[827]: <13>Oct 13 23:16:48 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: + update_succeeded=1
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: + break
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: + ((  1  ))
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: ++ local property_name=am.primary_only
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: +++ local property_name=am.primary_only
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: ++++ tail -n 1
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: +++ local property_value=false
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: +++ echo false
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: ++ local property_value=false
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: ++ echo false
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-secondarynamenode[1495]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO util.HostsFileReader: Adding a node "cluster-5cb0-w-0.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO util.HostsFileReader: Adding a node "cluster-5cb0-w-1.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO util.HostsFileReader: Adding a node "cluster-5cb0-w-2.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.retry-hostname-dns-lookup=true
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Oct 13 23:16:49
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO util.GSet: Computing capacity for map BlocksMap
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO util.GSet: 2.0% max memory 1.4 GB = 29.6 MB
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO util.GSet: capacity      = 2^22 = 4194304 entries
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 2019-10-13T23:16:49.692+0000: 11.888: [GC (Allocation Failure) 2019-10-13T23:16:49.692+0000: 11.888: [ParNew: 35455K->3968K(36288K), 0.0409145 secs] 39749K->9704K(116864K), 0.0409913 secs] [Times: user=0.02 sys=0.00, real=0.04 secs] 
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManager: defaultReplication         = 2
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManager: maxReplication             = 512
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManager: minReplication             = 1
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:49 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:49 google-dataproc-startup[827]: <13>Oct 13 23:16:49 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO namenode.FSNamesystem: Append Enabled: true
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 3.'
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 3.'
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 3.
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: Computing capacity for map INodeMap
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: 1.0% max memory 1.4 GB = 14.8 MB
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: capacity      = 2^21 = 2097152 entries
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 2019-10-13T23:16:50.394+0000: 12.591: [GC (Allocation Failure) 2019-10-13T23:16:50.394+0000: 12.591: [ParNew: 28293K->1249K(36288K), 0.2110232 secs] 34030K->23929K(116864K), 0.2111007 secs] [Times: user=0.06 sys=0.02, real=0.21 secs] 
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO namenode.FSDirectory: ACLs enabled? false
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO namenode.FSDirectory: XAttrs enabled? true
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO namenode.NameNode: Caching file names occurring more than 10 times
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: Computing capacity for map cachedBlocks
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: 0.25% max memory 1.4 GB = 3.7 MB
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: capacity      = 2^19 = 524288 entries
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: Computing capacity for map NameNodeRetryCache
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 454.5 KB
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO util.GSet: capacity      = 2^16 = 65536 entries
<13>Oct 13 23:16:50 google-dataproc-startup[827]: <13>Oct 13 23:16:50 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:50 INFO namenode.FSImage: Allocated new BlockPoolId: BP-6969458-10.128.0.20-1571008610894
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:51 INFO common.Storage: Storage directory /hadoop/dfs/name has been successfully formatted.
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 4.'
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 4.'
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 4.
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:51 INFO namenode.FSImageFormatProtobuf: Saving image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 uninstall[1420]: Removing krb5-user (1.15-1+deb9u1) ...
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 uninstall[1420]: Removing krb5-config (2.6) ...
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:51 INFO namenode.FSImageFormatProtobuf: Image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 319 bytes saved in 0 seconds .
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 uninstall[1420]: Removing bind9-host (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:51 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 uninstall[1420]: Removing druid (0.13.0-incubating-1) ...
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]: 19/10/13 23:16:51 INFO namenode.NameNode: SHUTDOWN_MSG: 
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]: /************************************************************
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]: SHUTDOWN_MSG: Shutting down NameNode at cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal/10.128.0.20
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]: ************************************************************/
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]: Heap
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]:  par new generation   total 36288K, used 30545K [0x00000000a2800000, 0x00000000a4f50000, 0x00000000ace60000)
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]:   eden space 32320K,  90% used [0x00000000a2800000, 0x00000000a449c0f8, 0x00000000a4790000)
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]:   from space 3968K,  31% used [0x00000000a4790000, 0x00000000a48c84a0, 0x00000000a4b70000)
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]:   to   space 3968K,   0% used [0x00000000a4b70000, 0x00000000a4b70000, 0x00000000a4f50000)
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]:  concurrent mark-sweep generation total 80576K, used 22680K [0x00000000ace60000, 0x00000000b1d10000, 0x0000000100000000)
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]:  Metaspace       used 19030K, capacity 19228K, committed 19456K, reserved 1067008K
<13>Oct 13 23:16:51 google-dataproc-startup[827]: <13>Oct 13 23:16:51 setup-hadoop-hdfs-namenode[1482]:   class space    used 2133K, capacity 2192K, committed 2304K, reserved 1048576K
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 uninstall[1420]: dpkg: warning: while removing druid, directory '/usr/lib/druid/extensions/mysql-metadata-storage' not empty so not removed
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 uninstall[1420]: dpkg: warning: while removing druid, directory '/usr/lib/druid/conf/druid/_common' not empty so not removed
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + update_succeeded=1
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + break
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + ((  1  ))
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + enable_service hadoop-hdfs-namenode
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + local service=hadoop-hdfs-namenode
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + local unit=hadoop-hdfs-namenode.service
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + run_with_retries systemctl enable hadoop-hdfs-namenode.service
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + local retry_backoff
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + cmd=("$@")
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + local -a cmd
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + loginfo 'About to run '\''systemctl enable hadoop-hdfs-namenode.service'\'' with retries...'
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + echo 'About to run '\''systemctl enable hadoop-hdfs-namenode.service'\'' with retries...'
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: About to run 'systemctl enable hadoop-hdfs-namenode.service' with retries...
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + local update_succeeded=0
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + (( i = 0 ))
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + (( i < 12 ))
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: + systemctl enable hadoop-hdfs-namenode.service
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 uninstall[1420]: Removing r-cran-rmarkdown (1.11+dfsg-1~bpo9+1) ...
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: hadoop-hdfs-namenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hadoop-hdfs-namenode[1482]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-namenode
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 5.'
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 5.'
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 5.
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 uninstall[1420]: Removing r-cran-shiny (1.2.0+dfsg-1~bpo9+1) ...
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 uninstall[1420]: Removing fonts-font-awesome (4.7.0~dfsg-1) ...
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 uninstall[1420]: Removing r-cran-knitr (1.21+dfsg-2~bpo9+1) ...
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 uninstall[1420]: Removing r-cran-markdown (0.9+dfsg-1~bpo9+1) ...
<13>Oct 13 23:16:52 google-dataproc-startup[827]: <13>Oct 13 23:16:52 uninstall[1420]: Removing libjs-mathjax (2.7.0-2) ...
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 6.'
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 6.'
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 6.
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + update_succeeded=1
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + break
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + ((  1  ))
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: ++ systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + local 'props=Restart=no
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: RemainAfterExit=no'
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + [[ Restart=no
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + [[ Restart=no
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + local drop_in_dir=/etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + mkdir /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + in_array hadoop-hdfs-namenode DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + local value=hadoop-hdfs-namenode
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-hdfs-namenode  ]]
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + return 1
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + run_with_retries systemctl start hadoop-hdfs-namenode
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + local retry_backoff
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + cmd=("$@")
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + local -a cmd
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + loginfo 'About to run '\''systemctl start hadoop-hdfs-namenode'\'' with retries...'
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + echo 'About to run '\''systemctl start hadoop-hdfs-namenode'\'' with retries...'
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: About to run 'systemctl start hadoop-hdfs-namenode' with retries...
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + local update_succeeded=0
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + (( i = 0 ))
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + (( i < 12 ))
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: + systemctl start hadoop-hdfs-namenode
<13>Oct 13 23:16:53 google-dataproc-startup[827]: <13>Oct 13 23:16:53 setup-hadoop-hdfs-namenode[1482]: Warning: hadoop-hdfs-namenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 7.'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 7.'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 7.
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: + update_succeeded=1
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: + break
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: + ((  1  ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: + [[ google-fluentd == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 uninstall[1420]: Removing fonts-mathjax (2.7.0-2) ...
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: ++ local property_name=am.primary_only
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: +++ local property_name=am.primary_only
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: ++++ cut -d = -f 2-
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: ++++ tail -n 1
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 uninstall[1420]: Removing geoip-database (20170512-1) ...
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: +++ local property_value=false
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: +++ echo false
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: ++ local property_value=false
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: ++ echo false
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 setup-google-fluentd[1600]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + pid=1495
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + cmd='setup_service hadoop-hdfs-secondarynamenode'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1495 cmd=[setup_service hadoop-hdfs-secondarynamenode]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + echo 'Waiting on pid=1495 cmd=[setup_service hadoop-hdfs-secondarynamenode]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: Waiting on pid=1495 cmd=[setup_service hadoop-hdfs-secondarynamenode]
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + wait 1495
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + pid=1494
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + cmd='setup_service mariadb'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1494 cmd=[setup_service mariadb]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + echo 'Waiting on pid=1494 cmd=[setup_service mariadb]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: Waiting on pid=1494 cmd=[setup_service mariadb]
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + wait 1494
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + pid=1489
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + cmd='setup_service hadoop-yarn-timelineserver'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1489 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + echo 'Waiting on pid=1489 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: Waiting on pid=1489 cmd=[setup_service hadoop-yarn-timelineserver]
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + wait 1489
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + pid=1487
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + cmd='setup_service spark-history-server'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1487 cmd=[setup_service spark-history-server]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + echo 'Waiting on pid=1487 cmd=[setup_service spark-history-server]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: Waiting on pid=1487 cmd=[setup_service spark-history-server]
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + wait 1487
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + pid=1486
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + cmd='setup_service hadoop-mapreduce-historyserver'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1486 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + echo 'Waiting on pid=1486 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: Waiting on pid=1486 cmd=[setup_service hadoop-mapreduce-historyserver]
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + wait 1486
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + pid=1485
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + cmd='setup_service hive-server2'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1485 cmd=[setup_service hive-server2]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + echo 'Waiting on pid=1485 cmd=[setup_service hive-server2]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: Waiting on pid=1485 cmd=[setup_service hive-server2]
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + wait 1485
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + pid=1484
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + cmd='setup_service hive-metastore'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1484 cmd=[setup_service hive-metastore]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + echo 'Waiting on pid=1484 cmd=[setup_service hive-metastore]'
<13>Oct 13 23:16:54 google-dataproc-startup[827]: Waiting on pid=1484 cmd=[setup_service hive-metastore]
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:16:54 google-dataproc-startup[827]: + wait 1484
<13>Oct 13 23:16:54 google-dataproc-startup[827]: <13>Oct 13 23:16:54 uninstall[1420]: Removing hadoop-hdfs-datanode (2.9.2-1) ...
<13>Oct 13 23:16:55 google-dataproc-startup[827]: <13>Oct 13 23:16:55 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:55 google-dataproc-startup[827]: <13>Oct 13 23:16:55 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:55 google-dataproc-startup[827]: <13>Oct 13 23:16:55 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:55 google-dataproc-startup[827]: <13>Oct 13 23:16:55 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 8.'
<13>Oct 13 23:16:55 google-dataproc-startup[827]: <13>Oct 13 23:16:55 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 8.'
<13>Oct 13 23:16:55 google-dataproc-startup[827]: <13>Oct 13 23:16:55 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 8.
<13>Oct 13 23:16:55 google-dataproc-startup[827]: <13>Oct 13 23:16:55 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:55 google-dataproc-startup[827]: <13>Oct 13 23:16:55 uninstall[1420]: Removing hadoop-hdfs-journalnode (2.9.2-1) ...
<13>Oct 13 23:16:56 google-dataproc-startup[827]: <13>Oct 13 23:16:56 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:56 google-dataproc-startup[827]: <13>Oct 13 23:16:56 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:56 google-dataproc-startup[827]: <13>Oct 13 23:16:56 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:56 google-dataproc-startup[827]: <13>Oct 13 23:16:56 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 9.'
<13>Oct 13 23:16:56 google-dataproc-startup[827]: <13>Oct 13 23:16:56 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 9.'
<13>Oct 13 23:16:56 google-dataproc-startup[827]: <13>Oct 13 23:16:56 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 9.
<13>Oct 13 23:16:56 google-dataproc-startup[827]: <13>Oct 13 23:16:56 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:56 google-dataproc-startup[827]: <13>Oct 13 23:16:56 uninstall[1420]: Removing hadoop-hdfs-zkfc (2.9.2-1) ...
<13>Oct 13 23:16:57 google-dataproc-startup[827]: <13>Oct 13 23:16:57 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:57 google-dataproc-startup[827]: <13>Oct 13 23:16:57 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:57 google-dataproc-startup[827]: <13>Oct 13 23:16:57 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:57 google-dataproc-startup[827]: <13>Oct 13 23:16:57 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 10.'
<13>Oct 13 23:16:57 google-dataproc-startup[827]: <13>Oct 13 23:16:57 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 10.'
<13>Oct 13 23:16:57 google-dataproc-startup[827]: <13>Oct 13 23:16:57 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 10.
<13>Oct 13 23:16:57 google-dataproc-startup[827]: <13>Oct 13 23:16:57 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:57 google-dataproc-startup[827]: <13>Oct 13 23:16:57 uninstall[1420]: Removing hadoop-yarn-nodemanager (2.9.2-1) ...
<13>Oct 13 23:16:58 google-dataproc-startup[827]: <13>Oct 13 23:16:58 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:58 google-dataproc-startup[827]: <13>Oct 13 23:16:58 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:58 google-dataproc-startup[827]: <13>Oct 13 23:16:58 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:58 google-dataproc-startup[827]: <13>Oct 13 23:16:58 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 11.'
<13>Oct 13 23:16:58 google-dataproc-startup[827]: <13>Oct 13 23:16:58 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 11.'
<13>Oct 13 23:16:58 google-dataproc-startup[827]: <13>Oct 13 23:16:58 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 11.
<13>Oct 13 23:16:58 google-dataproc-startup[827]: <13>Oct 13 23:16:58 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:58 google-dataproc-startup[827]: <13>Oct 13 23:16:58 uninstall[1420]: Removing hive-webhcat-server (2.3.5-1) ...
<13>Oct 13 23:16:59 google-dataproc-startup[827]: <13>Oct 13 23:16:59 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:16:59 google-dataproc-startup[827]: <13>Oct 13 23:16:59 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:16:59 google-dataproc-startup[827]: <13>Oct 13 23:16:59 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:16:59 google-dataproc-startup[827]: <13>Oct 13 23:16:59 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 12.'
<13>Oct 13 23:16:59 google-dataproc-startup[827]: <13>Oct 13 23:16:59 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 12.'
<13>Oct 13 23:16:59 google-dataproc-startup[827]: <13>Oct 13 23:16:59 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 12.
<13>Oct 13 23:16:59 google-dataproc-startup[827]: <13>Oct 13 23:16:59 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:16:59 google-dataproc-startup[827]: <13>Oct 13 23:16:59 uninstall[1420]: Removing hive-webhcat (2.3.5-1) ...
<13>Oct 13 23:16:59 google-dataproc-startup[827]: <13>Oct 13 23:16:59 uninstall[1420]: Removing javascript-common (11) ...
<13>Oct 13 23:16:59 google-dataproc-startup[827]: <13>Oct 13 23:16:59 uninstall[1420]: Removing kafka-server (1.1.1-1) ...
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 uninstall[1420]: Removing kafka (1.1.1-1) ...
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 uninstall[1420]: Removing knox (1.1.0-1) ...
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 13.'
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 13.'
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 13.
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 uninstall[1420]: Removing libbind9-140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 uninstall[1420]: Removing node-highlight.js (8.2+ds-5) ...
<13>Oct 13 23:17:00 google-dataproc-startup[827]: <13>Oct 13 23:17:00 uninstall[1420]: Removing nodejs (8.11.1~dfsg-2~bpo9+1) ...
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 uninstall[1420]: Removing libc-ares2:amd64 (1.14.0-1~bpo9+1) ...
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 uninstall[1420]: Removing libisccfg140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 uninstall[1420]: Removing libdns162:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 uninstall[1420]: Removing update-inetd (4.44) ...
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 uninstall[1420]: Removing libfile-copy-recursive-perl (0.38-1) ...
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 14.'
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 14.'
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 14.
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 uninstall[1420]: Removing liblwres141:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 uninstall[1420]: Removing libisccc140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 uninstall[1420]: Removing libkadm5srv-mit11:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 uninstall[1420]: Removing libkdb5-8:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 uninstall[1420]: Removing libkadm5clnt-mit11:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 23:17:01 google-dataproc-startup[827]: <13>Oct 13 23:17:01 uninstall[1420]: Removing libgssrpc4:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libhttp-parser2.8:amd64 (2.8.1-1~bpo9+1) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libisc160:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libjs-bootstrap (3.3.7+dfsg-2+deb9u2) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libjs-d3 (3.5.17-2) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libjs-es5-shim (4.5.9-1) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing r-cran-highr (0.6-1) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libjs-highlight.js (8.2+ds-5) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 15.'
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 15.'
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 15.
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libjs-jquery-selectize.js (0.12.4+dfsg-1~bpo9+1) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libjs-twitter-bootstrap-datepicker (1.3.1+dfsg1-1) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libjs-twitter-bootstrap (2.0.2+dfsg-10) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libjs-jquery-tablesorter (1:2.31.1+dfsg1-1~bpo9+1) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libjs-jquery-datatables (1.10.13+dfsg-2) ...
<13>Oct 13 23:17:02 google-dataproc-startup[827]: <13>Oct 13 23:17:02 uninstall[1420]: Removing libjs-jquery-metadata (11-3) ...
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 uninstall[1420]: Removing libjs-jquery-ui (1.12.1+dfsg-4) ...
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + update_succeeded=1
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + break
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + ((  1  ))
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + [[ 0 -eq 0 ]]
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + loginfo 'Waiting for namenode to listen on rpc port'
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + echo 'Waiting for namenode to listen on rpc port'
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: Waiting for namenode to listen on rpc port
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + wait_for_port cluster-5cb0-m 8020
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + local -r host=cluster-5cb0-m
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + local -r port=8020
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + loginfo 'Waiting for service to come up on host=cluster-5cb0-m port=8020.'
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + echo 'Waiting for service to come up on host=cluster-5cb0-m port=8020.'
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: Waiting for service to come up on host=cluster-5cb0-m port=8020.
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + retry_with_constant_backoff nc -v -z -w 0 cluster-5cb0-m 8020
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + local max_retry=300
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + cmd=("$@")
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + local -a cmd
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + local update_succeeded=0
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: ++ seq 1 300
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + nc -v -z -w 0 cluster-5cb0-m 8020
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: nc: connect to cluster-5cb0-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 1.'
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + echo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 1.'
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 1.
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hadoop-hdfs-namenode[1482]: + sleep 1
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 uninstall[1420]: Removing libjs-json (0~20160510-1) ...
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 uninstall[1420]: Removing libjs-microplugin.js (0.0.3+dfsg-1) ...
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 uninstall[1420]: Removing libjs-modernizr (2.6.2+ds1-1) ...
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 uninstall[1420]: Removing libjs-prettify (2013.03.04+dfsg-4) ...
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 16.'
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 16.'
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 16.
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 uninstall[1420]: Removing libjs-sifter.js (0.5.1+dfsg-2) ...
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 uninstall[1420]: Removing pandoc (1.17.2~dfsg-3) ...
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 uninstall[1420]: Removing liblua5.1-0:amd64 (5.1.5-8.1+b2) ...
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 uninstall[1420]: Removing libluajit-5.1-2:amd64 (2.1.0~beta3+dfsg-5.1~bpo9+1) ...
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 uninstall[1420]: Removing libluajit-5.1-common (2.1.0~beta3+dfsg-5.1~bpo9+1) ...
<13>Oct 13 23:17:03 google-dataproc-startup[827]: <13>Oct 13 23:17:03 uninstall[1420]: Removing r-cran-httpuv (1.4.5.1+dfsg-1~bpo9+1) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing libuv1:amd64 (1.18.0-3~bpo9+1) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing libyaml-0-2:amd64 (0.2.1-1~bpo9+1) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing r-cran-dplyr (0.7.8-1~bpo9+1) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hadoop-hdfs-namenode[1482]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hadoop-hdfs-namenode[1482]: + nc -v -z -w 0 cluster-5cb0-m 8020
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hadoop-hdfs-namenode[1482]: nc: connect to cluster-5cb0-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hadoop-hdfs-namenode[1482]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 2.'
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hadoop-hdfs-namenode[1482]: + echo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 2.'
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hadoop-hdfs-namenode[1482]: nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 2.
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hadoop-hdfs-namenode[1482]: + sleep 1
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing r-cran-tidyselect (0.2.5-1~bpo9+1) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing r-cran-ggplot2 (3.1.0-1~bpo9+1) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing r-cran-scales (1.0.0-1~bpo9+1) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing littler (0.3.1-1) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 17.'
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 17.'
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 17.
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing node-normalize.css (8.0.0-3~bpo9+1) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing nodejs-doc (8.11.1~dfsg-2~bpo9+1) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing pandoc-data (1.17.2~dfsg-3) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing r-cran-tibble (2.0.1-1~bpo9+1) ...
<13>Oct 13 23:17:04 google-dataproc-startup[827]: <13>Oct 13 23:17:04 uninstall[1420]: Removing r-cran-pillar (1.3.1-1~bpo9+1) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-base64enc (0.1-3-1) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-bindrcpp (0.2.2-2~bpo9+1) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hadoop-hdfs-namenode[1482]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hadoop-hdfs-namenode[1482]: + nc -v -z -w 0 cluster-5cb0-m 8020
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-bindr (0.1.1-2~bpo9+1) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hadoop-hdfs-namenode[1482]: nc: connect to cluster-5cb0-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hadoop-hdfs-namenode[1482]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 3.'
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hadoop-hdfs-namenode[1482]: + echo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 3.'
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hadoop-hdfs-namenode[1482]: nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 3.
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hadoop-hdfs-namenode[1482]: + sleep 1
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-bit64 (0.9-7-2~bpo9+1) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-bit (1.1-14-1~bpo9+1) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-munsell (0.5.0-1~bpo9+1) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-colorspace (1.3-2-1) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 18.'
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 18.'
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 18.
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-testthat (2.0.1-1~bpo9+1) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-data.table (1.10.0-1) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-rsqlite (1.1-2-1) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-dbi (1.0.0-1~bpo9+2) ...
<13>Oct 13 23:17:05 google-dataproc-startup[827]: <13>Oct 13 23:17:05 uninstall[1420]: Removing r-cran-memoise (1.1.0-1~bpo9+1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-htmlwidgets (1.3+dfsg-1~bpo9+1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-htmltools (0.3.6-2~bpo9+1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-digest (0.6.11-1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-evaluate (0.10-1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hadoop-hdfs-namenode[1482]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hadoop-hdfs-namenode[1482]: + nc -v -z -w 0 cluster-5cb0-m 8020
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hadoop-hdfs-namenode[1482]: nc: connect to cluster-5cb0-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hadoop-hdfs-namenode[1482]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 4.'
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hadoop-hdfs-namenode[1482]: + echo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 4.'
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hadoop-hdfs-namenode[1482]: nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 4.
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hadoop-hdfs-namenode[1482]: + sleep 1
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-fansi (0.4.0-1~bpo9+1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-tikzdevice (0.10-1-1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-filehash (2.3-1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-reshape2 (1.4.2-1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-stringr (1.4.0-1~bpo9+1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 19.'
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 19.'
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 19.
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-glue (1.3.0-1~bpo9+1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-googlevis (0.6.2-1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-gtable (0.2.0-1) ...
<13>Oct 13 23:17:06 google-dataproc-startup[827]: <13>Oct 13 23:17:06 uninstall[1420]: Removing r-cran-hexbin (1.27.1-1) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-hms (0.4.2-1~bpo9+1) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-jsonlite (1.6+dfsg-1~bpo9+1) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-labeling (0.3-1) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-promises (1.0.1-2~bpo9+1) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-later (0.7.5+dfsg-2~bpo9+1) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hadoop-hdfs-namenode[1482]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hadoop-hdfs-namenode[1482]: + nc -v -z -w 0 cluster-5cb0-m 8020
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hadoop-hdfs-namenode[1482]: nc: connect to cluster-5cb0-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hadoop-hdfs-namenode[1482]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 5.'
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hadoop-hdfs-namenode[1482]: + echo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 5.'
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hadoop-hdfs-namenode[1482]: nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 5.
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hadoop-hdfs-namenode[1482]: + sleep 1
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-lazyeval (0.2.0-1) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-purrr (0.3.0-1~bpo9+1) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-magrittr (1.5-3) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-mapproj (1.2-4-1) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 20.'
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 20.'
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 20.
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-maps (3.1.1-1) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-mime (0.5-1) ...
<13>Oct 13 23:17:07 google-dataproc-startup[827]: <13>Oct 13 23:17:07 uninstall[1420]: Removing r-cran-pkgconfig (2.0.2-1~bpo9+1) ...
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 uninstall[1420]: Removing r-cran-plyr (1.8.4-1) ...
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 uninstall[1420]: Removing r-cran-png (0.1-7-1) ...
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 uninstall[1420]: Removing r-cran-praise (1.0.0-1) ...
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 uninstall[1420]: Removing r-cran-r6 (2.4.0-1~bpo9+1) ...
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 uninstall[1420]: Removing r-cran-rcolorbrewer (1.1-2-1) ...
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hadoop-hdfs-namenode[1482]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hadoop-hdfs-namenode[1482]: + nc -v -z -w 0 cluster-5cb0-m 8020
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hadoop-hdfs-namenode[1482]: nc: connect to cluster-5cb0-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hadoop-hdfs-namenode[1482]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 6.'
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hadoop-hdfs-namenode[1482]: + echo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 6.'
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hadoop-hdfs-namenode[1482]: nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 6.
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hadoop-hdfs-namenode[1482]: + sleep 1
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 uninstall[1420]: Removing r-cran-rlang (0.3.1-2~bpo9+1) ...
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 uninstall[1420]: Removing r-cran-sourcetools (0.1.5-1) ...
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 uninstall[1420]: Removing r-cran-sp (1:1.2-4-1) ...
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 uninstall[1420]: Removing r-cran-stringi (1.2.4-2~bpo9+1) ...
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 21.'
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 21.'
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 21.
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 uninstall[1420]: Removing r-cran-testit (0.6-1) ...
<13>Oct 13 23:17:08 google-dataproc-startup[827]: <13>Oct 13 23:17:08 uninstall[1420]: Removing r-cran-tinytex (0.10-1~bpo9+1) ...
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 uninstall[1420]: Removing r-cran-utf8 (1.1.4-1~bpo9+1) ...
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 uninstall[1420]: Removing r-cran-viridislite (0.3.0-3~bpo9+1) ...
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 uninstall[1420]: Removing r-cran-withr (2.1.2-1~bpo9+1) ...
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 uninstall[1420]: Removing r-cran-xfun (0.4-1~bpo9+1) ...
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 uninstall[1420]: Removing r-cran-xml2 (1.1.0-1) ...
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 uninstall[1420]: Removing r-cran-xtable (1:1.8-2-1) ...
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hadoop-hdfs-namenode[1482]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hadoop-hdfs-namenode[1482]: + nc -v -z -w 0 cluster-5cb0-m 8020
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hadoop-hdfs-namenode[1482]: nc: connect to cluster-5cb0-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hadoop-hdfs-namenode[1482]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 7.'
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hadoop-hdfs-namenode[1482]: + echo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 7.'
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hadoop-hdfs-namenode[1482]: nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 7.
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hadoop-hdfs-namenode[1482]: + sleep 1
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 uninstall[1420]: Removing r-cran-yaml (2.2.0-1~bpo9+1) ...
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 uninstall[1420]: Removing solr-server (6.6.5-1) ...
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 22.'
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 22.'
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 22.
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:09 google-dataproc-startup[827]: <13>Oct 13 23:17:09 uninstall[1420]: Removing solr (6.6.5-1) ...
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 uninstall[1420]: Removing xinetd (1:2.3.15-7) ...
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hadoop-hdfs-namenode[1482]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hadoop-hdfs-namenode[1482]: + nc -v -z -w 0 cluster-5cb0-m 8020
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hadoop-hdfs-namenode[1482]: nc: connect to cluster-5cb0-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hadoop-hdfs-namenode[1482]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 8.'
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hadoop-hdfs-namenode[1482]: + echo 'nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 8.'
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hadoop-hdfs-namenode[1482]: nc -v -z -w 0 cluster-5cb0-m 8020 failed. Retry attempt: 8.
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hadoop-hdfs-namenode[1482]: + sleep 1
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 23.'
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 23.'
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 23.
<13>Oct 13 23:17:10 google-dataproc-startup[827]: <13>Oct 13 23:17:10 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 uninstall[1420]: Removing zeppelin (0.8.0-1) ...
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + nc -v -z -w 0 cluster-5cb0-m 8020
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: Connection to cluster-5cb0-m 8020 port [tcp/*] succeeded!
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + update_succeeded=1
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 8020 succeeded.'
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + echo 'nc -v -z -w 0 cluster-5cb0-m 8020 succeeded.'
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: nc -v -z -w 0 cluster-5cb0-m 8020 succeeded.
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + break
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + ((  1  ))
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + loginfo 'Service up on host=cluster-5cb0-m port=8020.'
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + echo 'Service up on host=cluster-5cb0-m port=8020.'
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: Service up on host=cluster-5cb0-m port=8020.
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + loginfo 'Initializing HDFS directories'
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + echo 'Initializing HDFS directories'
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: Initializing HDFS directories
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + HADOOP_USERS=(hdfs mapred yarn spark pig hive hbase zookeeper)
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + REAL_USERS=($(getent passwd | awk -F: '1000 < $3 && $3 < 6000 { print $1}'))
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: ++ getent passwd
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: ++ awk -F: '1000 < $3 && $3 < 6000 { print $1}'
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + HDFS_USERS=("${HADOOP_USERS[@]}" "${REAL_USERS[@]}")
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + HDFS_USER_DIRS=("${HDFS_USERS[@]/#//user/}")
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.eventLog.dir
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: ++ local property_file=/etc/spark/conf/spark-defaults.conf
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: ++ local property_name=spark.eventLog.dir
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: +++ tail -n 1
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: +++ grep '^spark.eventLog.dir=' /etc/spark/conf/spark-defaults.conf
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: +++ cut -d = -f 2-
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: ++ local property_value=hdfs://cluster-5cb0-m/user/spark/eventlog
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: ++ echo hdfs://cluster-5cb0-m/user/spark/eventlog
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + SPARK_EVENTLOG_DIR=hdfs://cluster-5cb0-m/user/spark/eventlog
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hadoop-hdfs-namenode[1482]: + su -s /bin/bash hdfs -c 'login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-5cb0-m.us-central1-a.c.lustrous-drake-255300.internal &&              hadoop fs -mkdir -p              /tmp/hadoop-yarn/staging/history /user/hdfs /user/mapred /user/yarn /user/spark /user/pig /user/hive /user/hbase /user/zookeeper hdfs://cluster-5cb0-m/user/spark/eventlog'
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 24.'
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 24.'
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 24.
<13>Oct 13 23:17:11 google-dataproc-startup[827]: <13>Oct 13 23:17:11 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:12 google-dataproc-startup[827]: <13>Oct 13 23:17:12 uninstall[1420]: Removing zookeeper-server (3.4.13-1) ...
<13>Oct 13 23:17:12 google-dataproc-startup[827]: <13>Oct 13 23:17:12 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:12 google-dataproc-startup[827]: <13>Oct 13 23:17:12 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:12 google-dataproc-startup[827]: <13>Oct 13 23:17:12 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:12 google-dataproc-startup[827]: <13>Oct 13 23:17:12 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 25.'
<13>Oct 13 23:17:12 google-dataproc-startup[827]: <13>Oct 13 23:17:12 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 25.'
<13>Oct 13 23:17:12 google-dataproc-startup[827]: <13>Oct 13 23:17:12 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 25.
<13>Oct 13 23:17:12 google-dataproc-startup[827]: <13>Oct 13 23:17:12 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 uninstall[1420]: Removing libgeoip1:amd64 (1.6.9-4) ...
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 uninstall[1420]: Removing libjs-jquery (3.1.1-2+deb9u1) ...
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 uninstall[1420]: Removing r-cran-rcpp (1.0.0-1~bpo9+1) ...
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 uninstall[1420]: Removing r-cran-cli (1.0.1-1~bpo9+1) ...
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 uninstall[1420]: Removing r-cran-assertthat (0.2.0-1~bpo9+1) ...
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 uninstall[1420]: Removing r-cran-crayon (1.3.4-2~bpo9+1) ...
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 uninstall[1420]: Removing r-cran-littler (0.3.1-1) ...
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 uninstall[1420]: Removing r-cran-pkgkitten (0.1.4-1) ...
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 26.'
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 26.'
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 26.
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:13 google-dataproc-startup[827]: <13>Oct 13 23:17:13 uninstall[1420]: Removing libverto1:amd64 (0.2.4-2.1) ...
<13>Oct 13 23:17:14 google-dataproc-startup[827]: <13>Oct 13 23:17:14 uninstall[1420]: Removing libverto-libev1:amd64 (0.2.4-2.1) ...
<13>Oct 13 23:17:14 google-dataproc-startup[827]: <13>Oct 13 23:17:14 uninstall[1420]: Removing libev4 (1:4.22-1+b1) ...
<13>Oct 13 23:17:14 google-dataproc-startup[827]: <13>Oct 13 23:17:14 uninstall[1420]: Processing triggers for libc-bin (2.24-11+deb9u4) ...
<13>Oct 13 23:17:14 google-dataproc-startup[827]: <13>Oct 13 23:17:14 uninstall[1420]: Processing triggers for man-db (2.7.6.1-2) ...
<13>Oct 13 23:17:14 google-dataproc-startup[827]: <13>Oct 13 23:17:14 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:14 google-dataproc-startup[827]: <13>Oct 13 23:17:14 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:15 google-dataproc-startup[827]: <13>Oct 13 23:17:15 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:15 google-dataproc-startup[827]: <13>Oct 13 23:17:15 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 27.'
<13>Oct 13 23:17:15 google-dataproc-startup[827]: <13>Oct 13 23:17:15 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 27.'
<13>Oct 13 23:17:15 google-dataproc-startup[827]: <13>Oct 13 23:17:15 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 27.
<13>Oct 13 23:17:15 google-dataproc-startup[827]: <13>Oct 13 23:17:15 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:16 google-dataproc-startup[827]: <13>Oct 13 23:17:16 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:16 google-dataproc-startup[827]: <13>Oct 13 23:17:16 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:16 google-dataproc-startup[827]: <13>Oct 13 23:17:16 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:16 google-dataproc-startup[827]: <13>Oct 13 23:17:16 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 28.'
<13>Oct 13 23:17:16 google-dataproc-startup[827]: <13>Oct 13 23:17:16 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 28.'
<13>Oct 13 23:17:16 google-dataproc-startup[827]: <13>Oct 13 23:17:16 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 28.
<13>Oct 13 23:17:16 google-dataproc-startup[827]: <13>Oct 13 23:17:16 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:17 google-dataproc-startup[827]: <13>Oct 13 23:17:17 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:17 google-dataproc-startup[827]: <13>Oct 13 23:17:17 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:17 google-dataproc-startup[827]: <13>Oct 13 23:17:17 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:17 google-dataproc-startup[827]: <13>Oct 13 23:17:17 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 29.'
<13>Oct 13 23:17:17 google-dataproc-startup[827]: <13>Oct 13 23:17:17 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 29.'
<13>Oct 13 23:17:17 google-dataproc-startup[827]: <13>Oct 13 23:17:17 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 29.
<13>Oct 13 23:17:17 google-dataproc-startup[827]: <13>Oct 13 23:17:17 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 30.'
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 30.'
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 30.
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 uninstall[1420]: Processing triggers for fontconfig (2.11.0-6.7+b1) ...
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: + run_with_retries sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: + local retry_backoff
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: + cmd=("$@")
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: + local -a cmd
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: + loginfo 'About to run '\''sudo -u hdfs hadoop fs -chmod -R 1777 /'\'' with retries...'
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: + echo 'About to run '\''sudo -u hdfs hadoop fs -chmod -R 1777 /'\'' with retries...'
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: About to run 'sudo -u hdfs hadoop fs -chmod -R 1777 /' with retries...
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: + local update_succeeded=0
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: + (( i = 0 ))
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: + (( i < 12 ))
<13>Oct 13 23:17:18 google-dataproc-startup[827]: <13>Oct 13 23:17:18 setup-hadoop-hdfs-namenode[1482]: + sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Oct 13 23:17:19 google-dataproc-startup[827]: <13>Oct 13 23:17:19 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:19 google-dataproc-startup[827]: <13>Oct 13 23:17:19 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:19 google-dataproc-startup[827]: <13>Oct 13 23:17:19 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:19 google-dataproc-startup[827]: <13>Oct 13 23:17:19 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 31.'
<13>Oct 13 23:17:19 google-dataproc-startup[827]: <13>Oct 13 23:17:19 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 31.'
<13>Oct 13 23:17:19 google-dataproc-startup[827]: <13>Oct 13 23:17:19 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 31.
<13>Oct 13 23:17:19 google-dataproc-startup[827]: <13>Oct 13 23:17:19 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:20 google-dataproc-startup[827]: <13>Oct 13 23:17:20 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:20 google-dataproc-startup[827]: <13>Oct 13 23:17:20 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:20 google-dataproc-startup[827]: <13>Oct 13 23:17:20 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:20 google-dataproc-startup[827]: <13>Oct 13 23:17:20 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 32.'
<13>Oct 13 23:17:20 google-dataproc-startup[827]: <13>Oct 13 23:17:20 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 32.'
<13>Oct 13 23:17:20 google-dataproc-startup[827]: <13>Oct 13 23:17:20 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 32.
<13>Oct 13 23:17:20 google-dataproc-startup[827]: <13>Oct 13 23:17:20 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:21 google-dataproc-startup[827]: <13>Oct 13 23:17:21 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:21 google-dataproc-startup[827]: <13>Oct 13 23:17:21 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:21 google-dataproc-startup[827]: <13>Oct 13 23:17:21 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:21 google-dataproc-startup[827]: <13>Oct 13 23:17:21 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 33.'
<13>Oct 13 23:17:21 google-dataproc-startup[827]: <13>Oct 13 23:17:21 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 33.'
<13>Oct 13 23:17:21 google-dataproc-startup[827]: <13>Oct 13 23:17:21 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 33.
<13>Oct 13 23:17:21 google-dataproc-startup[827]: <13>Oct 13 23:17:21 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:21 google-dataproc-startup[827]: <13>Oct 13 23:17:21 uninstall[1420]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 102578 files and directories currently installed.)
<13>Oct 13 23:17:21 google-dataproc-startup[827]: <13>Oct 13 23:17:21 uninstall[1420]: Purging configuration files for update-inetd (4.44) ...
<13>Oct 13 23:17:21 google-dataproc-startup[827]: <13>Oct 13 23:17:21 uninstall[1420]: Purging configuration files for hive-webhcat-server (2.3.5-1) ...
<13>Oct 13 23:17:22 google-dataproc-startup[827]: <13>Oct 13 23:17:22 uninstall[1420]: Purging configuration files for xinetd (1:2.3.15-7) ...
<13>Oct 13 23:17:22 google-dataproc-startup[827]: <13>Oct 13 23:17:22 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:22 google-dataproc-startup[827]: <13>Oct 13 23:17:22 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:22 google-dataproc-startup[827]: <13>Oct 13 23:17:22 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:22 google-dataproc-startup[827]: <13>Oct 13 23:17:22 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 34.'
<13>Oct 13 23:17:22 google-dataproc-startup[827]: <13>Oct 13 23:17:22 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 34.'
<13>Oct 13 23:17:22 google-dataproc-startup[827]: <13>Oct 13 23:17:22 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 34.
<13>Oct 13 23:17:22 google-dataproc-startup[827]: <13>Oct 13 23:17:22 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:22 google-dataproc-startup[827]: <13>Oct 13 23:17:22 uninstall[1420]: Purging configuration files for zookeeper-server (3.4.13-1) ...
<13>Oct 13 23:17:23 google-dataproc-startup[827]: <13>Oct 13 23:17:23 uninstall[1420]: Purging configuration files for hadoop-hdfs-journalnode (2.9.2-1) ...
<13>Oct 13 23:17:23 google-dataproc-startup[827]: <13>Oct 13 23:17:23 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:23 google-dataproc-startup[827]: <13>Oct 13 23:17:23 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:23 google-dataproc-startup[827]: <13>Oct 13 23:17:23 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:23 google-dataproc-startup[827]: <13>Oct 13 23:17:23 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 35.'
<13>Oct 13 23:17:23 google-dataproc-startup[827]: <13>Oct 13 23:17:23 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 35.'
<13>Oct 13 23:17:23 google-dataproc-startup[827]: <13>Oct 13 23:17:23 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 35.
<13>Oct 13 23:17:23 google-dataproc-startup[827]: <13>Oct 13 23:17:23 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:23 google-dataproc-startup[827]: <13>Oct 13 23:17:23 uninstall[1420]: Purging configuration files for solr (6.6.5-1) ...
<13>Oct 13 23:17:23 google-dataproc-startup[827]: <13>Oct 13 23:17:23 uninstall[1420]: Purging configuration files for krb5-admin-server (1.15-1+deb9u1) ...
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 36.'
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 36.'
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 36.
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + update_succeeded=1
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + break
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + ((  1  ))
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + run_with_retries sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-5cb0-m/user/spark/eventlog
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + local retry_backoff
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + cmd=("$@")
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + local -a cmd
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + loginfo 'About to run '\''sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-5cb0-m/user/spark/eventlog'\'' with retries...'
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + echo 'About to run '\''sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-5cb0-m/user/spark/eventlog'\'' with retries...'
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: About to run 'sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-5cb0-m/user/spark/eventlog' with retries...
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + local update_succeeded=0
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + (( i = 0 ))
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + (( i < 12 ))
<13>Oct 13 23:17:24 google-dataproc-startup[827]: <13>Oct 13 23:17:24 setup-hadoop-hdfs-namenode[1482]: + sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-5cb0-m/user/spark/eventlog
<13>Oct 13 23:17:25 google-dataproc-startup[827]: <13>Oct 13 23:17:25 uninstall[1420]: Purging configuration files for kafka (1.1.1-1) ...
<13>Oct 13 23:17:25 google-dataproc-startup[827]: <13>Oct 13 23:17:25 uninstall[1420]: Purging configuration files for hadoop-hdfs-zkfc (2.9.2-1) ...
<13>Oct 13 23:17:25 google-dataproc-startup[827]: <13>Oct 13 23:17:25 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:25 google-dataproc-startup[827]: <13>Oct 13 23:17:25 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:25 google-dataproc-startup[827]: <13>Oct 13 23:17:25 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:25 google-dataproc-startup[827]: <13>Oct 13 23:17:25 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 37.'
<13>Oct 13 23:17:25 google-dataproc-startup[827]: <13>Oct 13 23:17:25 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 37.'
<13>Oct 13 23:17:25 google-dataproc-startup[827]: <13>Oct 13 23:17:25 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 37.
<13>Oct 13 23:17:25 google-dataproc-startup[827]: <13>Oct 13 23:17:25 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:25 google-dataproc-startup[827]: <13>Oct 13 23:17:25 uninstall[1420]: Purging configuration files for krb5-kdc (1.15-1+deb9u1) ...
<13>Oct 13 23:17:26 google-dataproc-startup[827]: <13>Oct 13 23:17:26 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:26 google-dataproc-startup[827]: <13>Oct 13 23:17:26 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:26 google-dataproc-startup[827]: <13>Oct 13 23:17:26 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:26 google-dataproc-startup[827]: <13>Oct 13 23:17:26 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 38.'
<13>Oct 13 23:17:26 google-dataproc-startup[827]: <13>Oct 13 23:17:26 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 38.'
<13>Oct 13 23:17:26 google-dataproc-startup[827]: <13>Oct 13 23:17:26 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 38.
<13>Oct 13 23:17:26 google-dataproc-startup[827]: <13>Oct 13 23:17:26 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:27 google-dataproc-startup[827]: <13>Oct 13 23:17:27 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:27 google-dataproc-startup[827]: <13>Oct 13 23:17:27 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:27 google-dataproc-startup[827]: <13>Oct 13 23:17:27 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:27 google-dataproc-startup[827]: <13>Oct 13 23:17:27 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 39.'
<13>Oct 13 23:17:27 google-dataproc-startup[827]: <13>Oct 13 23:17:27 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 39.'
<13>Oct 13 23:17:27 google-dataproc-startup[827]: <13>Oct 13 23:17:27 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 39.
<13>Oct 13 23:17:27 google-dataproc-startup[827]: <13>Oct 13 23:17:27 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:27 google-dataproc-startup[827]: <13>Oct 13 23:17:27 uninstall[1420]: Purging configuration files for libjs-jquery-ui (1.12.1+dfsg-4) ...
<13>Oct 13 23:17:27 google-dataproc-startup[827]: <13>Oct 13 23:17:27 uninstall[1420]: Purging configuration files for krb5-kpropd (1.15-1+deb9u1) ...
<13>Oct 13 23:17:28 google-dataproc-startup[827]: <13>Oct 13 23:17:28 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:28 google-dataproc-startup[827]: <13>Oct 13 23:17:28 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:28 google-dataproc-startup[827]: <13>Oct 13 23:17:28 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:28 google-dataproc-startup[827]: <13>Oct 13 23:17:28 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 40.'
<13>Oct 13 23:17:28 google-dataproc-startup[827]: <13>Oct 13 23:17:28 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 40.'
<13>Oct 13 23:17:28 google-dataproc-startup[827]: <13>Oct 13 23:17:28 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 40.
<13>Oct 13 23:17:28 google-dataproc-startup[827]: <13>Oct 13 23:17:28 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:28 google-dataproc-startup[827]: <13>Oct 13 23:17:28 uninstall[1420]: Purging configuration files for hadoop-hdfs-datanode (2.9.2-1) ...
<13>Oct 13 23:17:28 google-dataproc-startup[827]: <13>Oct 13 23:17:28 uninstall[1420]: Purging configuration files for javascript-common (11) ...
<13>Oct 13 23:17:29 google-dataproc-startup[827]: <13>Oct 13 23:17:29 uninstall[1420]: Purging configuration files for krb5-config (2.6) ...
<13>Oct 13 23:17:29 google-dataproc-startup[827]: <13>Oct 13 23:17:29 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:29 google-dataproc-startup[827]: <13>Oct 13 23:17:29 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:29 google-dataproc-startup[827]: <13>Oct 13 23:17:29 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:29 google-dataproc-startup[827]: <13>Oct 13 23:17:29 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 41.'
<13>Oct 13 23:17:29 google-dataproc-startup[827]: <13>Oct 13 23:17:29 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 41.'
<13>Oct 13 23:17:29 google-dataproc-startup[827]: <13>Oct 13 23:17:29 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 41.
<13>Oct 13 23:17:29 google-dataproc-startup[827]: <13>Oct 13 23:17:29 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:29 google-dataproc-startup[827]: <13>Oct 13 23:17:29 uninstall[1420]: Purging configuration files for knox (1.1.0-1) ...
<13>Oct 13 23:17:29 google-dataproc-startup[827]: <13>Oct 13 23:17:29 uninstall[1420]: Purging configuration files for r-cran-rmarkdown (1.11+dfsg-1~bpo9+1) ...
<13>Oct 13 23:17:29 google-dataproc-startup[827]: <13>Oct 13 23:17:29 uninstall[1420]: Purging configuration files for hive-webhcat (2.3.5-1) ...
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 uninstall[1420]: Purging configuration files for solr-server (6.6.5-1) ...
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 uninstall[1420]: Purging configuration files for kafka-server (1.1.1-1) ...
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + update_succeeded=1
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + break
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + ((  1  ))
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + for SERVICE in "${DATAPROC_START_AFTER_HDFS_SERVICES[@]}"
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + run_with_retries systemctl start hadoop-mapreduce-historyserver
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + local retry_backoff
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + cmd=("$@")
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + local -a cmd
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + loginfo 'About to run '\''systemctl start hadoop-mapreduce-historyserver'\'' with retries...'
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + echo 'About to run '\''systemctl start hadoop-mapreduce-historyserver'\'' with retries...'
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: About to run 'systemctl start hadoop-mapreduce-historyserver' with retries...
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + local update_succeeded=0
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + (( i = 0 ))
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + (( i < 12 ))
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hadoop-hdfs-namenode[1482]: + systemctl start hadoop-mapreduce-historyserver
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 uninstall[1420]: Purging configuration files for zeppelin (0.8.0-1) ...
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 42.'
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 42.'
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 42.
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:30 google-dataproc-startup[827]: <13>Oct 13 23:17:30 uninstall[1420]: Purging configuration files for hadoop-yarn-nodemanager (2.9.2-1) ...
<13>Oct 13 23:17:31 google-dataproc-startup[827]: <13>Oct 13 23:17:31 uninstall[1420]: Processing triggers for systemd (232-25+deb9u12) ...
<13>Oct 13 23:17:31 google-dataproc-startup[827]: <13>Oct 13 23:17:31 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:31 google-dataproc-startup[827]: <13>Oct 13 23:17:31 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:31 google-dataproc-startup[827]: <13>Oct 13 23:17:31 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:31 google-dataproc-startup[827]: <13>Oct 13 23:17:31 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 43.'
<13>Oct 13 23:17:31 google-dataproc-startup[827]: <13>Oct 13 23:17:31 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 43.'
<13>Oct 13 23:17:31 google-dataproc-startup[827]: <13>Oct 13 23:17:31 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 43.
<13>Oct 13 23:17:31 google-dataproc-startup[827]: <13>Oct 13 23:17:31 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:32 google-dataproc-startup[827]: <13>Oct 13 23:17:32 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:32 google-dataproc-startup[827]: <13>Oct 13 23:17:32 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:32 google-dataproc-startup[827]: <13>Oct 13 23:17:32 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:32 google-dataproc-startup[827]: <13>Oct 13 23:17:32 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 44.'
<13>Oct 13 23:17:32 google-dataproc-startup[827]: <13>Oct 13 23:17:32 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 44.'
<13>Oct 13 23:17:32 google-dataproc-startup[827]: <13>Oct 13 23:17:32 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 44.
<13>Oct 13 23:17:32 google-dataproc-startup[827]: <13>Oct 13 23:17:32 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:33 google-dataproc-startup[827]: <13>Oct 13 23:17:33 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:33 google-dataproc-startup[827]: <13>Oct 13 23:17:33 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:33 google-dataproc-startup[827]: <13>Oct 13 23:17:33 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:33 google-dataproc-startup[827]: <13>Oct 13 23:17:33 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 45.'
<13>Oct 13 23:17:33 google-dataproc-startup[827]: <13>Oct 13 23:17:33 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 45.'
<13>Oct 13 23:17:33 google-dataproc-startup[827]: <13>Oct 13 23:17:33 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 45.
<13>Oct 13 23:17:33 google-dataproc-startup[827]: <13>Oct 13 23:17:33 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:34 google-dataproc-startup[827]: <13>Oct 13 23:17:34 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:34 google-dataproc-startup[827]: <13>Oct 13 23:17:34 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:34 google-dataproc-startup[827]: <13>Oct 13 23:17:34 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:17:34 google-dataproc-startup[827]: <13>Oct 13 23:17:34 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 46.'
<13>Oct 13 23:17:34 google-dataproc-startup[827]: <13>Oct 13 23:17:34 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 46.'
<13>Oct 13 23:17:34 google-dataproc-startup[827]: <13>Oct 13 23:17:34 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 failed. Retry attempt: 46.
<13>Oct 13 23:17:34 google-dataproc-startup[827]: <13>Oct 13 23:17:34 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 9083
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: Connection to cluster-5cb0-m 9083 port [tcp/*] succeeded!
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + update_succeeded=1
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 9083 succeeded.'
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 9083 succeeded.'
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 9083 succeeded.
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + break
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + ((  1  ))
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + loginfo 'Service up on host=cluster-5cb0-m port=9083.'
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + echo 'Service up on host=cluster-5cb0-m port=9083.'
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: Service up on host=cluster-5cb0-m port=9083.
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + run_with_retries systemctl start hive-server2
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + local retry_backoff
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + cmd=("$@")
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + local -a cmd
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + loginfo 'About to run '\''systemctl start hive-server2'\'' with retries...'
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + echo 'About to run '\''systemctl start hive-server2'\'' with retries...'
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: About to run 'systemctl start hive-server2' with retries...
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + local update_succeeded=0
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + (( i = 0 ))
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + (( i < 12 ))
<13>Oct 13 23:17:35 google-dataproc-startup[827]: <13>Oct 13 23:17:35 setup-hive-metastore[1484]: + systemctl start hive-server2
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + update_succeeded=1
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + break
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + ((  1  ))
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + for SERVICE in "${DATAPROC_START_AFTER_HDFS_SERVICES[@]}"
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + run_with_retries systemctl start spark-history-server
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + local retry_backoff
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + cmd=("$@")
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + local -a cmd
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + loginfo 'About to run '\''systemctl start spark-history-server'\'' with retries...'
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + echo 'About to run '\''systemctl start spark-history-server'\'' with retries...'
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: About to run 'systemctl start spark-history-server' with retries...
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + local update_succeeded=0
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + (( i = 0 ))
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + (( i < 12 ))
<13>Oct 13 23:17:36 google-dataproc-startup[827]: <13>Oct 13 23:17:36 setup-hadoop-hdfs-namenode[1482]: + systemctl start spark-history-server
<13>Oct 13 23:17:38 google-dataproc-startup[827]: <13>Oct 13 23:17:38 setup-hive-metastore[1484]: + update_succeeded=1
<13>Oct 13 23:17:38 google-dataproc-startup[827]: <13>Oct 13 23:17:38 setup-hive-metastore[1484]: + break
<13>Oct 13 23:17:38 google-dataproc-startup[827]: <13>Oct 13 23:17:38 setup-hive-metastore[1484]: + ((  1  ))
<13>Oct 13 23:17:38 google-dataproc-startup[827]: <13>Oct 13 23:17:38 setup-hive-metastore[1484]: ++ get_xml_property_or_default /etc/hive/conf/hive-site.xml hive.server2.thrift.port 10000
<13>Oct 13 23:17:38 google-dataproc-startup[827]: <13>Oct 13 23:17:38 setup-hive-metastore[1484]: ++ file=/etc/hive/conf/hive-site.xml
<13>Oct 13 23:17:38 google-dataproc-startup[827]: <13>Oct 13 23:17:38 setup-hive-metastore[1484]: ++ property=hive.server2.thrift.port
<13>Oct 13 23:17:38 google-dataproc-startup[827]: <13>Oct 13 23:17:38 setup-hive-metastore[1484]: ++ default_value=10000
<13>Oct 13 23:17:38 google-dataproc-startup[827]: <13>Oct 13 23:17:38 setup-hive-metastore[1484]: +++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.server2.thrift.port
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: ++ val=None
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: ++ [[ None = \N\o\n\e ]]
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: ++ val=10000
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: ++ echo 10000
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + thrift_port=10000
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + wait_for_port cluster-5cb0-m 10000
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + local -r host=cluster-5cb0-m
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + local -r port=10000
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + loginfo 'Waiting for service to come up on host=cluster-5cb0-m port=10000.'
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + echo 'Waiting for service to come up on host=cluster-5cb0-m port=10000.'
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: Waiting for service to come up on host=cluster-5cb0-m port=10000.
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + retry_with_constant_backoff nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + local max_retry=300
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + cmd=("$@")
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + local -a cmd
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + local update_succeeded=0
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: ++ seq 1 300
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 1.'
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 1.'
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 1.
<13>Oct 13 23:17:39 google-dataproc-startup[827]: <13>Oct 13 23:17:39 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: + update_succeeded=1
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: + break
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: + ((  1  ))
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: ++ local property_name=am.primary_only
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: +++ local property_name=am.primary_only
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: ++++ tail -n 1
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: ++++ cut -d = -f 2-
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: +++ local property_value=false
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: +++ echo false
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: ++ local property_value=false
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: ++ echo false
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hadoop-hdfs-namenode[1482]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 2.'
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 2.'
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 2.
<13>Oct 13 23:17:40 google-dataproc-startup[827]: <13>Oct 13 23:17:40 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:41 google-dataproc-startup[827]: <13>Oct 13 23:17:41 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:41 google-dataproc-startup[827]: <13>Oct 13 23:17:41 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:41 google-dataproc-startup[827]: <13>Oct 13 23:17:41 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:41 google-dataproc-startup[827]: <13>Oct 13 23:17:41 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 3.'
<13>Oct 13 23:17:41 google-dataproc-startup[827]: <13>Oct 13 23:17:41 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 3.'
<13>Oct 13 23:17:41 google-dataproc-startup[827]: <13>Oct 13 23:17:41 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 3.
<13>Oct 13 23:17:41 google-dataproc-startup[827]: <13>Oct 13 23:17:41 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:42 google-dataproc-startup[827]: <13>Oct 13 23:17:42 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:42 google-dataproc-startup[827]: <13>Oct 13 23:17:42 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:42 google-dataproc-startup[827]: <13>Oct 13 23:17:42 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:42 google-dataproc-startup[827]: <13>Oct 13 23:17:42 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 4.'
<13>Oct 13 23:17:42 google-dataproc-startup[827]: <13>Oct 13 23:17:42 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 4.'
<13>Oct 13 23:17:42 google-dataproc-startup[827]: <13>Oct 13 23:17:42 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 4.
<13>Oct 13 23:17:42 google-dataproc-startup[827]: <13>Oct 13 23:17:42 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:43 google-dataproc-startup[827]: <13>Oct 13 23:17:43 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:43 google-dataproc-startup[827]: <13>Oct 13 23:17:43 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:43 google-dataproc-startup[827]: <13>Oct 13 23:17:43 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:43 google-dataproc-startup[827]: <13>Oct 13 23:17:43 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 5.'
<13>Oct 13 23:17:43 google-dataproc-startup[827]: <13>Oct 13 23:17:43 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 5.'
<13>Oct 13 23:17:43 google-dataproc-startup[827]: <13>Oct 13 23:17:43 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 5.
<13>Oct 13 23:17:43 google-dataproc-startup[827]: <13>Oct 13 23:17:43 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:44 google-dataproc-startup[827]: <13>Oct 13 23:17:44 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:44 google-dataproc-startup[827]: <13>Oct 13 23:17:44 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:44 google-dataproc-startup[827]: <13>Oct 13 23:17:44 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:44 google-dataproc-startup[827]: <13>Oct 13 23:17:44 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 6.'
<13>Oct 13 23:17:44 google-dataproc-startup[827]: <13>Oct 13 23:17:44 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 6.'
<13>Oct 13 23:17:44 google-dataproc-startup[827]: <13>Oct 13 23:17:44 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 6.
<13>Oct 13 23:17:44 google-dataproc-startup[827]: <13>Oct 13 23:17:44 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:45 google-dataproc-startup[827]: <13>Oct 13 23:17:45 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:45 google-dataproc-startup[827]: <13>Oct 13 23:17:45 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:45 google-dataproc-startup[827]: <13>Oct 13 23:17:45 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:45 google-dataproc-startup[827]: <13>Oct 13 23:17:45 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 7.'
<13>Oct 13 23:17:45 google-dataproc-startup[827]: <13>Oct 13 23:17:45 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 7.'
<13>Oct 13 23:17:45 google-dataproc-startup[827]: <13>Oct 13 23:17:45 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 7.
<13>Oct 13 23:17:45 google-dataproc-startup[827]: <13>Oct 13 23:17:45 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:46 google-dataproc-startup[827]: <13>Oct 13 23:17:46 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:46 google-dataproc-startup[827]: <13>Oct 13 23:17:46 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:46 google-dataproc-startup[827]: <13>Oct 13 23:17:46 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:46 google-dataproc-startup[827]: <13>Oct 13 23:17:46 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 8.'
<13>Oct 13 23:17:46 google-dataproc-startup[827]: <13>Oct 13 23:17:46 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 8.'
<13>Oct 13 23:17:46 google-dataproc-startup[827]: <13>Oct 13 23:17:46 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 8.
<13>Oct 13 23:17:46 google-dataproc-startup[827]: <13>Oct 13 23:17:46 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:47 google-dataproc-startup[827]: <13>Oct 13 23:17:47 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:47 google-dataproc-startup[827]: <13>Oct 13 23:17:47 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:47 google-dataproc-startup[827]: <13>Oct 13 23:17:47 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:47 google-dataproc-startup[827]: <13>Oct 13 23:17:47 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 9.'
<13>Oct 13 23:17:47 google-dataproc-startup[827]: <13>Oct 13 23:17:47 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 9.'
<13>Oct 13 23:17:47 google-dataproc-startup[827]: <13>Oct 13 23:17:47 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 9.
<13>Oct 13 23:17:47 google-dataproc-startup[827]: <13>Oct 13 23:17:47 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:48 google-dataproc-startup[827]: <13>Oct 13 23:17:48 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:48 google-dataproc-startup[827]: <13>Oct 13 23:17:48 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:48 google-dataproc-startup[827]: <13>Oct 13 23:17:48 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:48 google-dataproc-startup[827]: <13>Oct 13 23:17:48 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 10.'
<13>Oct 13 23:17:48 google-dataproc-startup[827]: <13>Oct 13 23:17:48 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 10.'
<13>Oct 13 23:17:48 google-dataproc-startup[827]: <13>Oct 13 23:17:48 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 10.
<13>Oct 13 23:17:48 google-dataproc-startup[827]: <13>Oct 13 23:17:48 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:49 google-dataproc-startup[827]: <13>Oct 13 23:17:49 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:49 google-dataproc-startup[827]: <13>Oct 13 23:17:49 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:49 google-dataproc-startup[827]: <13>Oct 13 23:17:49 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:49 google-dataproc-startup[827]: <13>Oct 13 23:17:49 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 11.'
<13>Oct 13 23:17:49 google-dataproc-startup[827]: <13>Oct 13 23:17:49 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 11.'
<13>Oct 13 23:17:49 google-dataproc-startup[827]: <13>Oct 13 23:17:49 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 11.
<13>Oct 13 23:17:49 google-dataproc-startup[827]: <13>Oct 13 23:17:49 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:50 google-dataproc-startup[827]: <13>Oct 13 23:17:50 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:50 google-dataproc-startup[827]: <13>Oct 13 23:17:50 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:50 google-dataproc-startup[827]: <13>Oct 13 23:17:50 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:50 google-dataproc-startup[827]: <13>Oct 13 23:17:50 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 12.'
<13>Oct 13 23:17:50 google-dataproc-startup[827]: <13>Oct 13 23:17:50 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 12.'
<13>Oct 13 23:17:50 google-dataproc-startup[827]: <13>Oct 13 23:17:50 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 12.
<13>Oct 13 23:17:50 google-dataproc-startup[827]: <13>Oct 13 23:17:50 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:51 google-dataproc-startup[827]: <13>Oct 13 23:17:51 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:51 google-dataproc-startup[827]: <13>Oct 13 23:17:51 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:51 google-dataproc-startup[827]: <13>Oct 13 23:17:51 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:51 google-dataproc-startup[827]: <13>Oct 13 23:17:51 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 13.'
<13>Oct 13 23:17:51 google-dataproc-startup[827]: <13>Oct 13 23:17:51 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 13.'
<13>Oct 13 23:17:51 google-dataproc-startup[827]: <13>Oct 13 23:17:51 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 13.
<13>Oct 13 23:17:51 google-dataproc-startup[827]: <13>Oct 13 23:17:51 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:52 google-dataproc-startup[827]: <13>Oct 13 23:17:52 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:52 google-dataproc-startup[827]: <13>Oct 13 23:17:52 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:52 google-dataproc-startup[827]: <13>Oct 13 23:17:52 setup-hive-metastore[1484]: nc: connect to cluster-5cb0-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:17:52 google-dataproc-startup[827]: <13>Oct 13 23:17:52 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 14.'
<13>Oct 13 23:17:52 google-dataproc-startup[827]: <13>Oct 13 23:17:52 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 14.'
<13>Oct 13 23:17:52 google-dataproc-startup[827]: <13>Oct 13 23:17:52 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 failed. Retry attempt: 14.
<13>Oct 13 23:17:52 google-dataproc-startup[827]: <13>Oct 13 23:17:52 setup-hive-metastore[1484]: + sleep 1
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + nc -v -z -w 0 cluster-5cb0-m 10000
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: Connection to cluster-5cb0-m 10000 port [tcp/webmin] succeeded!
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + update_succeeded=1
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + loginfo 'nc -v -z -w 0 cluster-5cb0-m 10000 succeeded.'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + echo 'nc -v -z -w 0 cluster-5cb0-m 10000 succeeded.'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: nc -v -z -w 0 cluster-5cb0-m 10000 succeeded.
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + break
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + ((  1  ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + loginfo 'Service up on host=cluster-5cb0-m port=10000.'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + echo 'Service up on host=cluster-5cb0-m port=10000.'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: Service up on host=cluster-5cb0-m port=10000.
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: ++ local property_name=am.primary_only
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: +++ local property_name=am.primary_only
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: ++++ tail -n 1
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: ++++ cut -d = -f 2-
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: +++ local property_value=false
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: +++ echo false
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: ++ local property_value=false
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: ++ echo false
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:17:53 google-dataproc-startup[827]: <13>Oct 13 23:17:53 setup-hive-metastore[1484]: + [[ hive-metastore == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + pid=1483
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + cmd='setup_service hadoop-yarn-resourcemanager'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1483 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + echo 'Waiting on pid=1483 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: Waiting on pid=1483 cmd=[setup_service hadoop-yarn-resourcemanager]
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + wait 1483
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + pid=1482
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + cmd='setup_service hadoop-hdfs-namenode'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1482 cmd=[setup_service hadoop-hdfs-namenode]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + echo 'Waiting on pid=1482 cmd=[setup_service hadoop-hdfs-namenode]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: Waiting on pid=1482 cmd=[setup_service hadoop-hdfs-namenode]
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + wait 1482
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + pid=1425
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + cmd='uninstall_component proxy-agent'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1425 cmd=[uninstall_component proxy-agent]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + echo 'Waiting on pid=1425 cmd=[uninstall_component proxy-agent]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: Waiting on pid=1425 cmd=[uninstall_component proxy-agent]
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + wait 1425
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + pid=1424
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + cmd='uninstall_component presto'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1424 cmd=[uninstall_component presto]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + echo 'Waiting on pid=1424 cmd=[uninstall_component presto]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: Waiting on pid=1424 cmd=[uninstall_component presto]
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + wait 1424
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + pid=1423
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + cmd='uninstall_component kerberos'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1423 cmd=[uninstall_component kerberos]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + echo 'Waiting on pid=1423 cmd=[uninstall_component kerberos]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: Waiting on pid=1423 cmd=[uninstall_component kerberos]
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + wait 1423
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + pid=1422
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + cmd='uninstall_component jupyter'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1422 cmd=[uninstall_component jupyter]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + echo 'Waiting on pid=1422 cmd=[uninstall_component jupyter]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: Waiting on pid=1422 cmd=[uninstall_component jupyter]
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + wait 1422
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + pid=1421
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + cmd='uninstall_component anaconda'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1421 cmd=[uninstall_component anaconda]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + echo 'Waiting on pid=1421 cmd=[uninstall_component anaconda]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: Waiting on pid=1421 cmd=[uninstall_component anaconda]
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + wait 1421
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + pid=1420
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + cmd='bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + loginfo 'Waiting on pid=1420 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + echo 'Waiting on pid=1420 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]'
<13>Oct 13 23:17:53 google-dataproc-startup[827]: Waiting on pid=1420 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + status=0
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + wait 1420
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( status != 0 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( ++i  ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + (( i < 16 ))
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + BACKGROUND_PROCESSES=()
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + BACKGROUND_COMMANDS=()
<13>Oct 13 23:17:53 google-dataproc-startup[827]: + systemctl daemon-reload
<13>Oct 13 23:17:54 google-dataproc-startup[827]: + loginfo 'All done'
<13>Oct 13 23:17:54 google-dataproc-startup[827]: + echo 'All done'
<13>Oct 13 23:17:54 google-dataproc-startup[827]: All done
