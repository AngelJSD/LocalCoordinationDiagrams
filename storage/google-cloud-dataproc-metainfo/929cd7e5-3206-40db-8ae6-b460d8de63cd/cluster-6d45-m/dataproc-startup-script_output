+ run_with_logger --tag google-dataproc-startup
+ local tag=
+ local pid=811
+ [[ --tag == \-\-\t\a\g ]]
+ tag=google-dataproc-startup
+ shift 2
+ exec
++ logger -s -t 'google-dataproc-startup[811]'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=()
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=()
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + cd /tmp
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ INSTALL_GCS_CONNECTOR=1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ ENABLE_HDFS=1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ ENABLE_NFS_GCS_FILE_CACHE=1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ GCS_ADMIN=gcsadmin
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ GCS_CACHE_CLEANER_LOG_DIRECTORY=/var/log/hadoop-hdfs
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,DRFA
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,RFA
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ NUM_WORKERS=10
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ WORKERS=()
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ CORES_PER_MAP_TASK=1.0
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ CORES_PER_APP_MASTER=2.0
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ HDFS_DATA_DIRS_PERM=700
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ COMMON_JAVA_OPTS=("-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance")
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ DATAPROC_VERSION_KEY=1_3
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ declare -A DATAPROC_TO_GCS_CONNECTOR_VERSION
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ DATAPROC_TO_GCS_CONNECTOR_VERSION=(["1_0"]="1.6.10-hadoop2" ["1_1"]="1.6.10-hadoop2" ["1_2"]="1.6.10-hadoop2" ["1_3"]="hadoop2-1.9.17" ["1_4"]="hadoop2-1.9.17" ["1_5"]="hadoop2-1.9.17" ["2_0"]="hadoop3-1.9.17")
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ GCS_CONNECTOR_VERSION=hadoop2-1.9.17
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ GCS_CONNECTOR_JAR=https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-1.9.17.jar
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ /usr/share/google/get_metadata_value attributes/dataproc_gcs_connector_url
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ true
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ GCS_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ [[ ! -z '' ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ declare -A DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ DATAPROC_TO_BQ_CONNECTOR_VERSION=(["1_0"]="0.10.11-hadoop2" ["1_1"]="0.10.11-hadoop2" ["1_2"]="0.10.11-hadoop2")
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ BIGQUERY_CONNECTOR_VERSION=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ [[ -n '' ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ /usr/share/google/get_metadata_value attributes/dataproc_bq_connector_url
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ true
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ BIGQUERY_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ [[ ! -z '' ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ unset DATAPROC_VERSION_KEY GCS_CONNECTOR_VERSION BIGQUERY_CONNECTOR_VERSION DATAPROC_TO_GCS_CONNECTOR_VERSION DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ dpkg -s hive
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ HIVE_VERSION=2.3.5
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ dpkg -s spark-core
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ SPARK_VERSION=2.3.3
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ readonly APT_SENTINEL=apt.lastupdate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ APT_SENTINEL=apt.lastupdate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + source /usr/local/share/google/dataproc/bdutil/cluster_properties.sh
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + source /usr/local/share/google/dataproc/bdutil/components/startup-script-components.sh
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ readonly COMPONENTS_DIR=/var/lib/google/dataproc/components
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ COMPONENTS_DIR=/var/lib/google/dataproc/components
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + trap logstacktrace ERR
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + loginfo 'Starting Dataproc startup script'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + echo 'Starting Dataproc startup script'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: Starting Dataproc startup script
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ hostname -s
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + MY_HOSTNAME=cluster-6d45-m
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ hostname -f
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ dnsdomainname
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + DOMAIN=us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ sed -r 's/-([mw](-[0-9]*)?)$//'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ echo cluster-6d45-m
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + PREFIX=cluster-6d45
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + merge_java_properties /tmp/cluster/properties/dataproc.properties /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + local dest=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/dataproc.properties ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + cat /tmp/cluster/properties/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: Merged /tmp/cluster/properties/dataproc.properties.
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ local property_name=dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_name=dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_value=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ echo ''
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ local property_value=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ echo ''
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + OPTIONAL_COMPONENTS_VALUE=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + is_version_at_least 1.3 1.4
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + local ver2=1.4
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + local log=/tmp/tmp.dCrH4ksHOu
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + err_code=1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.dCrH4ksHOu
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + rm -f /tmp/tmp.dCrH4ksHOu
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + DATAPROC_MASTER=cluster-6d45-m
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master-additional
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + DATAPROC_MASTER_ADDITIONAL=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + MASTER_COUNT=1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + ((  1 > 1  ))
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + is_component_selected kafka-server
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + local component=kafka-server
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ local property_name=dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_name=dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_value=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ echo ''
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ local property_value=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ echo ''
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + local activated_components=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + [[ '' == *kafka-server* ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + COMPONENTS_TO_ACTIVATE=(${OPTIONAL_COMPONENTS_VALUE})
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + KERBEROS_ENABLED=false
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + is_component_selected kerberos
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + local component=kerberos
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ local property_name=dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_name=dataproc.components.activate
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_value=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ echo ''
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ local property_value=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ echo ''
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + local activated_components=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + [[ '' == *kerberos* ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ /usr/share/google/get_metadata_value ../project/project-id
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + PROJECT=lustrous-drake-255300
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ /usr/share/google/get_metadata_value attributes/dataproc-bucket
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + CONFIGBUCKET=dataproc-ed3c3d29-fb10-47bb-aca7-dcc358c68973-us-central1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ /usr/share/google/get_metadata_value attributes/dataproc-role
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + ROLE=Master
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-name
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + CLUSTER_NAME=cluster-6d45
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-uuid
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + CLUSTER_UUID=929cd7e5-3206-40db-8ae6-b460d8de63cd
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ /usr/share/google/get_metadata_value attributes/dataproc-worker-count
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + WORKER_COUNT=6
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + HBASE_CONF_DIR=/etc/hbase/conf
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + PIG_CONF_DIR=/etc/pig/conf
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + SPARK_CONF_DIR=/etc/spark/conf
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + ZOOKEEPER_CONF_DIR=/etc/zookeeper/conf
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + HADOOP_2_PORTS=(50010 50020 50070 50090)
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + ((  1 > 1  ))
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + HDFS_ROOT_URI=hdfs://cluster-6d45-m
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + hostname=cluster-6d45-m
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + for i in "${!MASTER_HOSTNAMES[@]}"
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + [[ cluster-6d45-m == \c\l\u\s\t\e\r\-\6\d\4\5\-\m ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + MASTER_INDEX=0
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + break
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + ((  6 == 0  ))
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + ((  1 > 1  ))
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + PACKAGES_TO_UNINSTALL=(${DATAPROC_MASTER_HA_SERVICES} ${DATAPROC_WORKER_SERVICES})
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + SERVICES=(${DATAPROC_MASTER_SERVICES} ${DATAPROC_MASTER_EXCLUSIVE_SERVICES} ${DATAPROC_MASTER_STANDALONE_SERVICES})
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + loginfo 'Generating helper scripts'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + echo 'Generating helper scripts'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: Generating helper scripts
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + sudo -u hdfs hdfs namenode -genclusterid
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ cat /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ (( i=0 ))
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ (( i<1 ))
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ echo MASTER_HOSTNAME_0=cluster-6d45-m
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ (( i++  ))
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ (( i<1 ))
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ cat /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ cat /usr/local/share/google/dataproc/bdutil/setup_master_nfs.sh /usr/local/share/google/dataproc/bdutil/setup_client_nfs.sh
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ cat /usr/local/share/google/dataproc/bdutil/configure_hadoop.sh /usr/local/share/google/dataproc/bdutil/configure_mysql.sh /usr/local/share/google/dataproc/bdutil/configure_hive.sh /usr/local/share/google/dataproc/bdutil/configure_hdfs.sh /usr/local/share/google/dataproc/bdutil/configure_connectors.sh /usr/local/share/google/dataproc/bdutil/configure_spark.sh /usr/local/share/google/dataproc/bdutil/configure_tez.sh /usr/local/share/google/dataproc/bdutil/configure_zookeeper.sh
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + cp -r /usr/local/share/google/dataproc/bdutil/conf/bq-mapred-template.xml /usr/local/share/google/dataproc/bdutil/conf/capacity-scheduler-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-template.xml /usr/local/share/google/dataproc/bdutil/conf/distcp-template.xml /usr/local/share/google/dataproc/bdutil/conf/gcs-core-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-template.xml /usr/local/share/google/dataproc/bdutil/conf/hive-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hive-template.xml /usr/local/share/google/dataproc/bdutil/conf/knox /usr/local/share/google/dataproc/bdutil/conf/mapred-template.xml /usr/local/share/google/dataproc/b
<13>Oct 13 23:00:29 google-dataproc-startup[811]: dutil/conf/yarn-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-template.xml /tmp
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + cp /usr/local/share/google/dataproc/bdutil/configure_mrv2_mem.py /tmp
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + chmod +x configure_mrv2_mem.py
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + loginfo 'Running helper scripts'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + echo 'Running helper scripts'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: Running helper scripts
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ get_dataproc_property dataproc.localssd.mount.enable
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ local property_name=dataproc.localssd.mount.enable
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.localssd.mount.enable
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_name=dataproc.localssd.mount.enable
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++++ grep '^dataproc.localssd.mount.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ local property_value=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: +++ echo ''
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ local property_value=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: ++ echo ''
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + MOUNT_DISKS_ENABLED=
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + [[ '' == \f\a\l\s\e ]]
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + DATAPROC_MOUNT_SERVICE_FILE=/usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + chmod +x /usr/local/share/google/dataproc/bdutil/mount_disks.sh
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + chmod 644 /usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Oct 13 23:00:29 google-dataproc-startup[811]: + systemctl enable google-dataproc-disk-mount
<13>Oct 13 23:00:29 google-dataproc-startup[811]: Created symlink /etc/systemd/system/multi-user.target.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 23:00:29 google-dataproc-startup[811]: Created symlink /etc/systemd/system/hadoop-hdfs-namenode.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 23:00:29 google-dataproc-startup[811]: Created symlink /etc/systemd/system/hadoop-hdfs-datanode.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 23:00:29 google-dataproc-startup[811]: Created symlink /etc/systemd/system/hadoop-yarn-resourcemanager.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 23:00:29 google-dataproc-startup[811]: Created symlink /etc/systemd/system/hadoop-yarn-nodemanager.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + systemctl start google-dataproc-disk-mount
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + in_array nfs-kernel-server DATAPROC_MASTER_EXCLUSIVE_SERVICES
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + local value=nfs-kernel-server
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + local -n values=DATAPROC_MASTER_EXCLUSIVE_SERVICES
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + [[ !  hadoop-mapreduce-historyserver spark-history-server hive-webhcat-server jupyter knox proxy-agent zeppelin hadoop-yarn-timelineserver mariadb-server  =~  nfs-kernel-server  ]]
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + bash configuration_script.sh
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ INSTALL_GCS_CONNECTOR=1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ ENABLE_HDFS=1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ ENABLE_NFS_GCS_FILE_CACHE=1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ GCS_ADMIN=gcsadmin
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ GCS_CACHE_CLEANER_LOG_DIRECTORY=/var/log/hadoop-hdfs
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,DRFA
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,RFA
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ NUM_WORKERS=10
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ WORKERS=()
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ CORES_PER_MAP_TASK=1.0
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ CORES_PER_APP_MASTER=2.0
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ HDFS_DATA_DIRS_PERM=700
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ COMMON_JAVA_OPTS=("-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance")
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ DATAPROC_VERSION_KEY=1_3
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ declare -A DATAPROC_TO_GCS_CONNECTOR_VERSION
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ DATAPROC_TO_GCS_CONNECTOR_VERSION=(["1_0"]="1.6.10-hadoop2" ["1_1"]="1.6.10-hadoop2" ["1_2"]="1.6.10-hadoop2" ["1_3"]="hadoop2-1.9.17" ["1_4"]="hadoop2-1.9.17" ["1_5"]="hadoop2-1.9.17" ["2_0"]="hadoop3-1.9.17")
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ GCS_CONNECTOR_VERSION=hadoop2-1.9.17
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ GCS_CONNECTOR_JAR=https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-1.9.17.jar
<13>Oct 13 23:00:30 google-dataproc-startup[811]: +++ /usr/share/google/get_metadata_value attributes/dataproc_gcs_connector_url
<13>Oct 13 23:00:30 google-dataproc-startup[811]: +++ true
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ GCS_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ [[ ! -z '' ]]
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ declare -A DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ DATAPROC_TO_BQ_CONNECTOR_VERSION=(["1_0"]="0.10.11-hadoop2" ["1_1"]="0.10.11-hadoop2" ["1_2"]="0.10.11-hadoop2")
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ BIGQUERY_CONNECTOR_VERSION=
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ [[ -n '' ]]
<13>Oct 13 23:00:30 google-dataproc-startup[811]: +++ /usr/share/google/get_metadata_value attributes/dataproc_bq_connector_url
<13>Oct 13 23:00:30 google-dataproc-startup[811]: +++ true
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ BIGQUERY_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ [[ ! -z '' ]]
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ unset DATAPROC_VERSION_KEY GCS_CONNECTOR_VERSION BIGQUERY_CONNECTOR_VERSION DATAPROC_TO_GCS_CONNECTOR_VERSION DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Oct 13 23:00:30 google-dataproc-startup[811]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 23:00:30 google-dataproc-startup[811]: +++ dpkg -s hive
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ HIVE_VERSION=2.3.5
<13>Oct 13 23:00:30 google-dataproc-startup[811]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 23:00:30 google-dataproc-startup[811]: +++ dpkg -s spark-core
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ SPARK_VERSION=2.3.3
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ CLUSTER_NAME=cluster-6d45
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ CLUSTER_UUID=929cd7e5-3206-40db-8ae6-b460d8de63cd
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ CONFIGBUCKET=dataproc-ed3c3d29-fb10-47bb-aca7-dcc358c68973-us-central1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ HDFS_ROOT_URI=hdfs://cluster-6d45-m
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ MASTER_HOSTNAME_0=cluster-6d45-m
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ MASTER_HOSTNAMES=(cluster-6d45-m)
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ NUM_MASTERS=1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ NUM_WORKERS=6
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ PREFIX=cluster-6d45
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ PROJECT=lustrous-drake-255300
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ ROLE=Master
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ set +a
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ readonly APT_SENTINEL=apt.lastupdate
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ APT_SENTINEL=apt.lastupdate
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + set -e
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + loginfo 'Running configure_hadoop.sh'
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + echo 'Running configure_hadoop.sh'
<13>Oct 13 23:00:30 google-dataproc-startup[811]: Running configure_hadoop.sh
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + export HADOOP_TMP_DIR=/hadoop/tmp
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + HADOOP_TMP_DIR=/hadoop/tmp
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + mkdir -p /hadoop/tmp
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + export DEFAULT_NUM_MAPS=60
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + DEFAULT_NUM_MAPS=60
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + export DEFAULT_NUM_REDUCES=24
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + DEFAULT_NUM_REDUCES=24
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ grep -c processor /proc/cpuinfo
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + export NUM_CORES=1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + NUM_CORES=1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ python -c 'print int(1 //     1.0)'
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + export MAP_SLOTS=1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + MAP_SLOTS=1
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ python -c 'print int(1 //     2.0)'
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + export REDUCE_SLOTS=0
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + REDUCE_SLOTS=0
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ awk '/^Mem:/{print $2}'
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ free -m
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + TOTAL_MEM=3704
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ python -c 'print int(3704 *     0.4)'
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + HADOOP_MR_MASTER_MEM_MB=1481
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + [[ -x configure_mrv2_mem.py ]]
<13>Oct 13 23:00:30 google-dataproc-startup[811]: ++ mktemp /tmp/mrv2_XXX_tmp_env.sh
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + TEMP_ENV_FILE=/tmp/mrv2_IOy_tmp_env.sh
<13>Oct 13 23:00:30 google-dataproc-startup[811]: + ./configure_mrv2_mem.py --output_file /tmp/mrv2_IOy_tmp_env.sh --total_memory 3704 --available_memory_ratio 0.8 --total_cores 1 --cores_per_map 1.0 --cores_per_reduce 2.0 --cores_per_app_master 2.0
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + source /tmp/mrv2_IOy_tmp_env.sh
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export YARN_MIN_MEM_MB=256
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ YARN_MIN_MEM_MB=256
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export YARN_MAX_MEM_MB=2816
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ YARN_MAX_MEM_MB=2816
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export NODEMANAGER_MEM_MB=2816
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ NODEMANAGER_MEM_MB=2816
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export APP_MASTER_MEM_MB=2816
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ APP_MASTER_MEM_MB=2816
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export CORES_PER_APP_MASTER_ROUNDED=1
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ CORES_PER_APP_MASTER_ROUNDED=1
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export APP_MASTER_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ APP_MASTER_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export MAP_MEM_MB=2816
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ MAP_MEM_MB=2816
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ CORES_PER_MAP_ROUNDED=1
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export MAP_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ MAP_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export REDUCE_MEM_MB=2816
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ REDUCE_MEM_MB=2816
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export CORES_PER_REDUCE_ROUNDED=1
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ CORES_PER_REDUCE_ROUNDED=1
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ export REDUCE_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ REDUCE_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ python -c 'print int(3704 / 4)'
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + HADOOP_CLIENT_MEM_MB=926
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + is_version_at_least 1.3 1.4
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + local ver2=1.4
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + local log=/tmp/tmp.l8jz697Itt
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + err_code=1
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.l8jz697Itt
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + rm -f /tmp/tmp.l8jz697Itt
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + DATA_DIRS=($(get_data_dirs))
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ get_data_dirs
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ mount_points=($(find /mnt/[0-9]*/ -maxdepth 0 || true))
<13>Oct 13 23:00:31 google-dataproc-startup[811]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Oct 13 23:00:31 google-dataproc-startup[811]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Oct 13 23:00:31 google-dataproc-startup[811]: +++ true
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ local mount_points
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ ((  0  ))
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ echo /
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ return
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + mkdir -p /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + chgrp hadoop -L -R /hadoop /hadoop/tmp /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + chmod g+rwx -R /hadoop /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + chmod 777 -R /hadoop/tmp
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + export MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + export NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + YARN_ENV_FILE=/etc/hadoop/conf/yarn-env.sh
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + [[ -f /etc/hadoop/conf/yarn-env.sh ]]
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ get_dataproc_property simplified.scaling.enable
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ local property_name=simplified.scaling.enable
<13>Oct 13 23:00:31 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Oct 13 23:00:31 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:31 google-dataproc-startup[811]: +++ local property_name=simplified.scaling.enable
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:31 google-dataproc-startup[811]: +++ local property_value=
<13>Oct 13 23:00:31 google-dataproc-startup[811]: +++ echo ''
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ local property_value=
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ echo ''
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + [[ '' == \t\r\u\e ]]
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + touch /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + chown root:hadoop /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + [[ 1 -gt 1 ]]
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + CORE_TEMPLATE=core-template.xml
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + YARN_TEMPLATE=yarn-template.xml
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file core-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file mapred-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file yarn-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file capacity-scheduler-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file distcp-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + is_version_at_least 1.3 1.4
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + local ver2=1.4
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + local log=/tmp/tmp.MqPTU3hJeu
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + err_code=1
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.MqPTU3hJeu
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + rm -f /tmp/tmp.MqPTU3hJeu
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value false --clobber
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + ZK_QUORUM=cluster-6d45-m:2181,:2181,:2181
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + [[ 1 -gt 1 ]]
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + is_version_at_least 1.3 1.2
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + local ver2=1.2
<13>Oct 13 23:00:31 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + local log=/tmp/tmp.5pYzw573eE
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.2
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + err_code=0
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.5pYzw573eE
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + rm -f /tmp/tmp.5pYzw573eE
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + return 0
<13>Oct 13 23:00:31 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.file.close.num-committed-allowed --value 1 --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + is_version_at_least 1.3 1.3
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local ver2=1.3
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local log=/tmp/tmp.j28FBjfuz4
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + err_code=0
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.j28FBjfuz4
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + rm -f /tmp/tmp.j28FBjfuz4
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + return 0
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/core-site.xml --name hadoop.http.filter.initializers --value org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.webapp.cross-origin.enabled --value true --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.http-cross-origin.enabled --value true --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.enabled --value true --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.hostname --value cluster-6d45-m --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.bind-host --value 0.0.0.0 --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.system-metrics-publisher.enabled --value true --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.generic-application-history.enabled --value true --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ local property_name=am.primary_only
<13>Oct 13 23:00:32 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:00:32 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:32 google-dataproc-startup[811]: +++ local property_name=am.primary_only
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:32 google-dataproc-startup[811]: +++ local property_value=false
<13>Oct 13 23:00:32 google-dataproc-startup[811]: +++ echo false
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ local property_value=false
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ echo false
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ /usr/share/google/get_metadata_value attributes/dataproc-datanode-enabled
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + DATAPROC_DATANODE_ENABLED=true
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + [[ '' == \t\r\u\e ]]
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + set -e -x
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + xargs -n1 sed -i 's/^\(bind-address\)\s*=.*/\1 = 0.0.0.0/'
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + grep -lr bind-address /etc/mysql
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + set -e -x
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + is_version_at_least 1.3 1.3
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local ver2=1.3
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local log=/tmp/tmp.aCekq25eKz
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + err_code=0
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.aCekq25eKz
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + rm -f /tmp/tmp.aCekq25eKz
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + return 0
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + is_version_at_least 1.3 1.4
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local ver2=1.4
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local log=/tmp/tmp.R0IxLu8Kc7
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + err_code=1
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.R0IxLu8Kc7
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + rm -f /tmp/tmp.R0IxLu8Kc7
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + [[ 1 -gt 1 ]]
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + METASTORE_URIS=thrift://cluster-6d45-m:9083
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris --value thrift://cluster-6d45-m:9083 --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + METADATA_STORE=jdbc:mysql://cluster-6d45-m/metastore
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name javax.jdo.option.ConnectionURL --value jdbc:mysql://cluster-6d45-m/metastore --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + [[ 1 -gt 1 ]]
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + set -e
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + loginfo 'Running configure_hdfs.sh'
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + echo 'Running configure_hdfs.sh'
<13>Oct 13 23:00:32 google-dataproc-startup[811]: Running configure_hdfs.sh
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + HDFS_ADMIN=hdfs
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + export HDFS_NAME_DIR=/hadoop/dfs/name
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + HDFS_NAME_DIR=/hadoop/dfs/name
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + export HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + DATA_DIRS=($(get_data_dirs))
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ get_data_dirs
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ mount_points=($(find /mnt/[0-9]*/ -maxdepth 0 || true))
<13>Oct 13 23:00:32 google-dataproc-startup[811]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Oct 13 23:00:32 google-dataproc-startup[811]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Oct 13 23:00:32 google-dataproc-startup[811]: +++ true
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ local mount_points
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ ((  0  ))
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ echo /
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ return
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + mkdir -p /hadoop/dfs /hadoop/dfs/data
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + chown hdfs:hadoop -L -R /hadoop/dfs /hadoop/dfs/data
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + chmod 700 /hadoop/dfs/data
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ awk '/^Mem:/{print $2}'
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ free -m
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + TOTAL_MEM=3704
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ python -c 'print int(3704 *     0.4 / 2)'
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + NAMENODE_MEM_MB=740
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + SECONDARYNAMENODE_MEM_MB=740
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + export HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + [[ 1 -gt 1 ]]
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + TEMPLATE=hdfs-template.xml
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file hdfs-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + ((  6 == 0  ))
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ get_java_property /tmp/cluster/properties/dataproc.properties simplified.scaling.enable
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ local property_file=/tmp/cluster/properties/dataproc.properties
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ local property_name=simplified.scaling.enable
<13>Oct 13 23:00:32 google-dataproc-startup[811]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:32 google-dataproc-startup[811]: +++ cut -d = -f 2-
<13>Oct 13 23:00:32 google-dataproc-startup[811]: +++ tail -n 1
<13>Oct 13 23:00:32 google-dataproc-startup[811]: +++ grep '^simplified.scaling.enable=' /tmp/cluster/properties/dataproc.properties
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ local property_value=
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ echo ''
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + [[ '' == \t\r\u\e ]]
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + set -e
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + loginfo 'Running configure_connectors.sh'
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + echo 'Running configure_connectors.sh'
<13>Oct 13 23:00:32 google-dataproc-startup[811]: Running configure_connectors.sh
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + ((  1  ))
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + export GCS_METADATA_CACHE_TYPE=FILESYSTEM_BACKED
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + GCS_METADATA_CACHE_TYPE=FILESYSTEM_BACKED
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ get_nfs_mount_point
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ echo /hadoop_gcs_connector_metadata_cache
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + export GCS_FILE_CACHE_DIRECTORY=/hadoop_gcs_connector_metadata_cache
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + GCS_FILE_CACHE_DIRECTORY=/hadoop_gcs_connector_metadata_cache
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file gcs-core-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: ++ hostname -s
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + [[ cluster-6d45-m == \c\l\u\s\t\e\r\-\6\d\4\5\-\m ]]
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + [[ 1 -ne 0 ]]
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + setup_cache_cleaner
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + mkdir -p /usr/lib/hadoop/google
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local cleaner=/usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + make_cache_cleaner_script
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local gc_cleaner=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemCacheCleaner
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local etab=/var/lib/nfs/etab
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + chmod 755 /usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + make_cleaner_crontab /usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local cleaner=/usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file bq-mapred-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + set -e
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + set -o nounset
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + loginfo 'Running configure_spark.sh'
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + echo 'Running configure_spark.sh'
<13>Oct 13 23:00:32 google-dataproc-startup[811]: Running configure_spark.sh
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + SPARK_EVENTLOG_DIR=hdfs://cluster-6d45-m/user/spark/eventlog
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + SPARK_TMPDIR=/hadoop/spark/tmp
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + SPARK_WORKDIR=/hadoop/spark/work
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + SPARK_LOG_DIR=/var/log/spark
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + mkdir -p /hadoop/spark/tmp /hadoop/spark/work /var/log/spark
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + chown spark:spark -R /hadoop/spark /var/log/spark
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + chmod 1777 -R /hadoop/spark /var/log/spark
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + is_version_at_least 2.3.3 2
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local ver1=2.3.3.0.0.0.0
<13>Oct 13 23:00:32 google-dataproc-startup[811]: + local ver2=2
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local log=/tmp/tmp.eEgp8QekKC
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + dpkg --compare-versions 2.3.3.0.0.0.0 '>=' 2
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + err_code=0
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.eEgp8QekKC
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + rm -f /tmp/tmp.eEgp8QekKC
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + return 0
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + RPC_SIZE_KEY=spark.rpc.message.maxSize
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + NUM_INITIAL_EXECUTORS_KEY=spark.executor.instances
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + SPARK_YARN_DIR=/usr/lib/spark/yarn
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + SPARKR_LIB_DIR=/usr/lib/spark/R/lib
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + [[ -f /usr/lib/spark/R/lib/sparkr.zip ]]
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ awk '/^Mem:/{print $2}'
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ free -m
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + TOTAL_MEM=3704
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ python -c 'print int(3704 * 0.15)'
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + SPARK_DAEMON_MEMORY=555
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ python -c 'print int(3704 / 4)'
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + SPARK_DRIVER_MEM_MB=926
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ python -c 'print int(926 / 2)'
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + SPARK_DRIVER_MAX_RESULT_MB=463
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ head -1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ find /tmp/mrv2_IOy_tmp_env.sh
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + YARN_MEMORY_ENV=/tmp/mrv2_IOy_tmp_env.sh
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + source /tmp/mrv2_IOy_tmp_env.sh
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export YARN_MIN_MEM_MB=256
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ YARN_MIN_MEM_MB=256
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export YARN_MAX_MEM_MB=2816
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ YARN_MAX_MEM_MB=2816
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export NODEMANAGER_MEM_MB=2816
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ NODEMANAGER_MEM_MB=2816
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export APP_MASTER_MEM_MB=2816
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ APP_MASTER_MEM_MB=2816
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export CORES_PER_APP_MASTER_ROUNDED=1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ CORES_PER_APP_MASTER_ROUNDED=1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export APP_MASTER_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ APP_MASTER_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export MAP_MEM_MB=2816
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ MAP_MEM_MB=2816
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ CORES_PER_MAP_ROUNDED=1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export MAP_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ MAP_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export REDUCE_MEM_MB=2816
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ REDUCE_MEM_MB=2816
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export CORES_PER_REDUCE_ROUNDED=1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ CORES_PER_REDUCE_ROUNDED=1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ export REDUCE_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ REDUCE_JAVA_OPTS=-Xmx2252m
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ python
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ cat
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + SPARK_EXECUTOR_MEMORY=1280
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ python -c 'print max(1,     1 / 2)'
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + SPARK_EXECUTOR_CORES=1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ python -c 'print int(max(     1280 / 11, 384))'
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + SPARK_YARN_EXECUTOR_MEMORY_OVERHEAD=384
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + SPARK_EXECUTOR_MEMORY=896
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + is_version_at_least 1.3 1.4
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local ver2=1.4
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local log=/tmp/tmp.mJEnpCSsbm
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + err_code=1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.mJEnpCSsbm
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + rm -f /tmp/tmp.mJEnpCSsbm
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + is_version_at_least 1.3 1.3
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local ver2=1.3
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local log=/tmp/tmp.vtGQ1PLMgw
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + err_code=0
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.vtGQ1PLMgw
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + rm -f /tmp/tmp.vtGQ1PLMgw
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + return 0
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + set -e
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + loginfo 'Running configure_tez.sh'
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + echo 'Running configure_tez.sh'
<13>Oct 13 23:00:33 google-dataproc-startup[811]: Running configure_tez.sh
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + readonly CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + is_version_at_least 1.3 1.3
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local ver2=1.3
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local log=/tmp/tmp.V1w6hnCIsW
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + err_code=0
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.V1w6hnCIsW
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + rm -f /tmp/tmp.V1w6hnCIsW
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + return 0
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ ls /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + configure_war /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local -r tez_war=/usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ mktemp -d
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local -r tmp_dir=/tmp/tmp.ZqSwvfiyzq
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + unzip -q /usr/lib/tez/tez-ui-0.9.2.war -d /tmp/tmp.ZqSwvfiyzq
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local -r tez_configs=/tmp/tmp.ZqSwvfiyzq/config/configs.env
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ cut -d ' ' -f 1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ md5sum /tmp/tmp.ZqSwvfiyzq/config/configs.env
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + [[ 23fbfca8f7b8e142395c6bb4676427ae != \2\3\f\b\f\c\a\8\f\7\b\8\e\1\4\2\3\9\5\c\6\b\b\4\6\7\6\4\2\7\a\e ]]
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + [[ -f /tmp/tmp.ZqSwvfiyzq/config/configs.env ]]
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + sed -i 's#\(.*\)//timeline: "http://localhost:8188"\(.*\)#\1timeline: "http://cluster-6d45-m:8188"\2#' /tmp/tmp.ZqSwvfiyzq/config/configs.env
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + sed -i 's#\(.*\)//rm: "http://localhost:8088"\(.*\)#\1rm: "http://cluster-6d45-m:8088"\2#' /tmp/tmp.ZqSwvfiyzq/config/configs.env
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ local property_name=dataproc.components.activate
<13>Oct 13 23:00:33 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 23:00:33 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:33 google-dataproc-startup[811]: +++ local property_name=dataproc.components.activate
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:33 google-dataproc-startup[811]: +++ local property_value=
<13>Oct 13 23:00:33 google-dataproc-startup[811]: +++ echo ''
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ local property_value=
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ echo ''
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + local -r optional_components_value=
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + [[ '' == *\k\n\o\x* ]]
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + cd /tmp/tmp.ZqSwvfiyzq
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + zip -q /usr/lib/tez/tez-ui-0.9.2.war -r ./assets ./config ./fonts ./index.html ./META-INF ./WEB-INF
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + cd ..
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + rm -rf /tmp/tmp.ZqSwvfiyzq
<13>Oct 13 23:00:33 google-dataproc-startup[811]: ++ stat /usr/lib/tez/tez-common-0.9.2.jar --format=%Y
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + touch -d @1568819629 /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-names --value tez --clobber
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-on-disk-path.tez --value /usr/lib/tez/tez-ui-0.9.2.war --clobber
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-web-path.tez --value /tez-ui --clobber
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.history.logging.service.class --value org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService --clobber
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.tez-ui.history-url.base --value http://cluster-6d45-m:8188/tez-ui/ --clobber
<13>Oct 13 23:00:33 google-dataproc-startup[811]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.am.node-blacklisting.enabled --value false --clobber
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + set -e
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + declare -r ZOOKEEPER_CONFIG=/etc/zookeeper/conf/zoo.cfg
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + declare -r ZOOKEEPER_DATA_DIR=/var/lib/zookeeper/
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i=0 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i<1 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo server.0=cluster-6d45-m:2888:3888
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i++  ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i<1 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo autopurge.purgeInterval=168
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ sed -e 's/.*-m-//'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ uname -n
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + declare -r MY_ID=cluster-6d45-m
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo cluster-6d45-m
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Populating initial cluster member list'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Populating initial cluster member list'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Populating initial cluster member list
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ get_dataproc_property dataproc.worker.custom.init.actions.mode
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local property_name=dataproc.worker.custom.init.actions.mode
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.worker.custom.init.actions.mode
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ local property_name=dataproc.worker.custom.init.actions.mode
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++++ grep '^dataproc.worker.custom.init.actions.mode=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ local property_value=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ echo ''
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local property_value=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ echo ''
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + WORKER_CUSTOM_INIT_ACTIONS_MODE=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + WORKER_COUNT=6
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ get_dataproc_property simplified.scaling.enable
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local property_name=simplified.scaling.enable
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ local property_name=simplified.scaling.enable
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ local property_value=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ echo ''
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local property_value=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ echo ''
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ '' != \t\r\u\e ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + ((  6 == 0  ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ '' == \R\U\N\_\B\E\F\O\R\E\_\S\E\R\V\I\C\E\S ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + MEMBERSHIP_FILE=/etc/hadoop/conf/nodes_include
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i=0 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i<6 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo cluster-6d45-w-0.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i++  ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i<6 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo cluster-6d45-w-1.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i++  ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i<6 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo cluster-6d45-w-2.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i++  ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i<6 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo cluster-6d45-w-3.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i++  ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i<6 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo cluster-6d45-w-4.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i++  ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i<6 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo cluster-6d45-w-5.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i++  ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i<6 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merging user-specified cluster properties'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merging user-specified cluster properties'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merging user-specified cluster properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_xml_properties /tmp/cluster/properties/capacity-scheduler.xml /etc/hadoop/conf/capacity-scheduler.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/capacity-scheduler.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/hadoop/conf/capacity-scheduler.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/capacity-scheduler.xml ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file /tmp/cluster/properties/capacity-scheduler.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/capacity-scheduler.xml.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_xml_properties /tmp/cluster/properties/core.xml /etc/hadoop/conf/core-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/core.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/hadoop/conf/core-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/core.xml ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file /tmp/cluster/properties/core.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/core.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/core.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/core.xml.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_xml_properties /tmp/cluster/properties/distcp.xml /etc/hadoop/conf/distcp-default.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/distcp.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/hadoop/conf/distcp-default.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/distcp.xml ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file /tmp/cluster/properties/distcp.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/distcp.xml.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_xml_properties /tmp/cluster/properties/hdfs.xml /etc/hadoop/conf/hdfs-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/hdfs.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/hadoop/conf/hdfs-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/hdfs.xml ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /tmp/cluster/properties/hdfs.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/hdfs.xml.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_xml_properties /tmp/cluster/properties/mapred.xml /etc/hadoop/conf/mapred-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/mapred.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/hadoop/conf/mapred-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/mapred.xml ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file /tmp/cluster/properties/mapred.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/mapred.xml.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_xml_properties /tmp/cluster/properties/yarn.xml /etc/hadoop/conf/yarn-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/yarn.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/hadoop/conf/yarn-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/yarn.xml ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /tmp/cluster/properties/yarn.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/yarn.xml.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_xml_properties /tmp/cluster/properties/hive.xml /etc/hive/conf/hive-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/hive.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/hive/conf/hive-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/hive.xml ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/hive/conf/hive-site.xml --source_configuration_file /tmp/cluster/properties/hive.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/hive.xml.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_java_properties /tmp/cluster/properties/pig.properties /etc/pig/conf/pig.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/pig.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/pig/conf/pig.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/pig.properties ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + cat /tmp/cluster/properties/pig.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/pig.properties.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_java_properties /tmp/cluster/properties/spark.properties /etc/spark/conf/spark-defaults.conf
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/spark.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/spark/conf/spark-defaults.conf
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/spark.properties ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + cat /tmp/cluster/properties/spark.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/spark.properties.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_java_properties /tmp/cluster/properties/zookeeper.properties /etc/zookeeper/conf/zoo.cfg
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/zookeeper.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/zookeeper/conf/zoo.cfg
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/zookeeper.properties ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + cat /tmp/cluster/properties/zookeeper.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/zookeeper.properties.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_sh_env_vars /tmp/cluster/properties/hadoop-env.sh /etc/hadoop/conf/hadoop-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/hadoop-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/hadoop/conf/hadoop-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/hadoop-env.sh ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + cat /tmp/cluster/properties/hadoop-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/hadoop-env.sh.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_sh_env_vars /tmp/cluster/properties/mapred-env.sh /etc/hadoop/conf/mapred-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/mapred-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/hadoop/conf/mapred-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/mapred-env.sh ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + cat /tmp/cluster/properties/mapred-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/mapred-env.sh.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_sh_env_vars /tmp/cluster/properties/spark-env.sh /etc/spark/conf/spark-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/spark-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/spark/conf/spark-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/spark-env.sh ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + cat /tmp/cluster/properties/spark-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/spark-env.sh.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_sh_env_vars /tmp/cluster/properties/yarn-env.sh /etc/hadoop/conf/yarn-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/yarn-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/hadoop/conf/yarn-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/yarn-env.sh ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + cat /tmp/cluster/properties/yarn-env.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/yarn-env.sh.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ -f /etc/hbase/conf/hbase-site.xml ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ -f /etc/tez/conf/tez-site.xml ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + merge_xml_properties /tmp/cluster/properties/tez.xml /etc/tez/conf/tez-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local src=/tmp/cluster/properties/tez.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local dest=/etc/tez/conf/tez-site.xml
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ ! -f /tmp/cluster/properties/tez.xml ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + bdconfig merge_configurations --configuration_file /etc/tez/conf/tez-site.xml --source_configuration_file /tmp/cluster/properties/tez.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Merged /tmp/cluster/properties/tez.xml.
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + ACTIVATABLE_COMPONENTS=(${DATAPROC_OPTIONAL_COMPONENTS})
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + DATAPROC_NON_DEBIAN_COMPONENTS=(${DATAPROC_NON_DEBIAN_COMPONENTS})
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + PACKAGES_TO_KEEP=("${SERVICES[@]}" ${DATAPROC_COMMON_PACKAGES})
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + DATAPROC_START_AFTER_HDFS_SERVICES=(${DATAPROC_START_AFTER_HDFS_SERVICES})
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + COMPONENTS_TO_ACTIVATE=($(intersection COMPONENTS_TO_ACTIVATE PACKAGES_TO_KEEP))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ intersection COMPONENTS_TO_ACTIVATE PACKAGES_TO_KEEP
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local -n values=COMPONENTS_TO_ACTIVATE
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local -n filter=PACKAGES_TO_KEEP
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ sort -u
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ sort -u
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ printf '%s\n' hadoop-hdfs-namenode hadoop-yarn-resourcemanager hive-metastore hive-server2 zookeeper-server solr-server hadoop-mapreduce-historyserver spark-history-server hive-webhcat-server jupyter knox proxy-agent zeppelin hadoop-yarn-timelineserver mariadb-server hadoop-hdfs-secondarynamenode openjdk-8-jdk openjdk-8-dbg libjansi-java python-numpy libmysql-java hadoop-client hive pig spark-core spark-python spark-r autofs libhdfs0 libsnappy-dev libzstd-dev libatlas3-base libopenblas-base libapr1 vim git bash-completion spark-yarn-shuffle spark-datanucleus spark-extras hadoop-lzo python-setuptools anaconda druid kafka-server kerberos presto openssl tez hive-hcatalog
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ printf '%s\n'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + NON_ACTIVATED_COMPONENTS=($(difference ACTIVATABLE_COMPONENTS COMPONENTS_TO_ACTIVATE))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ difference ACTIVATABLE_COMPONENTS COMPONENTS_TO_ACTIVATE
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local -n filter=COMPONENTS_TO_ACTIVATE
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ sort -u
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ sort -u
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ printf '%s\n'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ printf '%s\n' zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + PACKAGES_TO_UNINSTALL+=($(difference NON_ACTIVATED_COMPONENTS PACKAGES_TO_UNINSTALL))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ difference NON_ACTIVATED_COMPONENTS PACKAGES_TO_UNINSTALL
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local -n values=NON_ACTIVATED_COMPONENTS
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local -n filter=PACKAGES_TO_UNINSTALL
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ sort -u
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ sort -u
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ printf '%s\n' anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + NON_DEBIAN_COMPONENTS_TO_UNINSTALL=($(intersection PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ intersection PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local -n values=PACKAGES_TO_UNINSTALL
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local -n filter=DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ sort -u
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ sort -u
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ printf '%s\n' anaconda jupyter kerberos presto proxy-agent
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + DEBIAN_COMPONENTS_TO_UNINSTALL=($(difference PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ difference PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local -n values=PACKAGES_TO_UNINSTALL
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local -n filter=DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ sort -u
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ sort -u
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ printf '%s\n' anaconda jupyter kerberos presto proxy-agent
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ false != \t\r\u\e ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + DEBIAN_COMPONENTS_TO_UNINSTALL+=('krb5-kpropd' 'krb5-kdc' 'krb5-admin-server' 'krb5-user' 'krb5-config' 'xinetd')
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + uninstall_packages
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_with_retries set_selections
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local retry_backoff
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + cmd=("$@")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local -a cmd
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'About to run '\''set_selections'\'' with retries...'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'About to run '\''set_selections'\'' with retries...'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: About to run 'set_selections' with retries...
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local update_succeeded=0
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i = 0 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + (( i < 12 ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + set_selections
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + debconf-set-selections
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + cat
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + update_succeeded=1
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + break
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + ((  1  ))
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + loginfo 'Uninstalling un-needed daemons'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Uninstalling un-needed daemons'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Uninstalling un-needed daemons
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_in_background --tag uninstall bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + PID=1403
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Started background process [bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true] as pid 1403'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Started background process [bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true] as pid 1403
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_in_background --tag uninstall-component-anaconda uninstall_component anaconda
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + PID=1404
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Started background process [uninstall_component anaconda] as pid 1404'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Started background process [uninstall_component anaconda] as pid 1404
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_in_background --tag uninstall-component-jupyter uninstall_component jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + PID=1405
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Started background process [uninstall_component jupyter] as pid 1405'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Started background process [uninstall_component jupyter] as pid 1405
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_in_background --tag uninstall-component-kerberos uninstall_component kerberos
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + PID=1406
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Started background process [uninstall_component kerberos] as pid 1406'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Started background process [uninstall_component kerberos] as pid 1406
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_in_background --tag uninstall-component-presto uninstall_component presto
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + PID=1407
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Started background process [uninstall_component presto] as pid 1407'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Started background process [uninstall_component presto] as pid 1407
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_in_background --tag uninstall-component-proxy-agent uninstall_component proxy-agent
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + PID=1408
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + echo 'Started background process [uninstall_component proxy-agent] as pid 1408'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: Started background process [uninstall_component proxy-agent] as pid 1408
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + is_version_at_least 1.3 1.3
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local ver1=1.3.0.0.0.0
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local ver2=1.3
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_with_logger --tag uninstall-component-presto uninstall_component presto
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local pid=1407
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + tag=uninstall-component-presto
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_with_logger --tag uninstall-component-proxy-agent uninstall_component proxy-agent
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local pid=1408
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + tag=uninstall-component-proxy-agent
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ mktemp
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_with_logger --tag uninstall-component-kerberos uninstall_component kerberos
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local pid=1406
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + tag=uninstall-component-kerberos
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_with_logger --tag uninstall-component-jupyter uninstall_component jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local pid=1405
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + tag=uninstall-component-jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_with_logger --tag uninstall-component-anaconda uninstall_component anaconda
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local pid=1404
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + tag=uninstall-component-anaconda
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ logger -s -t 'uninstall-component-presto[1407]'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ logger -s -t 'uninstall-component-proxy-agent[1408]'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + run_with_logger --tag uninstall bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local pid=1403
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + tag=uninstall
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + local log=/tmp/tmp.PWqQGcTuYM
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ logger -s -t 'uninstall-component-kerberos[1406]'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ logger -s -t 'uninstall-component-jupyter[1405]'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ logger -s -t 'uninstall-component-anaconda[1404]'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: + uninstall_component presto
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: + uninstall_component proxy-agent
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ logger -s -t 'uninstall[1403]'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + err_code=0
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + grep -C 10 -i warning /tmp/tmp.PWqQGcTuYM
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-kerberos[1406]: + uninstall_component kerberos
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-kerberos[1406]: + local component=kerberos
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-kerberos[1406]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-kerberos[1406]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-kerberos[1406]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-kerberos[1406]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-kerberos[1406]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: + uninstall_component jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: + uninstall_component anaconda
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: + local component=anaconda
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall[1403]: + bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + rm -f /tmp/tmp.PWqQGcTuYM
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: + local component=proxy-agent
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: + set -exo pipefail
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: + local component=presto
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: + set -exo pipefail
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-kerberos[1406]: + set -euxo pipefail
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: + local component=jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh ]]
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh'
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: + set -euxo pipefail
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/jupyter.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: + set -exo pipefail
<13>Oct 13 23:00:34 google-dataproc-startup[811]: + return 0
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/anaconda.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ export WHEEL_DIR=/opt/dataproc/jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ WHEEL_DIR=/opt/dataproc/jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ export JGSCM_WHEEL=/opt/dataproc/jupyter/jgscm
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ JGSCM_WHEEL=/opt/dataproc/jupyter/jgscm
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ export JGSCM_REQUIREMENTS=/opt/dataproc/jupyter/jgscm.requirements
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ JGSCM_REQUIREMENTS=/opt/dataproc/jupyter/jgscm.requirements
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ export JUPYTER_ETC_DIR=/etc/jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: ++ export ANACONDA_INSTALL_PATH=/opt/conda/anaconda
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: ++ ANACONDA_INSTALL_PATH=/opt/conda/anaconda
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: ++ export ANACONDA_BIN_DIR=/opt/conda/anaconda/bin
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: ++ ANACONDA_BIN_DIR=/opt/conda/anaconda/bin
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: ++ export PATH=/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: ++ PATH=/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-anaconda[1404]: + rm -Rf /opt/conda/anaconda
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ JUPYTER_ETC_DIR=/etc/jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ export JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ export JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ export PYSPARK_KERNELSPEC=/opt/conda/anaconda/share/jupyter/kernels/pyspark/kernel.json
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ PYSPARK_KERNELSPEC=/opt/conda/anaconda/share/jupyter/kernels/pyspark/kernel.json
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ export JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ export JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: ++ JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: + rm -Rf /etc/jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/proxy-agent.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/presto.sh
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: ++ readonly PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: ++ PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: ++ readonly PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: ++ PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: ++ readonly PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: ++ PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: + rm -f /usr/bin/proxy-forwarding-agent
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ readonly HTTP_PORT=8060
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ HTTP_PORT=8060
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ readonly PRESTO_BIN_PATH=/usr/bin/presto
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ PRESTO_BIN_PATH=/usr/bin/presto
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ readonly PRESTO_CLI_PATH=/opt/presto-cli
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ PRESTO_CLI_PATH=/opt/presto-cli
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ readonly PRESTO_DATA_DIRECTORY=/var/presto/data
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ PRESTO_DATA_DIRECTORY=/var/presto/data
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ readonly PRESTO_INSTALL_DIRECTORY=/opt/presto-server
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ PRESTO_INSTALL_DIRECTORY=/opt/presto-server
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ readonly INIT_SCRIPT=/usr/lib/systemd/system/presto.service
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ INIT_SCRIPT=/usr/lib/systemd/system/presto.service
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ readonly PRESTO_VERSION=0.215
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: ++ PRESTO_VERSION=0.215
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-presto[1407]: + rm -Rf /opt/presto-server
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ get_dataproc_property dataproc.monitoring.stackdriver.enable
<13>Oct 13 23:00:34 google-dataproc-startup[811]: ++ local property_name=dataproc.monitoring.stackdriver.enable
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-jupyter[1405]: + rm -Rf /opt/dataproc/jupyter
<13>Oct 13 23:00:34 google-dataproc-startup[811]: <13>Oct 13 23:00:34 uninstall-component-proxy-agent[1408]: + rm -f /usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.monitoring.stackdriver.enable
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:34 google-dataproc-startup[811]: +++ local property_name=dataproc.monitoring.stackdriver.enable
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++++ grep '^dataproc.monitoring.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:35 google-dataproc-startup[811]: +++ local property_value=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: +++ echo false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ local property_value=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ echo false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + STACKDRIVER_MONITORING_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + loginfo 'Stackdriver monitoring disabled.'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Stackdriver monitoring disabled.'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Stackdriver monitoring disabled.
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + loginfo 'Replace dataproc plugin instance_name label with gce instance name.'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Replace dataproc plugin instance_name label with gce instance name.'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Replace dataproc plugin instance_name label with gce instance name.
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for PLUGIN_FILE in /opt/stackdriver/collectd/etc/collectd.d/dataproc*
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ hostname
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + sed -i 's/"label:instance_name".*$/"label:instance_name" "cluster-6d45-m"/g' /opt/stackdriver/collectd/etc/collectd.d/dataproc_collectd_default-20170324-133642.conf
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + chmod +x /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + loginfo 'Running verify_setup.sh'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 uninstall-component-presto[1407]: + rm -Rf /var/presto/data
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Running verify_setup.sh'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Running verify_setup.sh
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 uninstall-component-presto[1407]: + rm -f /usr/bin/presto
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.warehouse.dir
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 uninstall-component-presto[1407]: + rm -f /opt/presto-cli
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 uninstall-component-presto[1407]: + rm -f /usr/lib/systemd/system/presto.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + hive_warehouse_dir=None
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ None == \g\s\:\/\/* ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + loginfo 'Starting services'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Starting services'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Starting services
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array hadoop-hdfs-namenode ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=hadoop-hdfs-namenode
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-hdfs-namenode  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + case "${SERVICE}" in
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_in_background --tag setup-hadoop-hdfs-namenode setup_service hadoop-hdfs-namenode
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + PID=1465
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Started background process [setup_service hadoop-hdfs-namenode] as pid 1465'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Started background process [setup_service hadoop-hdfs-namenode] as pid 1465
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array hadoop-yarn-resourcemanager ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=hadoop-yarn-resourcemanager
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-yarn-resourcemanager  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + case "${SERVICE}" in
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_in_background --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + PID=1466
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Started background process [setup_service hadoop-yarn-resourcemanager] as pid 1466'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Started background process [setup_service hadoop-yarn-resourcemanager] as pid 1466
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array hive-metastore ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=hive-metastore
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-metastore  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + case "${SERVICE}" in
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_in_background --tag setup-hive-metastore setup_service hive-metastore
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + PID=1467
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Started background process [setup_service hive-metastore] as pid 1467'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Started background process [setup_service hive-metastore] as pid 1467
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array hive-server2 ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=hive-server2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-server2  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + case "${SERVICE}" in
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_in_background --tag setup-hive-server2 setup_service hive-server2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + PID=1468
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Started background process [setup_service hive-server2] as pid 1468'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Started background process [setup_service hive-server2] as pid 1468
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array zookeeper-server ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=zookeeper-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  zookeeper-server  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + continue
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array solr-server ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=solr-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  solr-server  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + continue
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array hadoop-mapreduce-historyserver ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=hadoop-mapreduce-historyserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-mapreduce-historyserver  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + case "${SERVICE}" in
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_in_background --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + PID=1469
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_with_logger --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local pid=1469
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + tag=setup-hadoop-mapreduce-historyserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_with_logger --tag setup-hive-server2 setup_service hive-server2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local pid=1468
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + tag=setup-hive-server2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_with_logger --tag setup-hadoop-hdfs-namenode setup_service hadoop-hdfs-namenode
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local pid=1465
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + tag=setup-hadoop-hdfs-namenode
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_with_logger --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local pid=1466
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + tag=setup-hadoop-yarn-resourcemanager
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_with_logger --tag setup-hive-metastore setup_service hive-metastore
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local pid=1467
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + tag=setup-hive-metastore
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Started background process [setup_service hadoop-mapreduce-historyserver] as pid 1469'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Started background process [setup_service hadoop-mapreduce-historyserver] as pid 1469
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array spark-history-server ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=spark-history-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  spark-history-server  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + case "${SERVICE}" in
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_in_background --tag setup-spark-history-server setup_service spark-history-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + PID=1479
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Started background process [setup_service spark-history-server] as pid 1479'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Started background process [setup_service spark-history-server] as pid 1479
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array hive-webhcat-server ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=hive-webhcat-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-webhcat-server  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + continue
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array jupyter ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=jupyter
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  jupyter  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + continue
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array knox ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=knox
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  knox  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + continue
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array proxy-agent ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=proxy-agent
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  proxy-agent  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + continue
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array zeppelin ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=zeppelin
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ logger -s -t 'setup-hadoop-mapreduce-historyserver[1469]'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ logger -s -t 'setup-hive-server2[1468]'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ logger -s -t 'setup-hadoop-yarn-resourcemanager[1466]'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ logger -s -t 'setup-hadoop-hdfs-namenode[1465]'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ logger -s -t 'setup-hive-metastore[1467]'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_with_logger --tag setup-spark-history-server setup_service spark-history-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local pid=1479
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + tag=setup-spark-history-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + setup_service hadoop-mapreduce-historyserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + setup_service hadoop-yarn-resourcemanager
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + setup_service hive-server2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + setup_service hadoop-hdfs-namenode
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  zeppelin  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + continue
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array hadoop-yarn-timelineserver ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=hadoop-yarn-timelineserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-yarn-timelineserver  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + case "${SERVICE}" in
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_in_background --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + PID=1489
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Started background process [setup_service hadoop-yarn-timelineserver] as pid 1489'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Started background process [setup_service hadoop-yarn-timelineserver] as pid 1489
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array mariadb-server ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=mariadb-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  mariadb-server  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + case "${SERVICE}" in
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_in_background --tag setup-mariadb setup_service mariadb
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + PID=1490
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + setup_service hive-metastore
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ logger -s -t 'setup-spark-history-server[1479]'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_with_logger --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local pid=1489
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + tag=setup-hadoop-yarn-timelineserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + setup_service spark-history-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + export MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + local service=hadoop-yarn-resourcemanager
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + enable_service hadoop-yarn-resourcemanager
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + local service=hadoop-yarn-resourcemanager
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + local unit=hadoop-yarn-resourcemanager.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + run_with_retries systemctl enable hadoop-yarn-resourcemanager.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + local retry_backoff
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + cmd=("$@")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + local -a cmd
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + loginfo 'About to run '\''systemctl enable hadoop-yarn-resourcemanager.service'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + echo 'About to run '\''systemctl enable hadoop-yarn-resourcemanager.service'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: About to run 'systemctl enable hadoop-yarn-resourcemanager.service' with retries...
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + local update_succeeded=0
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + (( i = 0 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + (( i < 12 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: + systemctl enable hadoop-yarn-resourcemanager.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: hadoop-yarn-resourcemanager.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-resourcemanager[1466]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-resourcemanager
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + export MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + local service=hive-server2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + [[ hive-server2 == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + enable_service hive-server2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + local service=hive-server2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + local unit=hive-server2.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + run_with_retries systemctl enable hive-server2.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + local retry_backoff
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + cmd=("$@")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + local -a cmd
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + loginfo 'About to run '\''systemctl enable hive-server2.service'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + echo 'About to run '\''systemctl enable hive-server2.service'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: About to run 'systemctl enable hive-server2.service' with retries...
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + local update_succeeded=0
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + (( i = 0 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + (( i < 12 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: + systemctl enable hive-server2.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: hive-server2.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-server2[1468]: Executing: /lib/systemd/systemd-sysv-install enable hive-server2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + export MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + local service=hadoop-mapreduce-historyserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + [[ hadoop-mapreduce-historyserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + enable_service hadoop-mapreduce-historyserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + local service=hadoop-mapreduce-historyserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + local unit=hadoop-mapreduce-historyserver.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + run_with_retries systemctl enable hadoop-mapreduce-historyserver.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + local retry_backoff
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + cmd=("$@")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + local -a cmd
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + loginfo 'About to run '\''systemctl enable hadoop-mapreduce-historyserver.service'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + echo 'About to run '\''systemctl enable hadoop-mapreduce-historyserver.service'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: About to run 'systemctl enable hadoop-mapreduce-historyserver.service' with retries...
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + local update_succeeded=0
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + (( i = 0 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + (( i < 12 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: + systemctl enable hadoop-mapreduce-historyserver.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: hadoop-mapreduce-historyserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-mapreduce-historyserver[1469]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-mapreduce-historyserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + export MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + local service=hadoop-hdfs-namenode
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + case "${MASTER_INDEX?}" in
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + loginfo 'Formatting NameNode'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + echo 'Formatting NameNode'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: Formatting NameNode
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + run_with_retries su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + local retry_backoff
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + cmd=("$@")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + local -a cmd
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + loginfo 'About to run '\''su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + echo 'About to run '\''su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: About to run 'su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive' with retries...
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + local update_succeeded=0
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + (( i = 0 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + (( i < 12 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-namenode[1465]: + su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_with_logger --tag setup-mariadb setup_service mariadb
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local pid=1490
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + tag=setup-mariadb
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + export MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + local service=hive-metastore
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + [[ hive-metastore == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + wait_for_port cluster-6d45-m 3306
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + local -r host=cluster-6d45-m
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + local -r port=3306
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + loginfo 'Waiting for service to come up on host=cluster-6d45-m port=3306.'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + echo 'Waiting for service to come up on host=cluster-6d45-m port=3306.'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: Waiting for service to come up on host=cluster-6d45-m port=3306.
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + retry_with_constant_backoff nc -v -z -w 0 cluster-6d45-m 3306
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + local max_retry=300
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + cmd=("$@")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + local -a cmd
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + local update_succeeded=0
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: ++ seq 1 300
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 3306
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Started background process [setup_service mariadb] as pid 1490'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Started background process [setup_service mariadb] as pid 1490
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + in_array hadoop-hdfs-secondarynamenode ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local value=hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-hdfs-secondarynamenode  ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + return 1
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + case "${SERVICE}" in
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_in_background --tag setup-hadoop-hdfs-secondarynamenode setup_service hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + PID=1498
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Started background process [setup_service hadoop-hdfs-secondarynamenode] as pid 1498'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Started background process [setup_service hadoop-hdfs-secondarynamenode] as pid 1498
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + loginfo 'Configuring optional components'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + echo 'Configuring optional components'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: Configuring optional components
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ logger -s -t 'setup-hadoop-yarn-timelineserver[1489]'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ logger -s -t 'setup-mariadb[1490]'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 1.'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 1.'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 1.
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + run_with_logger --tag setup-hadoop-hdfs-secondarynamenode setup_service hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + local pid=1498
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + tag=setup-hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:35 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + export MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + local service=spark-history-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + [[ spark-history-server == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + enable_service spark-history-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + local service=spark-history-server
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + local unit=spark-history-server.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + run_with_retries systemctl enable spark-history-server.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + local retry_backoff
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + cmd=("$@")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + local -a cmd
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + loginfo 'About to run '\''systemctl enable spark-history-server.service'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + echo 'About to run '\''systemctl enable spark-history-server.service'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: About to run 'systemctl enable spark-history-server.service' with retries...
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + local update_succeeded=0
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + (( i = 0 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + (( i < 12 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: + systemctl enable spark-history-server.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ get_dataproc_property dataproc.conscrypt.provider.enable
<13>Oct 13 23:00:35 google-dataproc-startup[811]: ++ local property_name=dataproc.conscrypt.provider.enable
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + setup_service hadoop-yarn-timelineserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + export MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + local service=hadoop-yarn-timelineserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + enable_service hadoop-yarn-timelineserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + local service=hadoop-yarn-timelineserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + local unit=hadoop-yarn-timelineserver.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + run_with_retries systemctl enable hadoop-yarn-timelineserver.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + local retry_backoff
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + cmd=("$@")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + local -a cmd
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + loginfo 'About to run '\''systemctl enable hadoop-yarn-timelineserver.service'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + echo 'About to run '\''systemctl enable hadoop-yarn-timelineserver.service'\'' with retries...'
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: About to run 'systemctl enable hadoop-yarn-timelineserver.service' with retries...
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + local update_succeeded=0
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + (( i = 0 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + (( i < 12 ))
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: + systemctl enable hadoop-yarn-timelineserver.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: hadoop-yarn-timelineserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-yarn-timelineserver[1489]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-timelineserver
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + setup_service mariadb
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + KERBEROS_ENABLED=false
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + export MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + local service=mariadb
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + [[ mariadb == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + enable_service mariadb
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + local service=mariadb
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + local unit=mariadb.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + run_with_retries systemctl enable mariadb.service
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + local retry_backoff
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + cmd=("$@")
<13>Oct 13 23:00:35 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + local -a cmd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + loginfo 'About to run '\''systemctl enable mariadb.service'\'' with retries...'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + echo 'About to run '\''systemctl enable mariadb.service'\'' with retries...'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: About to run 'systemctl enable mariadb.service' with retries...
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + local update_succeeded=0
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + (( i = 0 ))
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + (( i < 12 ))
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + systemctl enable mariadb.service
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: spark-history-server.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ logger -s -t 'setup-hadoop-hdfs-secondarynamenode[1498]'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.conscrypt.provider.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ local property_name=dataproc.conscrypt.provider.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + setup_service hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + KERBEROS_ENABLED=false
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + export MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + local service=hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + enable_service hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + local service=hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + local unit=hadoop-hdfs-secondarynamenode.service
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + run_with_retries systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + local retry_backoff
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + cmd=("$@")
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + local -a cmd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + loginfo 'About to run '\''systemctl enable hadoop-hdfs-secondarynamenode.service'\'' with retries...'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + echo 'About to run '\''systemctl enable hadoop-hdfs-secondarynamenode.service'\'' with retries...'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: About to run 'systemctl enable hadoop-hdfs-secondarynamenode.service' with retries...
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + local update_succeeded=0
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + (( i = 0 ))
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + (( i < 12 ))
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-spark-history-server[1479]: Executing: /lib/systemd/systemd-sysv-install enable spark-history-server
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ grep '^dataproc.conscrypt.provider.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: + systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ local property_value=true
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ echo true
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ local property_value=true
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ echo true
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + CONSCRYPT_ENABLED=true
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + [[ true == \t\r\u\e ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt.jar /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/libconscrypt.jar
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: Created symlink /etc/systemd/system/mysql.service → /lib/systemd/system/mariadb.service.
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: hadoop-hdfs-secondarynamenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: Created symlink /etc/systemd/system/mysqld.service → /lib/systemd/system/mariadb.service.
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-hadoop-hdfs-secondarynamenode[1498]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni.so /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/libconscrypt_openjdk_jni.so
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + cp /usr/local/share/google/dataproc/java.security.conscrypt /etc/java-8-openjdk/security/java.security
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: Created symlink /etc/systemd/system/multi-user.target.wants/mariadb.service → /lib/systemd/system/mariadb.service.
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ get_dataproc_property dataproc.logging.stackdriver.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ local property_name=dataproc.logging.stackdriver.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ local property_name=dataproc.logging.stackdriver.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ grep '^dataproc.logging.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ tail -n 1
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ local property_value=
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ echo ''
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ local property_value=
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ echo ''
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + STACKDRIVER_LOGGING_ENABLED=
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + [[ '' == \f\a\l\s\e ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + loginfo 'Stackdriver enabled; enabling google-fluentd.'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + echo 'Stackdriver enabled; enabling google-fluentd.'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: Stackdriver enabled; enabling google-fluentd.
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + source /usr/local/share/google/dataproc/bdutil/configure_fluentd.sh
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ set -e
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ set -u
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ loginfo 'Running configure_fluentd.sh'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ echo 'Running configure_fluentd.sh'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: Running configure_fluentd.sh
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ FLUENTD_BASE_DIR=/etc/google-fluentd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ FLUENTD_CONF_DIR=/etc/google-fluentd/config.d
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ FLUENTD_PLUGIN_DIR=/etc/google-fluentd/plugin
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ cp /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/monitoring.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/out_google_cloud.rb /etc/google-fluentd/plugin
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ get_dataproc_property dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ local property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ local property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++++ tail -n 1
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++++ cut -d = -f 2-
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++++ grep '^dataproc.logging.stackdriver.job.driver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ local property_value=
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ echo ''
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ local property_value=
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ echo ''
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ JOB_DRIVER_LOGGING_ENABLED=
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ [[ '' == \t\r\u\e ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ get_dataproc_property dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ local property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ local property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + update_succeeded=1
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + break
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: + ((  1  ))
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++++ cut -d = -f 2-
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++++ tail -n 1
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-mariadb[1490]: ++ systemctl show mariadb.service -p Restart,RemainAfterExit
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++++ grep '^dataproc.logging.stackdriver.job.yarn.container.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ local property_value=
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++++ echo ''
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ local property_value=
<13>Oct 13 23:00:36 google-dataproc-startup[811]: +++ echo ''
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ CONTAINER_LOGGING_ENABLED=
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ [[ '' == \t\r\u\e ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + run_in_background --tag setup-google-fluentd setup_service google-fluentd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + PID=1573
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + echo 'Started background process [setup_service google-fluentd] as pid 1573'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: Started background process [setup_service google-fluentd] as pid 1573
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + wait_on_async_processes
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + loginfo 'Waiting on async proccesses'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + echo 'Waiting on async proccesses'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: Waiting on async proccesses
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + (( i = 0 ))
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + pid=1573
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + cmd='setup_service google-fluentd'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1573 cmd=[setup_service google-fluentd]'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + echo 'Waiting on pid=1573 cmd=[setup_service google-fluentd]'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: Waiting on pid=1573 cmd=[setup_service google-fluentd]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + wait 1573
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + run_with_logger --tag setup-google-fluentd setup_service google-fluentd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + local tag=
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + local pid=1573
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + tag=setup-google-fluentd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + shift 2
<13>Oct 13 23:00:36 google-dataproc-startup[811]: + exec
<13>Oct 13 23:00:36 google-dataproc-startup[811]: ++ logger -s -t 'setup-google-fluentd[1573]'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + setup_service google-fluentd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + export KERBEROS_ENABLED=false
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + KERBEROS_ENABLED=false
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + export -f login_through_keytab_if_necessary
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + export MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + MY_FULL_HOSTNAME=cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + local service=google-fluentd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + [[ google-fluentd == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + enable_service google-fluentd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + local service=google-fluentd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + local unit=google-fluentd.service
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + run_with_retries systemctl enable google-fluentd.service
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + local retry_backoff
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + cmd=("$@")
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + local -a cmd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + loginfo 'About to run '\''systemctl enable google-fluentd.service'\'' with retries...'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + echo 'About to run '\''systemctl enable google-fluentd.service'\'' with retries...'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: About to run 'systemctl enable google-fluentd.service' with retries...
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + local update_succeeded=0
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + (( i = 0 ))
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + (( i < 12 ))
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: + systemctl enable google-fluentd.service
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: google-fluentd.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:35 setup-google-fluentd[1573]: Executing: /lib/systemd/systemd-sysv-install enable google-fluentd
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:36 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:36 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 3306
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:36 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:36 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 2.'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:36 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 2.'
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:36 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 2.
<13>Oct 13 23:00:36 google-dataproc-startup[811]: <13>Oct 13 23:00:36 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + local 'props=Restart=on-abort
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: RemainAfterExit=no'
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + [[ Restart=on-abort
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + in_array mariadb DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + local value=mariadb
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  mariadb  ]]
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + return 1
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + [[ mariadb == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + run_with_retries systemctl start mariadb
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + local retry_backoff
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + cmd=("$@")
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + local -a cmd
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + loginfo 'About to run '\''systemctl start mariadb'\'' with retries...'
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + echo 'About to run '\''systemctl start mariadb'\'' with retries...'
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: About to run 'systemctl start mariadb' with retries...
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + local update_succeeded=0
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + (( i = 0 ))
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + (( i < 12 ))
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-mariadb[1490]: + systemctl start mariadb
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 3306
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 3.'
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 3.'
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 3.
<13>Oct 13 23:00:37 google-dataproc-startup[811]: <13>Oct 13 23:00:37 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:38 google-dataproc-startup[811]: <13>Oct 13 23:00:38 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:38 google-dataproc-startup[811]: <13>Oct 13 23:00:38 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 3306
<13>Oct 13 23:00:38 google-dataproc-startup[811]: <13>Oct 13 23:00:38 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:00:38 google-dataproc-startup[811]: <13>Oct 13 23:00:38 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 4.'
<13>Oct 13 23:00:38 google-dataproc-startup[811]: <13>Oct 13 23:00:38 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 4.'
<13>Oct 13 23:00:38 google-dataproc-startup[811]: <13>Oct 13 23:00:38 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 4.
<13>Oct 13 23:00:38 google-dataproc-startup[811]: <13>Oct 13 23:00:38 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-yarn-resourcemanager[1466]: + update_succeeded=1
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-yarn-resourcemanager[1466]: + break
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-yarn-resourcemanager[1466]: + ((  1  ))
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-yarn-resourcemanager[1466]: ++ systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-mapreduce-historyserver[1469]: + update_succeeded=1
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-mapreduce-historyserver[1469]: + break
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-mapreduce-historyserver[1469]: + ((  1  ))
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-mapreduce-historyserver[1469]: ++ systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-spark-history-server[1479]: + update_succeeded=1
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-spark-history-server[1479]: + break
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-spark-history-server[1479]: + ((  1  ))
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-spark-history-server[1479]: ++ systemctl show spark-history-server.service -p Restart,RemainAfterExit
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 3306
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 5.'
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 5.'
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 5.
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hive-server2[1468]: + update_succeeded=1
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hive-server2[1468]: + break
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hive-server2[1468]: + ((  1  ))
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hive-server2[1468]: ++ systemctl show hive-server2.service -p Restart,RemainAfterExit
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-hdfs-secondarynamenode[1498]: + update_succeeded=1
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-hdfs-secondarynamenode[1498]: + break
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-hdfs-secondarynamenode[1498]: + ((  1  ))
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 setup-hadoop-hdfs-secondarynamenode[1498]: ++ systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit
<13>Oct 13 23:00:39 google-dataproc-startup[811]: <13>Oct 13 23:00:39 uninstall[1403]: Reading package lists...
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + update_succeeded=1
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + break
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + ((  1  ))
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: ++ systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + update_succeeded=1
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + break
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + ((  1  ))
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: ++ systemctl show google-fluentd.service -p Restart,RemainAfterExit
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + local 'props=Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: RemainAfterExit=no'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + local drop_in_dir=/etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + mkdir /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: + local 'props=Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: RemainAfterExit=no'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: + local drop_in_dir=/etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: + mkdir /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + in_array hadoop-yarn-resourcemanager DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + local value=hadoop-yarn-resourcemanager
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-yarn-resourcemanager  ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + return 1
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + run_with_retries systemctl start hadoop-yarn-resourcemanager
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + local retry_backoff
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + cmd=("$@")
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + local -a cmd
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + loginfo 'About to run '\''systemctl start hadoop-yarn-resourcemanager'\'' with retries...'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + echo 'About to run '\''systemctl start hadoop-yarn-resourcemanager'\'' with retries...'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: About to run 'systemctl start hadoop-yarn-resourcemanager' with retries...
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + local update_succeeded=0
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + (( i = 0 ))
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + (( i < 12 ))
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: + systemctl start hadoop-yarn-resourcemanager
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: + in_array hadoop-mapreduce-historyserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: + local value=hadoop-mapreduce-historyserver
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-mapreduce-historyserver  ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-mapreduce-historyserver[1469]: + return
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: + local 'props=Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: RemainAfterExit=no'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: + local drop_in_dir=/etc/systemd/system/spark-history-server.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: + mkdir /etc/systemd/system/spark-history-server.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/spark-history-server.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: + in_array spark-history-server DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: + local value=spark-history-server
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  spark-history-server  ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-spark-history-server[1479]: + return
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + local 'props=Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: RemainAfterExit=no'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + local drop_in_dir=/etc/systemd/system/hive-server2.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + mkdir /etc/systemd/system/hive-server2.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-server2.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + local 'props=Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: RemainAfterExit=no'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + local drop_in_dir=/etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + mkdir /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + in_array hive-server2 DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + local value=hive-server2
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hive-server2  ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + return 1
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + [[ hive-server2 == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-server2[1468]: + return
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + in_array hadoop-hdfs-secondarynamenode DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + local value=hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-hdfs-secondarynamenode  ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + return 1
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + run_with_retries systemctl start hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + local retry_backoff
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + cmd=("$@")
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + local -a cmd
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + loginfo 'About to run '\''systemctl start hadoop-hdfs-secondarynamenode'\'' with retries...'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + echo 'About to run '\''systemctl start hadoop-hdfs-secondarynamenode'\'' with retries...'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: About to run 'systemctl start hadoop-hdfs-secondarynamenode' with retries...
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + local update_succeeded=0
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + (( i = 0 ))
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + (( i < 12 ))
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: + systemctl start hadoop-hdfs-secondarynamenode
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + local 'props=Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: RemainAfterExit=no'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + local drop_in_dir=/etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + mkdir /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + local 'props=Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: RemainAfterExit=yes'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + [[ Restart=no
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + in_array google-fluentd DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + local value=google-fluentd
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  google-fluentd  ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + return 1
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + [[ google-fluentd == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + run_with_retries systemctl start google-fluentd
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + local retry_backoff
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + cmd=("$@")
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + local -a cmd
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + loginfo 'About to run '\''systemctl start google-fluentd'\'' with retries...'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + echo 'About to run '\''systemctl start google-fluentd'\'' with retries...'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: About to run 'systemctl start google-fluentd' with retries...
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + local update_succeeded=0
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + (( i = 0 ))
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + (( i < 12 ))
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-google-fluentd[1573]: + systemctl start google-fluentd
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + in_array hadoop-yarn-timelineserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + local value=hadoop-yarn-timelineserver
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-yarn-timelineserver  ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + return 1
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + run_with_retries systemctl start hadoop-yarn-timelineserver
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + local retry_backoff
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + cmd=("$@")
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + local -a cmd
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + loginfo 'About to run '\''systemctl start hadoop-yarn-timelineserver'\'' with retries...'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + echo 'About to run '\''systemctl start hadoop-yarn-timelineserver'\'' with retries...'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: About to run 'systemctl start hadoop-yarn-timelineserver' with retries...
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + local update_succeeded=0
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + (( i = 0 ))
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + (( i < 12 ))
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: + systemctl start hadoop-yarn-timelineserver
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-resourcemanager[1466]: Warning: hadoop-yarn-resourcemanager.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-hdfs-secondarynamenode[1498]: Warning: hadoop-hdfs-secondarynamenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hadoop-yarn-timelineserver[1489]: Warning: hadoop-yarn-timelineserver.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 3306
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 6.'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 6.'
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 6.
<13>Oct 13 23:00:40 google-dataproc-startup[811]: <13>Oct 13 23:00:40 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:41 google-dataproc-startup[811]: <13>Oct 13 23:00:41 uninstall[1403]: Building dependency tree...
<13>Oct 13 23:00:41 google-dataproc-startup[811]: <13>Oct 13 23:00:41 uninstall[1403]: Reading state information...
<13>Oct 13 23:00:41 google-dataproc-startup[811]: <13>Oct 13 23:00:41 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:41 google-dataproc-startup[811]: <13>Oct 13 23:00:41 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 3306
<13>Oct 13 23:00:41 google-dataproc-startup[811]: <13>Oct 13 23:00:41 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:00:41 google-dataproc-startup[811]: <13>Oct 13 23:00:41 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 7.'
<13>Oct 13 23:00:41 google-dataproc-startup[811]: <13>Oct 13 23:00:41 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 7.'
<13>Oct 13 23:00:41 google-dataproc-startup[811]: <13>Oct 13 23:00:41 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 7.
<13>Oct 13 23:00:41 google-dataproc-startup[811]: <13>Oct 13 23:00:41 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]: The following packages will be REMOVED:
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   bind9-host* druid* fonts-font-awesome* fonts-mathjax* geoip-database*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   hadoop-hdfs-datanode* hadoop-hdfs-journalnode* hadoop-hdfs-zkfc*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   hadoop-yarn-nodemanager* hive-webhcat* hive-webhcat-server*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   javascript-common* kafka* kafka-server* knox* krb5-admin-server*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   krb5-config* krb5-kdc* krb5-kpropd* krb5-user* libbind9-140* libc-ares2*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   libdns162* libev4* libfile-copy-recursive-perl* libgeoip1* libgssrpc4*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   libhttp-parser2.8* libisc160* libisccc140* libisccfg140* libjs-bootstrap*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   libjs-d3* libjs-es5-shim* libjs-highlight.js* libjs-jquery*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   libjs-jquery-datatables* libjs-jquery-metadata* libjs-jquery-selectize.js*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   libjs-jquery-tablesorter* libjs-jquery-ui* libjs-json* libjs-mathjax*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   libjs-microplugin.js* libjs-modernizr* libjs-prettify* libjs-sifter.js*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   libjs-twitter-bootstrap* libjs-twitter-bootstrap-datepicker*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   libkadm5clnt-mit11* libkadm5srv-mit11* libkdb5-8* liblua5.1-0*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   libluajit-5.1-2* libluajit-5.1-common* liblwres141* libuv1* libverto-libev1*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   libverto1* libyaml-0-2* littler* node-highlight.js* node-normalize.css*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   nodejs* nodejs-doc* pandoc* pandoc-data* r-cran-assertthat*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-base64enc* r-cran-bindr* r-cran-bindrcpp* r-cran-bit* r-cran-bit64*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-cli* r-cran-colorspace* r-cran-crayon* r-cran-data.table* r-cran-dbi*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-digest* r-cran-dplyr* r-cran-evaluate* r-cran-fansi* r-cran-filehash*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-ggplot2* r-cran-glue* r-cran-googlevis* r-cran-gtable* r-cran-hexbin*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-highr* r-cran-hms* r-cran-htmltools* r-cran-htmlwidgets*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-httpuv* r-cran-jsonlite* r-cran-knitr* r-cran-labeling* r-cran-later*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-lazyeval* r-cran-littler* r-cran-magrittr* r-cran-mapproj*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-maps* r-cran-markdown* r-cran-memoise* r-cran-mime* r-cran-munsell*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-pillar* r-cran-pkgconfig* r-cran-pkgkitten* r-cran-plyr* r-cran-png*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-praise* r-cran-promises* r-cran-purrr* r-cran-r6*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-rcolorbrewer* r-cran-rcpp* r-cran-reshape2* r-cran-rlang*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-rmarkdown* r-cran-rsqlite* r-cran-scales* r-cran-shiny*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-sourcetools* r-cran-sp* r-cran-stringi* r-cran-stringr*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-testit* r-cran-testthat* r-cran-tibble* r-cran-tidyselect*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-tikzdevice* r-cran-tinytex* r-cran-utf8* r-cran-viridislite*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   r-cran-withr* r-cran-xfun* r-cran-xml2* r-cran-xtable* r-cran-yaml* solr*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 uninstall[1403]:   solr-server* update-inetd* xinetd* zeppelin* zookeeper-server*
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 3306
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 8.'
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 8.'
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 3306 failed. Retry attempt: 8.
<13>Oct 13 23:00:42 google-dataproc-startup[811]: <13>Oct 13 23:00:42 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: + update_succeeded=1
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: + break
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: + ((  1  ))
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: + [[ mariadb == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: ++ local property_name=am.primary_only
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: +++ local property_name=am.primary_only
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: ++++ tail -n 1
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: +++ local property_value=false
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: +++ echo false
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: ++ local property_value=false
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: ++ echo false
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-mariadb[1490]: + [[ mariadb == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 3306
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: Connection to cluster-6d45-m 3306 port [tcp/mysql] succeeded!
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + update_succeeded=1
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 3306 succeeded.'
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 3306 succeeded.'
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 3306 succeeded.
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + break
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + ((  1  ))
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + loginfo 'Service up on host=cluster-6d45-m port=3306.'
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + echo 'Service up on host=cluster-6d45-m port=3306.'
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: Service up on host=cluster-6d45-m port=3306.
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + enable_service hive-metastore
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + local service=hive-metastore
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + local unit=hive-metastore.service
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + run_with_retries systemctl enable hive-metastore.service
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + local retry_backoff
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + cmd=("$@")
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + local -a cmd
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + loginfo 'About to run '\''systemctl enable hive-metastore.service'\'' with retries...'
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + echo 'About to run '\''systemctl enable hive-metastore.service'\'' with retries...'
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: About to run 'systemctl enable hive-metastore.service' with retries...
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + local update_succeeded=0
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + (( i = 0 ))
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + (( i < 12 ))
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: + systemctl enable hive-metastore.service
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: hive-metastore.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:00:43 google-dataproc-startup[811]: <13>Oct 13 23:00:43 setup-hive-metastore[1467]: Executing: /lib/systemd/systemd-sysv-install enable hive-metastore
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + update_succeeded=1
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + break
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + ((  1  ))
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: ++ systemctl show hive-metastore.service -p Restart,RemainAfterExit
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + local 'props=Restart=no
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: RemainAfterExit=no'
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + [[ Restart=no
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + [[ Restart=no
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + local drop_in_dir=/etc/systemd/system/hive-metastore.service.d
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + mkdir /etc/systemd/system/hive-metastore.service.d
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-metastore.service.d
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + in_array hive-metastore DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + local value=hive-metastore
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hive-metastore  ]]
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + return 1
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + [[ hive-metastore == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + run_with_retries systemctl start hive-metastore
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + local retry_backoff
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + cmd=("$@")
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + local -a cmd
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + loginfo 'About to run '\''systemctl start hive-metastore'\'' with retries...'
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + echo 'About to run '\''systemctl start hive-metastore'\'' with retries...'
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: About to run 'systemctl start hive-metastore' with retries...
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + local update_succeeded=0
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + (( i = 0 ))
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + (( i < 12 ))
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: + systemctl start hive-metastore
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hive-metastore[1467]: Warning: hive-metastore.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 23:00:46 google-dataproc-startup[811]: <13>Oct 13 23:00:46 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:00:46.772+0000: 4.546: [GC (Allocation Failure) 2019-10-13T23:00:46.772+0000: 4.546: [ParNew: 15936K->1920K(17856K), 0.1256999 secs] 15936K->3244K(57472K), 0.1257766 secs] [Times: user=0.00 sys=0.00, real=0.12 secs] 
<13>Oct 13 23:00:47 google-dataproc-startup[811]: <13>Oct 13 23:00:47 uninstall[1403]: 0 upgraded, 0 newly installed, 146 to remove and 1 not upgraded.
<13>Oct 13 23:00:47 google-dataproc-startup[811]: <13>Oct 13 23:00:47 uninstall[1403]: After this operation, 2,006 MB disk space will be freed.
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: + update_succeeded=1
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: + break
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: + ((  1  ))
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: + update_succeeded=1
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: + break
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: + ((  1  ))
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: ++ local property_name=am.primary_only
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: ++ local property_name=am.primary_only
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: +++ local property_name=am.primary_only
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: +++ local property_name=am.primary_only
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: ++++ tail -n 1
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: ++++ tail -n 1
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: +++ local property_value=false
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: +++ echo false
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: ++ local property_value=false
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: ++ echo false
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: + [[ 0 -eq 0 ]]
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-resourcemanager[1466]: + [[ false == \t\r\u\e ]]
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: +++ local property_value=false
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: +++ echo false
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: ++ local property_value=false
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: ++ echo false
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:00:48 google-dataproc-startup[811]: <13>Oct 13 23:00:48 setup-hadoop-yarn-timelineserver[1489]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:48 INFO namenode.NameNode: STARTUP_MSG: 
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: /************************************************************
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: STARTUP_MSG: Starting NameNode
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: STARTUP_MSG:   host = cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal/10.128.0.13
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: STARTUP_MSG:   args = [-format, -nonInteractive]
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: STARTUP_MSG:   version = 2.9.2
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/hadoop-lzo-0.4.20.jar:/usr/lib/hadoop/lib/httpclient-4.5.2.jar:/usr/lib/hadoop/lib/gcs-connector-hadoop2-1.9.17.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/gcs-connector.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.1
<13>Oct 13 23:00:49 google-dataproc-startup[811]: 3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jetty-6.1.26.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0
<13>Oct 13 23:00:49 google-dataproc-startup[811]: .4.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/u
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: sr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/httpcore-4.4.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/li
<13>Oct 13 23:00:49 google-dataproc-startup[811]: b/jetty-sslengine-6.1.26.jar:/usr/lib/hadoop/lib/m
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: ockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.9.2.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-common-2.9.2-tests.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-2.9.2.jar:/usr/lib/hadoop/.//hadoop-nfs-2.9.2.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth-2.9.2.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib
<13>Oct 13 23:00:49 google-dataproc-startup[811]: /hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoo
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: p-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2
<13>Oct 13 23:00:49 google-dataproc-startup[811]: .7.8.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: .jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.9.2-tests.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/l
<13>Oct 13 23:00:49 google-dataproc-startup[811]: ib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: /hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/api-as
<13>Oct 13 23:00:49 google-dataproc-startup[811]: n1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/comm
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: ons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hado
<13>Oct 13 23:00:49 google-dataproc-startup[811]: op-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/ha
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: doop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/ht
<13>Oct 13 23:00:49 google-dataproc-startup[811]: tpcore-4.4.4.jar:/usr/lib/hadoop-yarn/lib/jetty-ut
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: il-6.1.26.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.9.2.jar:/usr
<13>Oct 13 23:00:49 google-dataproc-startup[811]: /lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/li
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: b/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-y
<13>Oct 13 23:00:49 google-dataproc-startup[811]: arn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-ya
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: rn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.9.2.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce
<13>Oct 13 23:00:49 google-dataproc-startup[811]: /lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapredu
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: ce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//l
<13>Oct 13 23:00:49 google-dataproc-startup[811]: eveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/./
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: /jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ar
<13>Oct 13 23:00:49 google-dataproc-startup[811]: chive-logs.jar:/usr/lib/hadoop-mapreduce/.//log4j-
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: 1.2.17.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//h
<13>Oct 13 23:00:49 google-dataproc-startup[811]: adoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hado
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: op-archives-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.199.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2
<13>Oct 13 23:00:49 google-dataproc-startup[811]: .0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//mssql-jd
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: bc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client
<13>Oct 13 23:00:49 google-dataproc-startup[811]: -hs-plugins-2.9.2.jar:/usr/lib/hadoop-mapreduce/./
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: /hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.9.2-tests.jar:/usr
<13>Oct 13 23:00:49 google-dataproc-startup[811]: /lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: /lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.9.2.jar:/usr/lib/hadoop-ma
<13>Oct 13 23:00:49 google-dataproc-startup[811]: preduce/.//json-20170516.jar:/usr/lib/hadoop-mapre
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: duce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.5.j
<13>Oct 13 23:00:49 google-dataproc-startup[811]: ar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: .jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sd
<13>Oct 13 23:00:49 google-dataproc-startup[811]: k-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ar
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: chive-logs-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.13.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: STARTUP_MSG:   build = https://bigdataoss-internal.googlesource.com/third_party/apache/hadoop -r 849ee9eda72c7e8b1eb9fc5a830432c887914111; compiled by 'bigtop' on 2019-09-18T10:58Z
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: STARTUP_MSG:   java = 1.8.0_222
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: ************************************************************/
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 uninstall[1403]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 119619 files and directories currently installed.)
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:49 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
<13>Oct 13 23:00:49 google-dataproc-startup[811]: <13>Oct 13 23:00:49 uninstall[1403]: Removing krb5-admin-server (1.15-1+deb9u1) ...
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + update_succeeded=1
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + break
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + ((  1  ))
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + [[ hive-metastore == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + wait_for_port cluster-6d45-m 9083
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + local -r host=cluster-6d45-m
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + local -r port=9083
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + loginfo 'Waiting for service to come up on host=cluster-6d45-m port=9083.'
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + echo 'Waiting for service to come up on host=cluster-6d45-m port=9083.'
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: Waiting for service to come up on host=cluster-6d45-m port=9083.
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + retry_with_constant_backoff nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + local max_retry=300
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + cmd=("$@")
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + local -a cmd
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + local update_succeeded=0
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: ++ seq 1 300
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:50 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 1.'
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 1.'
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 1.
<13>Oct 13 23:00:50 google-dataproc-startup[811]: <13>Oct 13 23:00:50 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:51 google-dataproc-startup[811]: <13>Oct 13 23:00:51 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:51 google-dataproc-startup[811]: <13>Oct 13 23:00:51 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:00:51 google-dataproc-startup[811]: <13>Oct 13 23:00:51 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:00:51 google-dataproc-startup[811]: <13>Oct 13 23:00:51 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 2.'
<13>Oct 13 23:00:51 google-dataproc-startup[811]: <13>Oct 13 23:00:51 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 2.'
<13>Oct 13 23:00:51 google-dataproc-startup[811]: <13>Oct 13 23:00:51 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 2.
<13>Oct 13 23:00:51 google-dataproc-startup[811]: <13>Oct 13 23:00:51 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:51 google-dataproc-startup[811]: <13>Oct 13 23:00:51 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:00:51.299+0000: 9.074: [GC (Allocation Failure) 2019-10-13T23:00:51.299+0000: 9.074: [ParNew: 17856K->1920K(17856K), 0.2340534 secs] 19180K->5700K(57472K), 0.2341233 secs] [Times: user=0.02 sys=0.00, real=0.24 secs] 
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: + update_succeeded=1
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: + break
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: + ((  1  ))
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: ++ local property_name=am.primary_only
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: +++ local property_name=am.primary_only
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: ++++ tail -n 1
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: ++++ cut -d = -f 2-
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: +++ local property_value=false
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: +++ echo false
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: ++ local property_value=false
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: ++ echo false
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hadoop-hdfs-secondarynamenode[1498]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 3.'
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 3.'
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 3.
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:52 google-dataproc-startup[811]: <13>Oct 13 23:00:52 uninstall[1403]: Removing krb5-kpropd (1.15-1+deb9u1) ...
<13>Oct 13 23:00:53 google-dataproc-startup[811]: <13>Oct 13 23:00:53 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:53 google-dataproc-startup[811]: <13>Oct 13 23:00:53 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:00:53 google-dataproc-startup[811]: <13>Oct 13 23:00:53 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:00:53 google-dataproc-startup[811]: <13>Oct 13 23:00:53 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 4.'
<13>Oct 13 23:00:53 google-dataproc-startup[811]: <13>Oct 13 23:00:53 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 4.'
<13>Oct 13 23:00:53 google-dataproc-startup[811]: <13>Oct 13 23:00:53 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 4.
<13>Oct 13 23:00:53 google-dataproc-startup[811]: <13>Oct 13 23:00:53 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:54 google-dataproc-startup[811]: <13>Oct 13 23:00:54 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:00:54.052+0000: 11.827: [GC (Allocation Failure) 2019-10-13T23:00:54.052+0000: 11.827: [ParNew: 17856K->1766K(17856K), 0.0518917 secs] 21636K->6323K(57472K), 0.0519615 secs] [Times: user=0.01 sys=0.00, real=0.05 secs] 
<13>Oct 13 23:00:54 google-dataproc-startup[811]: <13>Oct 13 23:00:54 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:54 WARN common.Util: Path /hadoop/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
<13>Oct 13 23:00:54 google-dataproc-startup[811]: <13>Oct 13 23:00:54 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:54 WARN common.Util: Path /hadoop/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
<13>Oct 13 23:00:54 google-dataproc-startup[811]: <13>Oct 13 23:00:54 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:54 google-dataproc-startup[811]: <13>Oct 13 23:00:54 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:00:54 google-dataproc-startup[811]: <13>Oct 13 23:00:54 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:00:54 google-dataproc-startup[811]: <13>Oct 13 23:00:54 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 5.'
<13>Oct 13 23:00:54 google-dataproc-startup[811]: <13>Oct 13 23:00:54 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 5.'
<13>Oct 13 23:00:54 google-dataproc-startup[811]: <13>Oct 13 23:00:54 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 5.
<13>Oct 13 23:00:54 google-dataproc-startup[811]: <13>Oct 13 23:00:54 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:54 google-dataproc-startup[811]: <13>Oct 13 23:00:54 setup-hadoop-hdfs-namenode[1465]: Formatting using clusterid: CID-57c79ee1-21dd-42ac-9707-f7f41e753b6d
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:55 INFO namenode.FSEditLog: Edit logging is async:true
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 6.'
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 6.'
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 6.
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:55 INFO namenode.FSNamesystem: KeyProvider: null
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:55 INFO namenode.FSNamesystem: fsLock is fair: true
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:55 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:55 INFO namenode.FSNamesystem: fsOwner             = hdfs (auth:SIMPLE)
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:55 INFO namenode.FSNamesystem: supergroup          = hadoop
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:55 INFO namenode.FSNamesystem: isPermissionEnabled = false
<13>Oct 13 23:00:55 google-dataproc-startup[811]: <13>Oct 13 23:00:55 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:55 INFO namenode.FSNamesystem: HA Enabled: false
<13>Oct 13 23:00:56 google-dataproc-startup[811]: <13>Oct 13 23:00:56 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:00:56 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
<13>Oct 13 23:00:56 google-dataproc-startup[811]: <13>Oct 13 23:00:56 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:00:56.462+0000: 14.236: [GC (Allocation Failure) 2019-10-13T23:00:56.462+0000: 14.236: [ParNew: 17702K->1818K(17856K), 0.0658946 secs] 22259K->6982K(57472K), 0.0659683 secs] [Times: user=0.01 sys=0.00, real=0.07 secs] 
<13>Oct 13 23:00:56 google-dataproc-startup[811]: <13>Oct 13 23:00:56 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:56 google-dataproc-startup[811]: <13>Oct 13 23:00:56 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:00:56 google-dataproc-startup[811]: <13>Oct 13 23:00:56 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:00:56 google-dataproc-startup[811]: <13>Oct 13 23:00:56 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 7.'
<13>Oct 13 23:00:56 google-dataproc-startup[811]: <13>Oct 13 23:00:56 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 7.'
<13>Oct 13 23:00:56 google-dataproc-startup[811]: <13>Oct 13 23:00:56 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 7.
<13>Oct 13 23:00:56 google-dataproc-startup[811]: <13>Oct 13 23:00:56 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:57 google-dataproc-startup[811]: <13>Oct 13 23:00:57 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:57 google-dataproc-startup[811]: <13>Oct 13 23:00:57 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:00:57 google-dataproc-startup[811]: <13>Oct 13 23:00:57 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:00:57 google-dataproc-startup[811]: <13>Oct 13 23:00:57 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 8.'
<13>Oct 13 23:00:57 google-dataproc-startup[811]: <13>Oct 13 23:00:57 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 8.'
<13>Oct 13 23:00:57 google-dataproc-startup[811]: <13>Oct 13 23:00:57 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 8.
<13>Oct 13 23:00:57 google-dataproc-startup[811]: <13>Oct 13 23:00:57 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:57 google-dataproc-startup[811]: <13>Oct 13 23:00:57 uninstall[1403]: Removing krb5-kdc (1.15-1+deb9u1) ...
<13>Oct 13 23:00:58 google-dataproc-startup[811]: <13>Oct 13 23:00:58 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:58 google-dataproc-startup[811]: <13>Oct 13 23:00:58 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:00:58 google-dataproc-startup[811]: <13>Oct 13 23:00:58 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:00:58 google-dataproc-startup[811]: <13>Oct 13 23:00:58 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 9.'
<13>Oct 13 23:00:58 google-dataproc-startup[811]: <13>Oct 13 23:00:58 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 9.'
<13>Oct 13 23:00:58 google-dataproc-startup[811]: <13>Oct 13 23:00:58 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 9.
<13>Oct 13 23:00:58 google-dataproc-startup[811]: <13>Oct 13 23:00:58 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:00:58 google-dataproc-startup[811]: <13>Oct 13 23:00:58 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:00:58.649+0000: 16.423: [GC (Allocation Failure) 2019-10-13T23:00:58.649+0000: 16.423: [ParNew: 17754K->1919K(17856K), 0.0586415 secs] 22918K->8247K(57472K), 0.0587149 secs] [Times: user=0.01 sys=0.00, real=0.06 secs] 
<13>Oct 13 23:00:59 google-dataproc-startup[811]: <13>Oct 13 23:00:59 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:00:59 google-dataproc-startup[811]: <13>Oct 13 23:00:59 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:00:59 google-dataproc-startup[811]: <13>Oct 13 23:00:59 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:00:59 google-dataproc-startup[811]: <13>Oct 13 23:00:59 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 10.'
<13>Oct 13 23:00:59 google-dataproc-startup[811]: <13>Oct 13 23:00:59 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 10.'
<13>Oct 13 23:00:59 google-dataproc-startup[811]: <13>Oct 13 23:00:59 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 10.
<13>Oct 13 23:00:59 google-dataproc-startup[811]: <13>Oct 13 23:00:59 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO util.HostsFileReader: Adding a node "cluster-6d45-w-0.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO util.HostsFileReader: Adding a node "cluster-6d45-w-1.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO util.HostsFileReader: Adding a node "cluster-6d45-w-2.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO util.HostsFileReader: Adding a node "cluster-6d45-w-3.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO util.HostsFileReader: Adding a node "cluster-6d45-w-4.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO util.HostsFileReader: Adding a node "cluster-6d45-w-5.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.retry-hostname-dns-lookup=true
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Oct 13 23:01:00
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO util.GSet: Computing capacity for map BlocksMap
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO util.GSet: 2.0% max memory 731.7 MB = 14.6 MB
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO util.GSet: capacity      = 2^21 = 2097152 entries
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:00.690+0000: 18.465: [GC (Allocation Failure) 2019-10-13T23:01:00.690+0000: 18.465: [ParNew: 17221K->1920K(17856K), 0.0099118 secs] 23548K->8529K(57472K), 0.0099829 secs] [Times: user=0.00 sys=0.00, real=0.03 secs] 
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 11.'
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 11.'
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 11.
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
<13>Oct 13 23:01:00 google-dataproc-startup[811]: <13>Oct 13 23:01:00 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:00 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO blockmanagement.BlockManager: defaultReplication         = 2
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO blockmanagement.BlockManager: maxReplication             = 512
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO blockmanagement.BlockManager: minReplication             = 1
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO namenode.FSNamesystem: Append Enabled: true
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO util.GSet: Computing capacity for map INodeMap
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO util.GSet: 1.0% max memory 731.7 MB = 7.3 MB
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO util.GSet: capacity      = 2^20 = 1048576 entries
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 12.'
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 12.'
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 12.
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:01.660+0000: 19.434: [GC (Allocation Failure) 2019-10-13T23:01:01.660+0000: 19.434: [ParNew: 17856K->733K(17856K), 0.2038078 secs] 24465K->15840K(57472K), 0.2038836 secs] [Times: user=0.03 sys=0.00, real=0.21 secs] 
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO namenode.FSDirectory: ACLs enabled? false
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO namenode.FSDirectory: XAttrs enabled? true
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO namenode.NameNode: Caching file names occurring more than 10 times
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO util.GSet: Computing capacity for map cachedBlocks
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO util.GSet: 0.25% max memory 731.7 MB = 1.8 MB
<13>Oct 13 23:01:01 google-dataproc-startup[811]: <13>Oct 13 23:01:01 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:01 INFO util.GSet: capacity      = 2^18 = 262144 entries
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO util.GSet: Computing capacity for map NameNodeRetryCache
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO util.GSet: 0.029999999329447746% max memory 731.7 MB = 224.8 KB
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO util.GSet: capacity      = 2^15 = 32768 entries
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1275145929-10.128.0.13-1571007662345
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO common.Storage: Storage directory /hadoop/dfs/name has been successfully formatted.
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:02 INFO namenode.FSImageFormatProtobuf: Saving image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 13.'
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 13.'
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 13.
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:02 google-dataproc-startup[811]: <13>Oct 13 23:01:02 uninstall[1403]: Removing krb5-user (1.15-1+deb9u1) ...
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 uninstall[1403]: Removing krb5-config (2.6) ...
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 uninstall[1403]: Removing bind9-host (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 uninstall[1403]: Removing druid (0.13.0-incubating-1) ...
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:03.375+0000: 21.149: [GC (Allocation Failure) 2019-10-13T23:01:03.375+0000: 21.150: [ParNew: 16669K->1920K(17856K), 0.1214713 secs] 31776K->21827K(57472K), 0.1215429 secs] [Times: user=0.01 sys=0.00, real=0.12 secs] 
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:03.500+0000: 21.274: [GC (CMS Initial Mark) [1 CMS-initial-mark: 19907K(39616K)] 21962K(57472K), 0.0169917 secs] [Times: user=0.01 sys=0.00, real=0.02 secs] 
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:03.520+0000: 21.295: [CMS-concurrent-mark-start]
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 uninstall[1403]: dpkg: warning: while removing druid, directory '/usr/lib/druid/extensions/mysql-metadata-storage' not empty so not removed
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 uninstall[1403]: dpkg: warning: while removing druid, directory '/usr/lib/druid/conf/druid/_common' not empty so not removed
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 uninstall[1403]: Removing r-cran-rmarkdown (1.11+dfsg-1~bpo9+1) ...
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:03.611+0000: 21.414: [CMS-concurrent-mark: 0.090/0.091 secs] [Times: user=0.02 sys=0.00, real=0.12 secs] 
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:03.641+0000: 21.415: [CMS-concurrent-preclean-start]
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:03.642+0000: 21.417: [CMS-concurrent-preclean: 0.001/0.001 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:03.671+0000: 21.446: [GC (CMS Final Remark) [YG occupancy: 3610 K (17856 K)]2019-10-13T23:01:03.672+0000: 21.446: [Rescan (parallel) , 0.0024666 secs]2019-10-13T23:01:03.674+0000: 21.449: [weak refs processing, 0.0000924 secs]2019-10-13T23:01:03.674+0000: 21.449: [class unloading, 0.0124204 secs]2019-10-13T23:01:03.687+0000: 21.461: [scrub symbol table, 0.0729910 secs]2019-10-13T23:01:03.760+0000: 21.534: [scrub string table, 0.0004483 secs][1 CMS-remark: 19907K(39616K)] 23518K(57472K), 0.0887051 secs] [Times: user=0.01 sys=0.00, real=0.08 secs] 
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:03.776+0000: 21.550: [CMS-concurrent-sweep-start]
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:03.779+0000: 21.553: [CMS-concurrent-sweep: 0.003/0.003 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:03.780+0000: 21.554: [CMS-concurrent-reset-start]
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 14.'
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 14.'
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 14.
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 2019-10-13T23:01:03.869+0000: 21.669: [CMS-concurrent-reset: 0.089/0.089 secs] [Times: user=0.01 sys=0.01, real=0.12 secs] 
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 uninstall[1403]: Removing r-cran-shiny (1.2.0+dfsg-1~bpo9+1) ...
<13>Oct 13 23:01:03 google-dataproc-startup[811]: <13>Oct 13 23:01:03 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:03 INFO namenode.FSImageFormatProtobuf: Image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 319 bytes saved in 1 seconds .
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:04 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 uninstall[1403]: Removing fonts-font-awesome (4.7.0~dfsg-1) ...
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: 19/10/13 23:01:04 INFO namenode.NameNode: SHUTDOWN_MSG: 
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: /************************************************************
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: SHUTDOWN_MSG: Shutting down NameNode at cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal/10.128.0.13
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: ************************************************************/
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 uninstall[1403]: Removing r-cran-knitr (1.21+dfsg-2~bpo9+1) ...
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: Heap
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]:  par new generation   total 17856K, used 8976K [0x00000000d1c00000, 0x00000000d2f50000, 0x00000000d6f30000)
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]:   eden space 15936K,  44% used [0x00000000d1c00000, 0x00000000d22e4090, 0x00000000d2b90000)
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]:   from space 1920K, 100% used [0x00000000d2b90000, 0x00000000d2d70000, 0x00000000d2d70000)
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]:   to   space 1920K,   0% used [0x00000000d2d70000, 0x00000000d2d70000, 0x00000000d2f50000)
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]:  concurrent mark-sweep generation total 39616K, used 17002K [0x00000000d6f30000, 0x00000000d95e0000, 0x0000000100000000)
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]:  Metaspace       used 18999K, capacity 19212K, committed 19456K, reserved 1067008K
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]:   class space    used 2133K, capacity 2192K, committed 2304K, reserved 1048576K
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 uninstall[1403]: Removing r-cran-markdown (0.9+dfsg-1~bpo9+1) ...
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + update_succeeded=1
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + break
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + ((  1  ))
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + enable_service hadoop-hdfs-namenode
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + local service=hadoop-hdfs-namenode
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + local unit=hadoop-hdfs-namenode.service
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + run_with_retries systemctl enable hadoop-hdfs-namenode.service
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + local retry_backoff
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + cmd=("$@")
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + local -a cmd
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + loginfo 'About to run '\''systemctl enable hadoop-hdfs-namenode.service'\'' with retries...'
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + echo 'About to run '\''systemctl enable hadoop-hdfs-namenode.service'\'' with retries...'
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: About to run 'systemctl enable hadoop-hdfs-namenode.service' with retries...
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + local update_succeeded=0
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + (( i = 0 ))
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + (( i < 12 ))
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: + systemctl enable hadoop-hdfs-namenode.service
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: hadoop-hdfs-namenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hadoop-hdfs-namenode[1465]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-namenode
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 uninstall[1403]: Removing libjs-mathjax (2.7.0-2) ...
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 15.'
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 15.'
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 15.
<13>Oct 13 23:01:04 google-dataproc-startup[811]: <13>Oct 13 23:01:04 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: + update_succeeded=1
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: + break
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: + ((  1  ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: + [[ google-fluentd == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: ++ local property_name=am.primary_only
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: +++ local property_name=am.primary_only
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: ++++ cut -d = -f 2-
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: ++++ tail -n 1
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: +++ local property_value=false
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: +++ echo false
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + pid=1498
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + cmd='setup_service hadoop-hdfs-secondarynamenode'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1498 cmd=[setup_service hadoop-hdfs-secondarynamenode]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + echo 'Waiting on pid=1498 cmd=[setup_service hadoop-hdfs-secondarynamenode]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: Waiting on pid=1498 cmd=[setup_service hadoop-hdfs-secondarynamenode]
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + wait 1498
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + pid=1490
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + cmd='setup_service mariadb'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1490 cmd=[setup_service mariadb]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + echo 'Waiting on pid=1490 cmd=[setup_service mariadb]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: Waiting on pid=1490 cmd=[setup_service mariadb]
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + wait 1490
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + pid=1489
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + cmd='setup_service hadoop-yarn-timelineserver'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1489 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + echo 'Waiting on pid=1489 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: Waiting on pid=1489 cmd=[setup_service hadoop-yarn-timelineserver]
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + wait 1489
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + pid=1479
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + cmd='setup_service spark-history-server'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1479 cmd=[setup_service spark-history-server]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + echo 'Waiting on pid=1479 cmd=[setup_service spark-history-server]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: Waiting on pid=1479 cmd=[setup_service spark-history-server]
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + wait 1479
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + pid=1469
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + cmd='setup_service hadoop-mapreduce-historyserver'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1469 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + echo 'Waiting on pid=1469 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: Waiting on pid=1469 cmd=[setup_service hadoop-mapreduce-historyserver]
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + wait 1469
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + pid=1468
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + cmd='setup_service hive-server2'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1468 cmd=[setup_service hive-server2]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + echo 'Waiting on pid=1468 cmd=[setup_service hive-server2]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: Waiting on pid=1468 cmd=[setup_service hive-server2]
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + wait 1468
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + pid=1467
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + cmd='setup_service hive-metastore'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1467 cmd=[setup_service hive-metastore]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + echo 'Waiting on pid=1467 cmd=[setup_service hive-metastore]'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: Waiting on pid=1467 cmd=[setup_service hive-metastore]
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:01:05 google-dataproc-startup[811]: + wait 1467
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: ++ local property_value=false
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: ++ echo false
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-google-fluentd[1573]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 16.'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 16.'
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 16.
<13>Oct 13 23:01:05 google-dataproc-startup[811]: <13>Oct 13 23:01:05 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + update_succeeded=1
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + break
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + ((  1  ))
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: ++ systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + local 'props=Restart=no
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: RemainAfterExit=no'
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + [[ Restart=no
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + [[ Restart=no
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + local drop_in_dir=/etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + mkdir /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + in_array hadoop-hdfs-namenode DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + local value=hadoop-hdfs-namenode
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-hdfs-namenode  ]]
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + return 1
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + run_with_retries systemctl start hadoop-hdfs-namenode
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + local retry_backoff
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + cmd=("$@")
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + local -a cmd
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + loginfo 'About to run '\''systemctl start hadoop-hdfs-namenode'\'' with retries...'
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + echo 'About to run '\''systemctl start hadoop-hdfs-namenode'\'' with retries...'
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: About to run 'systemctl start hadoop-hdfs-namenode' with retries...
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + local update_succeeded=0
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + (( i = 0 ))
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + (( i < 12 ))
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: + systemctl start hadoop-hdfs-namenode
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hadoop-hdfs-namenode[1465]: Warning: hadoop-hdfs-namenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 17.'
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 17.'
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 17.
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:06 google-dataproc-startup[811]: <13>Oct 13 23:01:06 uninstall[1403]: Removing fonts-mathjax (2.7.0-2) ...
<13>Oct 13 23:01:07 google-dataproc-startup[811]: <13>Oct 13 23:01:07 uninstall[1403]: Removing geoip-database (20170512-1) ...
<13>Oct 13 23:01:07 google-dataproc-startup[811]: <13>Oct 13 23:01:07 uninstall[1403]: Removing hadoop-hdfs-datanode (2.9.2-1) ...
<13>Oct 13 23:01:08 google-dataproc-startup[811]: <13>Oct 13 23:01:07 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:08 google-dataproc-startup[811]: <13>Oct 13 23:01:07 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:08 google-dataproc-startup[811]: <13>Oct 13 23:01:08 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:08 google-dataproc-startup[811]: <13>Oct 13 23:01:08 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 18.'
<13>Oct 13 23:01:08 google-dataproc-startup[811]: <13>Oct 13 23:01:08 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 18.'
<13>Oct 13 23:01:08 google-dataproc-startup[811]: <13>Oct 13 23:01:08 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 18.
<13>Oct 13 23:01:08 google-dataproc-startup[811]: <13>Oct 13 23:01:08 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:08 google-dataproc-startup[811]: <13>Oct 13 23:01:08 uninstall[1403]: Removing hadoop-hdfs-journalnode (2.9.2-1) ...
<13>Oct 13 23:01:09 google-dataproc-startup[811]: <13>Oct 13 23:01:09 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:09 google-dataproc-startup[811]: <13>Oct 13 23:01:09 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:09 google-dataproc-startup[811]: <13>Oct 13 23:01:09 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:09 google-dataproc-startup[811]: <13>Oct 13 23:01:09 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 19.'
<13>Oct 13 23:01:09 google-dataproc-startup[811]: <13>Oct 13 23:01:09 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 19.'
<13>Oct 13 23:01:09 google-dataproc-startup[811]: <13>Oct 13 23:01:09 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 19.
<13>Oct 13 23:01:09 google-dataproc-startup[811]: <13>Oct 13 23:01:09 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:10 google-dataproc-startup[811]: <13>Oct 13 23:01:10 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:10 google-dataproc-startup[811]: <13>Oct 13 23:01:10 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:10 google-dataproc-startup[811]: <13>Oct 13 23:01:10 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:10 google-dataproc-startup[811]: <13>Oct 13 23:01:10 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 20.'
<13>Oct 13 23:01:10 google-dataproc-startup[811]: <13>Oct 13 23:01:10 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 20.'
<13>Oct 13 23:01:10 google-dataproc-startup[811]: <13>Oct 13 23:01:10 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 20.
<13>Oct 13 23:01:10 google-dataproc-startup[811]: <13>Oct 13 23:01:10 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:10 google-dataproc-startup[811]: <13>Oct 13 23:01:10 uninstall[1403]: Removing hadoop-hdfs-zkfc (2.9.2-1) ...
<13>Oct 13 23:01:11 google-dataproc-startup[811]: <13>Oct 13 23:01:11 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:11 google-dataproc-startup[811]: <13>Oct 13 23:01:11 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:11 google-dataproc-startup[811]: <13>Oct 13 23:01:11 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:11 google-dataproc-startup[811]: <13>Oct 13 23:01:11 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 21.'
<13>Oct 13 23:01:11 google-dataproc-startup[811]: <13>Oct 13 23:01:11 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 21.'
<13>Oct 13 23:01:11 google-dataproc-startup[811]: <13>Oct 13 23:01:11 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 21.
<13>Oct 13 23:01:11 google-dataproc-startup[811]: <13>Oct 13 23:01:11 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:11 google-dataproc-startup[811]: <13>Oct 13 23:01:11 uninstall[1403]: Removing hadoop-yarn-nodemanager (2.9.2-1) ...
<13>Oct 13 23:01:12 google-dataproc-startup[811]: <13>Oct 13 23:01:12 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:12 google-dataproc-startup[811]: <13>Oct 13 23:01:12 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:12 google-dataproc-startup[811]: <13>Oct 13 23:01:12 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:12 google-dataproc-startup[811]: <13>Oct 13 23:01:12 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 22.'
<13>Oct 13 23:01:12 google-dataproc-startup[811]: <13>Oct 13 23:01:12 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 22.'
<13>Oct 13 23:01:12 google-dataproc-startup[811]: <13>Oct 13 23:01:12 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 22.
<13>Oct 13 23:01:12 google-dataproc-startup[811]: <13>Oct 13 23:01:12 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:13 google-dataproc-startup[811]: <13>Oct 13 23:01:13 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:13 google-dataproc-startup[811]: <13>Oct 13 23:01:13 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:13 google-dataproc-startup[811]: <13>Oct 13 23:01:13 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:13 google-dataproc-startup[811]: <13>Oct 13 23:01:13 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 23.'
<13>Oct 13 23:01:13 google-dataproc-startup[811]: <13>Oct 13 23:01:13 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 23.'
<13>Oct 13 23:01:13 google-dataproc-startup[811]: <13>Oct 13 23:01:13 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 23.
<13>Oct 13 23:01:13 google-dataproc-startup[811]: <13>Oct 13 23:01:13 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:13 google-dataproc-startup[811]: <13>Oct 13 23:01:13 uninstall[1403]: Removing hive-webhcat-server (2.3.5-1) ...
<13>Oct 13 23:01:14 google-dataproc-startup[811]: <13>Oct 13 23:01:14 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:14 google-dataproc-startup[811]: <13>Oct 13 23:01:14 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:14 google-dataproc-startup[811]: <13>Oct 13 23:01:14 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:14 google-dataproc-startup[811]: <13>Oct 13 23:01:14 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 24.'
<13>Oct 13 23:01:14 google-dataproc-startup[811]: <13>Oct 13 23:01:14 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 24.'
<13>Oct 13 23:01:14 google-dataproc-startup[811]: <13>Oct 13 23:01:14 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 24.
<13>Oct 13 23:01:14 google-dataproc-startup[811]: <13>Oct 13 23:01:14 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:14 google-dataproc-startup[811]: <13>Oct 13 23:01:14 uninstall[1403]: Removing hive-webhcat (2.3.5-1) ...
<13>Oct 13 23:01:14 google-dataproc-startup[811]: <13>Oct 13 23:01:14 uninstall[1403]: Removing javascript-common (11) ...
<13>Oct 13 23:01:15 google-dataproc-startup[811]: <13>Oct 13 23:01:15 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:15 google-dataproc-startup[811]: <13>Oct 13 23:01:15 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:15 google-dataproc-startup[811]: <13>Oct 13 23:01:15 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:15 google-dataproc-startup[811]: <13>Oct 13 23:01:15 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 25.'
<13>Oct 13 23:01:15 google-dataproc-startup[811]: <13>Oct 13 23:01:15 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 25.'
<13>Oct 13 23:01:15 google-dataproc-startup[811]: <13>Oct 13 23:01:15 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 25.
<13>Oct 13 23:01:15 google-dataproc-startup[811]: <13>Oct 13 23:01:15 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:15 google-dataproc-startup[811]: <13>Oct 13 23:01:15 uninstall[1403]: Removing kafka-server (1.1.1-1) ...
<13>Oct 13 23:01:15 google-dataproc-startup[811]: <13>Oct 13 23:01:15 uninstall[1403]: Removing kafka (1.1.1-1) ...
<13>Oct 13 23:01:15 google-dataproc-startup[811]: <13>Oct 13 23:01:15 uninstall[1403]: Removing knox (1.1.0-1) ...
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 26.'
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 26.'
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 26.
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + update_succeeded=1
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + break
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + ((  1  ))
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + [[ 0 -eq 0 ]]
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + loginfo 'Waiting for namenode to listen on rpc port'
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + echo 'Waiting for namenode to listen on rpc port'
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: Waiting for namenode to listen on rpc port
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + wait_for_port cluster-6d45-m 8020
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + local -r host=cluster-6d45-m
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + local -r port=8020
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + loginfo 'Waiting for service to come up on host=cluster-6d45-m port=8020.'
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + echo 'Waiting for service to come up on host=cluster-6d45-m port=8020.'
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: Waiting for service to come up on host=cluster-6d45-m port=8020.
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + retry_with_constant_backoff nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + local max_retry=300
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + cmd=("$@")
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + local -a cmd
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + local update_succeeded=0
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: ++ seq 1 300
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 1.'
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 1.'
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 1.
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 uninstall[1403]: Removing libbind9-140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 uninstall[1403]: Removing node-highlight.js (8.2+ds-5) ...
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 uninstall[1403]: Removing nodejs (8.11.1~dfsg-2~bpo9+1) ...
<13>Oct 13 23:01:16 google-dataproc-startup[811]: <13>Oct 13 23:01:16 uninstall[1403]: Removing libc-ares2:amd64 (1.14.0-1~bpo9+1) ...
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 uninstall[1403]: Removing libisccfg140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 uninstall[1403]: Removing libdns162:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 uninstall[1403]: Removing update-inetd (4.44) ...
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 uninstall[1403]: Removing libfile-copy-recursive-perl (0.38-1) ...
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 27.'
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 27.'
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 27.
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 uninstall[1403]: Removing liblwres141:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 2.'
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 2.'
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 2.
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 uninstall[1403]: Removing libisccc140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 uninstall[1403]: Removing libkadm5srv-mit11:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 uninstall[1403]: Removing libkdb5-8:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 uninstall[1403]: Removing libkadm5clnt-mit11:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 23:01:17 google-dataproc-startup[811]: <13>Oct 13 23:01:17 uninstall[1403]: Removing libgssrpc4:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 uninstall[1403]: Removing libhttp-parser2.8:amd64 (2.8.1-1~bpo9+1) ...
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 uninstall[1403]: Removing libisc160:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 uninstall[1403]: Removing libjs-bootstrap (3.3.7+dfsg-2+deb9u2) ...
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 uninstall[1403]: Removing libjs-d3 (3.5.17-2) ...
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 28.'
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 28.'
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 28.
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 uninstall[1403]: Removing libjs-es5-shim (4.5.9-1) ...
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 3.'
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 3.'
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 3.
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 uninstall[1403]: Removing r-cran-highr (0.6-1) ...
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 uninstall[1403]: Removing libjs-highlight.js (8.2+ds-5) ...
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 uninstall[1403]: Removing libjs-jquery-selectize.js (0.12.4+dfsg-1~bpo9+1) ...
<13>Oct 13 23:01:18 google-dataproc-startup[811]: <13>Oct 13 23:01:18 uninstall[1403]: Removing libjs-twitter-bootstrap-datepicker (1.3.1+dfsg1-1) ...
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 uninstall[1403]: Removing libjs-twitter-bootstrap (2.0.2+dfsg-10) ...
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 uninstall[1403]: Removing libjs-jquery-tablesorter (1:2.31.1+dfsg1-1~bpo9+1) ...
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 uninstall[1403]: Removing libjs-jquery-datatables (1.10.13+dfsg-2) ...
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 uninstall[1403]: Removing libjs-jquery-metadata (11-3) ...
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 uninstall[1403]: Removing libjs-jquery-ui (1.12.1+dfsg-4) ...
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 29.'
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 29.'
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 29.
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 4.'
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 4.'
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 4.
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 uninstall[1403]: Removing libjs-json (0~20160510-1) ...
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 uninstall[1403]: Removing libjs-microplugin.js (0.0.3+dfsg-1) ...
<13>Oct 13 23:01:19 google-dataproc-startup[811]: <13>Oct 13 23:01:19 uninstall[1403]: Removing libjs-modernizr (2.6.2+ds1-1) ...
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 uninstall[1403]: Removing libjs-prettify (2013.03.04+dfsg-4) ...
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 uninstall[1403]: Removing libjs-sifter.js (0.5.1+dfsg-2) ...
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 uninstall[1403]: Removing pandoc (1.17.2~dfsg-3) ...
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 uninstall[1403]: Removing liblua5.1-0:amd64 (5.1.5-8.1+b2) ...
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 uninstall[1403]: Removing libluajit-5.1-2:amd64 (2.1.0~beta3+dfsg-5.1~bpo9+1) ...
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 30.'
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 30.'
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 30.
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 uninstall[1403]: Removing libluajit-5.1-common (2.1.0~beta3+dfsg-5.1~bpo9+1) ...
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 5.'
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 5.'
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 5.
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 uninstall[1403]: Removing r-cran-httpuv (1.4.5.1+dfsg-1~bpo9+1) ...
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 uninstall[1403]: Removing libuv1:amd64 (1.18.0-3~bpo9+1) ...
<13>Oct 13 23:01:20 google-dataproc-startup[811]: <13>Oct 13 23:01:20 uninstall[1403]: Removing libyaml-0-2:amd64 (0.2.1-1~bpo9+1) ...
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 uninstall[1403]: Removing r-cran-dplyr (0.7.8-1~bpo9+1) ...
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 uninstall[1403]: Removing r-cran-tidyselect (0.2.5-1~bpo9+1) ...
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 uninstall[1403]: Removing r-cran-ggplot2 (3.1.0-1~bpo9+1) ...
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 uninstall[1403]: Removing r-cran-scales (1.0.0-1~bpo9+1) ...
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 uninstall[1403]: Removing littler (0.3.1-1) ...
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 31.'
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 31.'
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 31.
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 uninstall[1403]: Removing node-normalize.css (8.0.0-3~bpo9+1) ...
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 uninstall[1403]: Removing nodejs-doc (8.11.1~dfsg-2~bpo9+1) ...
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 6.'
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 6.'
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 6.
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 uninstall[1403]: Removing pandoc-data (1.17.2~dfsg-3) ...
<13>Oct 13 23:01:21 google-dataproc-startup[811]: <13>Oct 13 23:01:21 uninstall[1403]: Removing r-cran-tibble (2.0.1-1~bpo9+1) ...
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 uninstall[1403]: Removing r-cran-pillar (1.3.1-1~bpo9+1) ...
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 uninstall[1403]: Removing r-cran-base64enc (0.1-3-1) ...
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 uninstall[1403]: Removing r-cran-bindrcpp (0.2.2-2~bpo9+1) ...
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 uninstall[1403]: Removing r-cran-bindr (0.1.1-2~bpo9+1) ...
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 32.'
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 32.'
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 32.
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 uninstall[1403]: Removing r-cran-bit64 (0.9-7-2~bpo9+1) ...
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 7.'
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 7.'
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 7.
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 uninstall[1403]: Removing r-cran-bit (1.1-14-1~bpo9+1) ...
<13>Oct 13 23:01:22 google-dataproc-startup[811]: <13>Oct 13 23:01:22 uninstall[1403]: Removing r-cran-munsell (0.5.0-1~bpo9+1) ...
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 uninstall[1403]: Removing r-cran-colorspace (1.3-2-1) ...
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 uninstall[1403]: Removing r-cran-testthat (2.0.1-1~bpo9+1) ...
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 uninstall[1403]: Removing r-cran-data.table (1.10.0-1) ...
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 uninstall[1403]: Removing r-cran-rsqlite (1.1-2-1) ...
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 uninstall[1403]: Removing r-cran-dbi (1.0.0-1~bpo9+2) ...
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 33.'
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 33.'
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 33.
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 uninstall[1403]: Removing r-cran-memoise (1.1.0-1~bpo9+1) ...
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 8.'
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 8.'
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 8.
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 uninstall[1403]: Removing r-cran-htmlwidgets (1.3+dfsg-1~bpo9+1) ...
<13>Oct 13 23:01:23 google-dataproc-startup[811]: <13>Oct 13 23:01:23 uninstall[1403]: Removing r-cran-htmltools (0.3.6-2~bpo9+1) ...
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 uninstall[1403]: Removing r-cran-digest (0.6.11-1) ...
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 uninstall[1403]: Removing r-cran-evaluate (0.10-1) ...
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 uninstall[1403]: Removing r-cran-fansi (0.4.0-1~bpo9+1) ...
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 uninstall[1403]: Removing r-cran-tikzdevice (0.10-1-1) ...
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 uninstall[1403]: Removing r-cran-filehash (2.3-1) ...
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 34.'
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 34.'
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 34.
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 9.'
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 9.'
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 9.
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 uninstall[1403]: Removing r-cran-reshape2 (1.4.2-1) ...
<13>Oct 13 23:01:24 google-dataproc-startup[811]: <13>Oct 13 23:01:24 uninstall[1403]: Removing r-cran-stringr (1.4.0-1~bpo9+1) ...
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 uninstall[1403]: Removing r-cran-glue (1.3.0-1~bpo9+1) ...
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 uninstall[1403]: Removing r-cran-googlevis (0.6.2-1) ...
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 uninstall[1403]: Removing r-cran-gtable (0.2.0-1) ...
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 uninstall[1403]: Removing r-cran-hexbin (1.27.1-1) ...
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 uninstall[1403]: Removing r-cran-hms (0.4.2-1~bpo9+1) ...
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 uninstall[1403]: Removing r-cran-jsonlite (1.6+dfsg-1~bpo9+1) ...
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 uninstall[1403]: Removing r-cran-labeling (0.3-1) ...
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 35.'
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 35.'
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 35.
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 10.'
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 10.'
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 10.
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 uninstall[1403]: Removing r-cran-promises (1.0.1-2~bpo9+1) ...
<13>Oct 13 23:01:25 google-dataproc-startup[811]: <13>Oct 13 23:01:25 uninstall[1403]: Removing r-cran-later (0.7.5+dfsg-2~bpo9+1) ...
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 uninstall[1403]: Removing r-cran-lazyeval (0.2.0-1) ...
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 uninstall[1403]: Removing r-cran-purrr (0.3.0-1~bpo9+1) ...
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 uninstall[1403]: Removing r-cran-magrittr (1.5-3) ...
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 uninstall[1403]: Removing r-cran-mapproj (1.2-4-1) ...
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 uninstall[1403]: Removing r-cran-maps (3.1.1-1) ...
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 uninstall[1403]: Removing r-cran-mime (0.5-1) ...
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 uninstall[1403]: Removing r-cran-pkgconfig (2.0.2-1~bpo9+1) ...
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 36.'
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 36.'
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 36.
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 uninstall[1403]: Removing r-cran-plyr (1.8.4-1) ...
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 11.'
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 11.'
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 11.
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:26 google-dataproc-startup[811]: <13>Oct 13 23:01:26 uninstall[1403]: Removing r-cran-png (0.1-7-1) ...
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 uninstall[1403]: Removing r-cran-praise (1.0.0-1) ...
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 uninstall[1403]: Removing r-cran-r6 (2.4.0-1~bpo9+1) ...
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 uninstall[1403]: Removing r-cran-rcolorbrewer (1.1-2-1) ...
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 uninstall[1403]: Removing r-cran-rlang (0.3.1-2~bpo9+1) ...
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 uninstall[1403]: Removing r-cran-sourcetools (0.1.5-1) ...
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 uninstall[1403]: Removing r-cran-sp (1:1.2-4-1) ...
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 uninstall[1403]: Removing r-cran-stringi (1.2.4-2~bpo9+1) ...
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 uninstall[1403]: Removing r-cran-testit (0.6-1) ...
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 37.'
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 37.'
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 37.
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 12.'
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 12.'
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 12.
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:27 google-dataproc-startup[811]: <13>Oct 13 23:01:27 uninstall[1403]: Removing r-cran-tinytex (0.10-1~bpo9+1) ...
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 uninstall[1403]: Removing r-cran-utf8 (1.1.4-1~bpo9+1) ...
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 uninstall[1403]: Removing r-cran-viridislite (0.3.0-3~bpo9+1) ...
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 uninstall[1403]: Removing r-cran-withr (2.1.2-1~bpo9+1) ...
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 uninstall[1403]: Removing r-cran-xfun (0.4-1~bpo9+1) ...
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 uninstall[1403]: Removing r-cran-xml2 (1.1.0-1) ...
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 uninstall[1403]: Removing r-cran-xtable (1:1.8-2-1) ...
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 uninstall[1403]: Removing r-cran-yaml (2.2.0-1~bpo9+1) ...
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 38.'
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 38.'
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 38.
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 uninstall[1403]: Removing solr-server (6.6.5-1) ...
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 13.'
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 13.'
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 13.
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:28 google-dataproc-startup[811]: <13>Oct 13 23:01:28 uninstall[1403]: Removing solr (6.6.5-1) ...
<13>Oct 13 23:01:29 google-dataproc-startup[811]: <13>Oct 13 23:01:29 uninstall[1403]: Removing xinetd (1:2.3.15-7) ...
<13>Oct 13 23:01:29 google-dataproc-startup[811]: <13>Oct 13 23:01:29 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:29 google-dataproc-startup[811]: <13>Oct 13 23:01:29 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:29 google-dataproc-startup[811]: <13>Oct 13 23:01:29 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:29 google-dataproc-startup[811]: <13>Oct 13 23:01:29 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 39.'
<13>Oct 13 23:01:29 google-dataproc-startup[811]: <13>Oct 13 23:01:29 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 39.'
<13>Oct 13 23:01:29 google-dataproc-startup[811]: <13>Oct 13 23:01:29 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 39.
<13>Oct 13 23:01:29 google-dataproc-startup[811]: <13>Oct 13 23:01:29 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:29 google-dataproc-startup[811]: <13>Oct 13 23:01:29 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:29 google-dataproc-startup[811]: <13>Oct 13 23:01:29 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:29 google-dataproc-startup[811]: <13>Oct 13 23:01:29 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 14.'
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 14.'
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 14.
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 uninstall[1403]: Removing zeppelin (0.8.0-1) ...
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 40.'
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 40.'
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 40.
<13>Oct 13 23:01:30 google-dataproc-startup[811]: <13>Oct 13 23:01:30 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 15.'
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 15.'
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 15.
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 uninstall[1403]: Removing zookeeper-server (3.4.13-1) ...
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 41.'
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 41.'
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 41.
<13>Oct 13 23:01:31 google-dataproc-startup[811]: <13>Oct 13 23:01:31 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 16.'
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 16.'
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 16.
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 uninstall[1403]: Removing libgeoip1:amd64 (1.6.9-4) ...
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 uninstall[1403]: Removing libjs-jquery (3.1.1-2+deb9u1) ...
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 uninstall[1403]: Removing r-cran-rcpp (1.0.0-1~bpo9+1) ...
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 42.'
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 42.'
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 42.
<13>Oct 13 23:01:32 google-dataproc-startup[811]: <13>Oct 13 23:01:32 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 17.'
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 17.'
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 17.
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 uninstall[1403]: Removing r-cran-cli (1.0.1-1~bpo9+1) ...
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 uninstall[1403]: Removing r-cran-assertthat (0.2.0-1~bpo9+1) ...
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 uninstall[1403]: Removing r-cran-crayon (1.3.4-2~bpo9+1) ...
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 uninstall[1403]: Removing r-cran-littler (0.3.1-1) ...
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 uninstall[1403]: Removing r-cran-pkgkitten (0.1.4-1) ...
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 uninstall[1403]: Removing libverto1:amd64 (0.2.4-2.1) ...
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 uninstall[1403]: Removing libverto-libev1:amd64 (0.2.4-2.1) ...
<13>Oct 13 23:01:33 google-dataproc-startup[811]: <13>Oct 13 23:01:33 uninstall[1403]: Removing libev4 (1:4.22-1+b1) ...
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 43.'
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 43.'
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 43.
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 uninstall[1403]: Processing triggers for libc-bin (2.24-11+deb9u4) ...
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hadoop-hdfs-namenode[1465]: nc: connect to cluster-6d45-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 18.'
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 18.'
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 failed. Retry attempt: 18.
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 setup-hadoop-hdfs-namenode[1465]: + sleep 1
<13>Oct 13 23:01:34 google-dataproc-startup[811]: <13>Oct 13 23:01:34 uninstall[1403]: Processing triggers for man-db (2.7.6.1-2) ...
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 44.'
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 44.'
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 44.
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + nc -v -z -w 0 cluster-6d45-m 8020
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: Connection to cluster-6d45-m 8020 port [tcp/*] succeeded!
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + update_succeeded=1
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 8020 succeeded.'
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + echo 'nc -v -z -w 0 cluster-6d45-m 8020 succeeded.'
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: nc -v -z -w 0 cluster-6d45-m 8020 succeeded.
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + break
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + ((  1  ))
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + loginfo 'Service up on host=cluster-6d45-m port=8020.'
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + echo 'Service up on host=cluster-6d45-m port=8020.'
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: Service up on host=cluster-6d45-m port=8020.
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + loginfo 'Initializing HDFS directories'
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + echo 'Initializing HDFS directories'
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: Initializing HDFS directories
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + HADOOP_USERS=(hdfs mapred yarn spark pig hive hbase zookeeper)
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + REAL_USERS=($(getent passwd | awk -F: '1000 < $3 && $3 < 6000 { print $1}'))
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: ++ getent passwd
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: ++ awk -F: '1000 < $3 && $3 < 6000 { print $1}'
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + HDFS_USERS=("${HADOOP_USERS[@]}" "${REAL_USERS[@]}")
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + HDFS_USER_DIRS=("${HDFS_USERS[@]/#//user/}")
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.eventLog.dir
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: ++ local property_file=/etc/spark/conf/spark-defaults.conf
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: ++ local property_name=spark.eventLog.dir
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: +++ grep '^spark.eventLog.dir=' /etc/spark/conf/spark-defaults.conf
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: +++ tail -n 1
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: +++ cut -d = -f 2-
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: ++ local property_value=hdfs://cluster-6d45-m/user/spark/eventlog
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: ++ echo hdfs://cluster-6d45-m/user/spark/eventlog
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + SPARK_EVENTLOG_DIR=hdfs://cluster-6d45-m/user/spark/eventlog
<13>Oct 13 23:01:35 google-dataproc-startup[811]: <13>Oct 13 23:01:35 setup-hadoop-hdfs-namenode[1465]: + su -s /bin/bash hdfs -c 'login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-6d45-m.us-central1-a.c.lustrous-drake-255300.internal &&              hadoop fs -mkdir -p              /tmp/hadoop-yarn/staging/history /user/hdfs /user/mapred /user/yarn /user/spark /user/pig /user/hive /user/hbase /user/zookeeper hdfs://cluster-6d45-m/user/spark/eventlog'
<13>Oct 13 23:01:36 google-dataproc-startup[811]: <13>Oct 13 23:01:36 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:36 google-dataproc-startup[811]: <13>Oct 13 23:01:36 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:36 google-dataproc-startup[811]: <13>Oct 13 23:01:36 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:36 google-dataproc-startup[811]: <13>Oct 13 23:01:36 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 45.'
<13>Oct 13 23:01:36 google-dataproc-startup[811]: <13>Oct 13 23:01:36 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 45.'
<13>Oct 13 23:01:36 google-dataproc-startup[811]: <13>Oct 13 23:01:36 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 45.
<13>Oct 13 23:01:36 google-dataproc-startup[811]: <13>Oct 13 23:01:36 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:37 google-dataproc-startup[811]: <13>Oct 13 23:01:37 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:37 google-dataproc-startup[811]: <13>Oct 13 23:01:37 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:37 google-dataproc-startup[811]: <13>Oct 13 23:01:37 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:37 google-dataproc-startup[811]: <13>Oct 13 23:01:37 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 46.'
<13>Oct 13 23:01:37 google-dataproc-startup[811]: <13>Oct 13 23:01:37 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 46.'
<13>Oct 13 23:01:37 google-dataproc-startup[811]: <13>Oct 13 23:01:37 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 46.
<13>Oct 13 23:01:37 google-dataproc-startup[811]: <13>Oct 13 23:01:37 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:38 google-dataproc-startup[811]: <13>Oct 13 23:01:38 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:38 google-dataproc-startup[811]: <13>Oct 13 23:01:38 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:38 google-dataproc-startup[811]: <13>Oct 13 23:01:38 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:38 google-dataproc-startup[811]: <13>Oct 13 23:01:38 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 47.'
<13>Oct 13 23:01:38 google-dataproc-startup[811]: <13>Oct 13 23:01:38 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 47.'
<13>Oct 13 23:01:38 google-dataproc-startup[811]: <13>Oct 13 23:01:38 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 47.
<13>Oct 13 23:01:38 google-dataproc-startup[811]: <13>Oct 13 23:01:38 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:39 google-dataproc-startup[811]: <13>Oct 13 23:01:39 uninstall[1403]: Processing triggers for fontconfig (2.11.0-6.7+b1) ...
<13>Oct 13 23:01:39 google-dataproc-startup[811]: <13>Oct 13 23:01:39 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:39 google-dataproc-startup[811]: <13>Oct 13 23:01:39 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:39 google-dataproc-startup[811]: <13>Oct 13 23:01:39 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:39 google-dataproc-startup[811]: <13>Oct 13 23:01:39 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 48.'
<13>Oct 13 23:01:39 google-dataproc-startup[811]: <13>Oct 13 23:01:39 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 48.'
<13>Oct 13 23:01:39 google-dataproc-startup[811]: <13>Oct 13 23:01:39 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 48.
<13>Oct 13 23:01:39 google-dataproc-startup[811]: <13>Oct 13 23:01:39 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:40 google-dataproc-startup[811]: <13>Oct 13 23:01:40 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:40 google-dataproc-startup[811]: <13>Oct 13 23:01:40 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:40 google-dataproc-startup[811]: <13>Oct 13 23:01:40 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:40 google-dataproc-startup[811]: <13>Oct 13 23:01:40 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 49.'
<13>Oct 13 23:01:40 google-dataproc-startup[811]: <13>Oct 13 23:01:40 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 49.'
<13>Oct 13 23:01:40 google-dataproc-startup[811]: <13>Oct 13 23:01:40 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 49.
<13>Oct 13 23:01:40 google-dataproc-startup[811]: <13>Oct 13 23:01:40 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:41 google-dataproc-startup[811]: <13>Oct 13 23:01:41 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:41 google-dataproc-startup[811]: <13>Oct 13 23:01:41 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:41 google-dataproc-startup[811]: <13>Oct 13 23:01:41 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:41 google-dataproc-startup[811]: <13>Oct 13 23:01:41 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 50.'
<13>Oct 13 23:01:41 google-dataproc-startup[811]: <13>Oct 13 23:01:41 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 50.'
<13>Oct 13 23:01:41 google-dataproc-startup[811]: <13>Oct 13 23:01:41 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 50.
<13>Oct 13 23:01:41 google-dataproc-startup[811]: <13>Oct 13 23:01:41 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:41 google-dataproc-startup[811]: <13>Oct 13 23:01:41 uninstall[1403]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 102578 files and directories currently installed.)
<13>Oct 13 23:01:41 google-dataproc-startup[811]: <13>Oct 13 23:01:41 uninstall[1403]: Purging configuration files for update-inetd (4.44) ...
<13>Oct 13 23:01:42 google-dataproc-startup[811]: <13>Oct 13 23:01:42 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:42 google-dataproc-startup[811]: <13>Oct 13 23:01:42 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:42 google-dataproc-startup[811]: <13>Oct 13 23:01:42 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:42 google-dataproc-startup[811]: <13>Oct 13 23:01:42 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 51.'
<13>Oct 13 23:01:42 google-dataproc-startup[811]: <13>Oct 13 23:01:42 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 51.'
<13>Oct 13 23:01:42 google-dataproc-startup[811]: <13>Oct 13 23:01:42 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 51.
<13>Oct 13 23:01:42 google-dataproc-startup[811]: <13>Oct 13 23:01:42 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:42 google-dataproc-startup[811]: <13>Oct 13 23:01:42 uninstall[1403]: Purging configuration files for hive-webhcat-server (2.3.5-1) ...
<13>Oct 13 23:01:42 google-dataproc-startup[811]: <13>Oct 13 23:01:42 uninstall[1403]: Purging configuration files for xinetd (1:2.3.15-7) ...
<13>Oct 13 23:01:43 google-dataproc-startup[811]: <13>Oct 13 23:01:43 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:43 google-dataproc-startup[811]: <13>Oct 13 23:01:43 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:43 google-dataproc-startup[811]: <13>Oct 13 23:01:43 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:43 google-dataproc-startup[811]: <13>Oct 13 23:01:43 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 52.'
<13>Oct 13 23:01:43 google-dataproc-startup[811]: <13>Oct 13 23:01:43 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 52.'
<13>Oct 13 23:01:43 google-dataproc-startup[811]: <13>Oct 13 23:01:43 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 52.
<13>Oct 13 23:01:43 google-dataproc-startup[811]: <13>Oct 13 23:01:43 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:43 google-dataproc-startup[811]: <13>Oct 13 23:01:43 uninstall[1403]: Purging configuration files for zookeeper-server (3.4.13-1) ...
<13>Oct 13 23:01:44 google-dataproc-startup[811]: <13>Oct 13 23:01:44 uninstall[1403]: Purging configuration files for hadoop-hdfs-journalnode (2.9.2-1) ...
<13>Oct 13 23:01:44 google-dataproc-startup[811]: <13>Oct 13 23:01:44 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:44 google-dataproc-startup[811]: <13>Oct 13 23:01:44 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:44 google-dataproc-startup[811]: <13>Oct 13 23:01:44 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:44 google-dataproc-startup[811]: <13>Oct 13 23:01:44 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 53.'
<13>Oct 13 23:01:44 google-dataproc-startup[811]: <13>Oct 13 23:01:44 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 53.'
<13>Oct 13 23:01:44 google-dataproc-startup[811]: <13>Oct 13 23:01:44 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 53.
<13>Oct 13 23:01:44 google-dataproc-startup[811]: <13>Oct 13 23:01:44 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:44 google-dataproc-startup[811]: <13>Oct 13 23:01:44 uninstall[1403]: Purging configuration files for solr (6.6.5-1) ...
<13>Oct 13 23:01:44 google-dataproc-startup[811]: <13>Oct 13 23:01:44 uninstall[1403]: Purging configuration files for krb5-admin-server (1.15-1+deb9u1) ...
<13>Oct 13 23:01:45 google-dataproc-startup[811]: <13>Oct 13 23:01:45 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:45 google-dataproc-startup[811]: <13>Oct 13 23:01:45 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:45 google-dataproc-startup[811]: <13>Oct 13 23:01:45 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:45 google-dataproc-startup[811]: <13>Oct 13 23:01:45 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 54.'
<13>Oct 13 23:01:45 google-dataproc-startup[811]: <13>Oct 13 23:01:45 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 54.'
<13>Oct 13 23:01:45 google-dataproc-startup[811]: <13>Oct 13 23:01:45 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 54.
<13>Oct 13 23:01:45 google-dataproc-startup[811]: <13>Oct 13 23:01:45 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 55.'
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 55.'
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 55.
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: + run_with_retries sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: + local retry_backoff
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: + cmd=("$@")
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: + local -a cmd
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: + loginfo 'About to run '\''sudo -u hdfs hadoop fs -chmod -R 1777 /'\'' with retries...'
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: + echo 'About to run '\''sudo -u hdfs hadoop fs -chmod -R 1777 /'\'' with retries...'
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: About to run 'sudo -u hdfs hadoop fs -chmod -R 1777 /' with retries...
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: + local update_succeeded=0
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: + (( i = 0 ))
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: + (( i < 12 ))
<13>Oct 13 23:01:46 google-dataproc-startup[811]: <13>Oct 13 23:01:46 setup-hadoop-hdfs-namenode[1465]: + sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Oct 13 23:01:47 google-dataproc-startup[811]: <13>Oct 13 23:01:47 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:47 google-dataproc-startup[811]: <13>Oct 13 23:01:47 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:47 google-dataproc-startup[811]: <13>Oct 13 23:01:47 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:47 google-dataproc-startup[811]: <13>Oct 13 23:01:47 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 56.'
<13>Oct 13 23:01:47 google-dataproc-startup[811]: <13>Oct 13 23:01:47 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 56.'
<13>Oct 13 23:01:47 google-dataproc-startup[811]: <13>Oct 13 23:01:47 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 56.
<13>Oct 13 23:01:47 google-dataproc-startup[811]: <13>Oct 13 23:01:47 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:48 google-dataproc-startup[811]: <13>Oct 13 23:01:48 uninstall[1403]: Purging configuration files for kafka (1.1.1-1) ...
<13>Oct 13 23:01:48 google-dataproc-startup[811]: <13>Oct 13 23:01:48 uninstall[1403]: Purging configuration files for hadoop-hdfs-zkfc (2.9.2-1) ...
<13>Oct 13 23:01:48 google-dataproc-startup[811]: <13>Oct 13 23:01:48 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:48 google-dataproc-startup[811]: <13>Oct 13 23:01:48 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:48 google-dataproc-startup[811]: <13>Oct 13 23:01:48 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:48 google-dataproc-startup[811]: <13>Oct 13 23:01:48 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 57.'
<13>Oct 13 23:01:48 google-dataproc-startup[811]: <13>Oct 13 23:01:48 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 57.'
<13>Oct 13 23:01:48 google-dataproc-startup[811]: <13>Oct 13 23:01:48 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 57.
<13>Oct 13 23:01:48 google-dataproc-startup[811]: <13>Oct 13 23:01:48 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:48 google-dataproc-startup[811]: <13>Oct 13 23:01:48 uninstall[1403]: Purging configuration files for krb5-kdc (1.15-1+deb9u1) ...
<13>Oct 13 23:01:49 google-dataproc-startup[811]: <13>Oct 13 23:01:49 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:49 google-dataproc-startup[811]: <13>Oct 13 23:01:49 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:49 google-dataproc-startup[811]: <13>Oct 13 23:01:49 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:49 google-dataproc-startup[811]: <13>Oct 13 23:01:49 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 58.'
<13>Oct 13 23:01:49 google-dataproc-startup[811]: <13>Oct 13 23:01:49 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 58.'
<13>Oct 13 23:01:49 google-dataproc-startup[811]: <13>Oct 13 23:01:49 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 58.
<13>Oct 13 23:01:49 google-dataproc-startup[811]: <13>Oct 13 23:01:49 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:50 google-dataproc-startup[811]: <13>Oct 13 23:01:50 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:50 google-dataproc-startup[811]: <13>Oct 13 23:01:50 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:50 google-dataproc-startup[811]: <13>Oct 13 23:01:50 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:50 google-dataproc-startup[811]: <13>Oct 13 23:01:50 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 59.'
<13>Oct 13 23:01:50 google-dataproc-startup[811]: <13>Oct 13 23:01:50 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 59.'
<13>Oct 13 23:01:50 google-dataproc-startup[811]: <13>Oct 13 23:01:50 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 59.
<13>Oct 13 23:01:50 google-dataproc-startup[811]: <13>Oct 13 23:01:50 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:51 google-dataproc-startup[811]: <13>Oct 13 23:01:51 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:51 google-dataproc-startup[811]: <13>Oct 13 23:01:51 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:51 google-dataproc-startup[811]: <13>Oct 13 23:01:51 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:51 google-dataproc-startup[811]: <13>Oct 13 23:01:51 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 60.'
<13>Oct 13 23:01:51 google-dataproc-startup[811]: <13>Oct 13 23:01:51 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 60.'
<13>Oct 13 23:01:51 google-dataproc-startup[811]: <13>Oct 13 23:01:51 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 60.
<13>Oct 13 23:01:51 google-dataproc-startup[811]: <13>Oct 13 23:01:51 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:52 google-dataproc-startup[811]: <13>Oct 13 23:01:52 uninstall[1403]: Purging configuration files for libjs-jquery-ui (1.12.1+dfsg-4) ...
<13>Oct 13 23:01:52 google-dataproc-startup[811]: <13>Oct 13 23:01:52 uninstall[1403]: Purging configuration files for krb5-kpropd (1.15-1+deb9u1) ...
<13>Oct 13 23:01:52 google-dataproc-startup[811]: <13>Oct 13 23:01:52 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:52 google-dataproc-startup[811]: <13>Oct 13 23:01:52 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:52 google-dataproc-startup[811]: <13>Oct 13 23:01:52 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:52 google-dataproc-startup[811]: <13>Oct 13 23:01:52 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 61.'
<13>Oct 13 23:01:52 google-dataproc-startup[811]: <13>Oct 13 23:01:52 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 61.'
<13>Oct 13 23:01:52 google-dataproc-startup[811]: <13>Oct 13 23:01:52 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 61.
<13>Oct 13 23:01:52 google-dataproc-startup[811]: <13>Oct 13 23:01:52 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:53 google-dataproc-startup[811]: <13>Oct 13 23:01:53 uninstall[1403]: Purging configuration files for hadoop-hdfs-datanode (2.9.2-1) ...
<13>Oct 13 23:01:53 google-dataproc-startup[811]: <13>Oct 13 23:01:53 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:53 google-dataproc-startup[811]: <13>Oct 13 23:01:53 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:53 google-dataproc-startup[811]: <13>Oct 13 23:01:53 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:53 google-dataproc-startup[811]: <13>Oct 13 23:01:53 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 62.'
<13>Oct 13 23:01:53 google-dataproc-startup[811]: <13>Oct 13 23:01:53 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 62.'
<13>Oct 13 23:01:53 google-dataproc-startup[811]: <13>Oct 13 23:01:53 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 62.
<13>Oct 13 23:01:53 google-dataproc-startup[811]: <13>Oct 13 23:01:53 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:53 google-dataproc-startup[811]: <13>Oct 13 23:01:53 uninstall[1403]: Purging configuration files for javascript-common (11) ...
<13>Oct 13 23:01:54 google-dataproc-startup[811]: <13>Oct 13 23:01:54 uninstall[1403]: Purging configuration files for krb5-config (2.6) ...
<13>Oct 13 23:01:54 google-dataproc-startup[811]: <13>Oct 13 23:01:54 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:54 google-dataproc-startup[811]: <13>Oct 13 23:01:54 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:54 google-dataproc-startup[811]: <13>Oct 13 23:01:54 uninstall[1403]: Purging configuration files for knox (1.1.0-1) ...
<13>Oct 13 23:01:54 google-dataproc-startup[811]: <13>Oct 13 23:01:54 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:54 google-dataproc-startup[811]: <13>Oct 13 23:01:54 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 63.'
<13>Oct 13 23:01:54 google-dataproc-startup[811]: <13>Oct 13 23:01:54 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 63.'
<13>Oct 13 23:01:54 google-dataproc-startup[811]: <13>Oct 13 23:01:54 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 63.
<13>Oct 13 23:01:54 google-dataproc-startup[811]: <13>Oct 13 23:01:54 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 uninstall[1403]: Purging configuration files for r-cran-rmarkdown (1.11+dfsg-1~bpo9+1) ...
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 uninstall[1403]: Purging configuration files for hive-webhcat (2.3.5-1) ...
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 uninstall[1403]: Purging configuration files for solr-server (6.6.5-1) ...
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 uninstall[1403]: Purging configuration files for kafka-server (1.1.1-1) ...
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 64.'
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 64.'
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 64.
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:55 google-dataproc-startup[811]: <13>Oct 13 23:01:55 uninstall[1403]: Purging configuration files for zeppelin (0.8.0-1) ...
<13>Oct 13 23:01:56 google-dataproc-startup[811]: <13>Oct 13 23:01:56 uninstall[1403]: Purging configuration files for hadoop-yarn-nodemanager (2.9.2-1) ...
<13>Oct 13 23:01:56 google-dataproc-startup[811]: <13>Oct 13 23:01:56 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:56 google-dataproc-startup[811]: <13>Oct 13 23:01:56 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:56 google-dataproc-startup[811]: <13>Oct 13 23:01:56 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:56 google-dataproc-startup[811]: <13>Oct 13 23:01:56 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 65.'
<13>Oct 13 23:01:56 google-dataproc-startup[811]: <13>Oct 13 23:01:56 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 65.'
<13>Oct 13 23:01:56 google-dataproc-startup[811]: <13>Oct 13 23:01:56 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 65.
<13>Oct 13 23:01:56 google-dataproc-startup[811]: <13>Oct 13 23:01:56 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 uninstall[1403]: Processing triggers for systemd (232-25+deb9u12) ...
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + update_succeeded=1
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + break
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + ((  1  ))
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + run_with_retries sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-6d45-m/user/spark/eventlog
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + local retry_backoff
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + cmd=("$@")
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + local -a cmd
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + loginfo 'About to run '\''sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-6d45-m/user/spark/eventlog'\'' with retries...'
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + echo 'About to run '\''sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-6d45-m/user/spark/eventlog'\'' with retries...'
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: About to run 'sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-6d45-m/user/spark/eventlog' with retries...
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + local update_succeeded=0
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + (( i = 0 ))
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + (( i < 12 ))
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hadoop-hdfs-namenode[1465]: + sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-6d45-m/user/spark/eventlog
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 66.'
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 66.'
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 66.
<13>Oct 13 23:01:57 google-dataproc-startup[811]: <13>Oct 13 23:01:57 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:58 google-dataproc-startup[811]: <13>Oct 13 23:01:58 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:58 google-dataproc-startup[811]: <13>Oct 13 23:01:58 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:58 google-dataproc-startup[811]: <13>Oct 13 23:01:58 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:58 google-dataproc-startup[811]: <13>Oct 13 23:01:58 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 67.'
<13>Oct 13 23:01:58 google-dataproc-startup[811]: <13>Oct 13 23:01:58 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 67.'
<13>Oct 13 23:01:58 google-dataproc-startup[811]: <13>Oct 13 23:01:58 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 67.
<13>Oct 13 23:01:58 google-dataproc-startup[811]: <13>Oct 13 23:01:58 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:01:59 google-dataproc-startup[811]: <13>Oct 13 23:01:59 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:01:59 google-dataproc-startup[811]: <13>Oct 13 23:01:59 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:01:59 google-dataproc-startup[811]: <13>Oct 13 23:01:59 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:01:59 google-dataproc-startup[811]: <13>Oct 13 23:01:59 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 68.'
<13>Oct 13 23:01:59 google-dataproc-startup[811]: <13>Oct 13 23:01:59 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 68.'
<13>Oct 13 23:01:59 google-dataproc-startup[811]: <13>Oct 13 23:01:59 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 68.
<13>Oct 13 23:01:59 google-dataproc-startup[811]: <13>Oct 13 23:01:59 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:00 google-dataproc-startup[811]: <13>Oct 13 23:02:00 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:00 google-dataproc-startup[811]: <13>Oct 13 23:02:00 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:00 google-dataproc-startup[811]: <13>Oct 13 23:02:00 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:00 google-dataproc-startup[811]: <13>Oct 13 23:02:00 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 69.'
<13>Oct 13 23:02:00 google-dataproc-startup[811]: <13>Oct 13 23:02:00 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 69.'
<13>Oct 13 23:02:00 google-dataproc-startup[811]: <13>Oct 13 23:02:00 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 69.
<13>Oct 13 23:02:00 google-dataproc-startup[811]: <13>Oct 13 23:02:00 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:01 google-dataproc-startup[811]: <13>Oct 13 23:02:01 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:01 google-dataproc-startup[811]: <13>Oct 13 23:02:01 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:01 google-dataproc-startup[811]: <13>Oct 13 23:02:01 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:01 google-dataproc-startup[811]: <13>Oct 13 23:02:01 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 70.'
<13>Oct 13 23:02:01 google-dataproc-startup[811]: <13>Oct 13 23:02:01 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 70.'
<13>Oct 13 23:02:01 google-dataproc-startup[811]: <13>Oct 13 23:02:01 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 70.
<13>Oct 13 23:02:01 google-dataproc-startup[811]: <13>Oct 13 23:02:01 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:02 google-dataproc-startup[811]: <13>Oct 13 23:02:02 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:02 google-dataproc-startup[811]: <13>Oct 13 23:02:02 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:02 google-dataproc-startup[811]: <13>Oct 13 23:02:02 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:02 google-dataproc-startup[811]: <13>Oct 13 23:02:02 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 71.'
<13>Oct 13 23:02:02 google-dataproc-startup[811]: <13>Oct 13 23:02:02 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 71.'
<13>Oct 13 23:02:02 google-dataproc-startup[811]: <13>Oct 13 23:02:02 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 71.
<13>Oct 13 23:02:02 google-dataproc-startup[811]: <13>Oct 13 23:02:02 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:03 google-dataproc-startup[811]: <13>Oct 13 23:02:03 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:03 google-dataproc-startup[811]: <13>Oct 13 23:02:03 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:03 google-dataproc-startup[811]: <13>Oct 13 23:02:03 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:03 google-dataproc-startup[811]: <13>Oct 13 23:02:03 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 72.'
<13>Oct 13 23:02:03 google-dataproc-startup[811]: <13>Oct 13 23:02:03 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 72.'
<13>Oct 13 23:02:03 google-dataproc-startup[811]: <13>Oct 13 23:02:03 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 72.
<13>Oct 13 23:02:03 google-dataproc-startup[811]: <13>Oct 13 23:02:03 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + update_succeeded=1
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + break
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + ((  1  ))
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + for SERVICE in "${DATAPROC_START_AFTER_HDFS_SERVICES[@]}"
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + run_with_retries systemctl start hadoop-mapreduce-historyserver
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + local retry_backoff
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + cmd=("$@")
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + local -a cmd
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + loginfo 'About to run '\''systemctl start hadoop-mapreduce-historyserver'\'' with retries...'
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + echo 'About to run '\''systemctl start hadoop-mapreduce-historyserver'\'' with retries...'
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: About to run 'systemctl start hadoop-mapreduce-historyserver' with retries...
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + local update_succeeded=0
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + (( i = 0 ))
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + (( i < 12 ))
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hadoop-hdfs-namenode[1465]: + systemctl start hadoop-mapreduce-historyserver
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 73.'
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 73.'
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 73.
<13>Oct 13 23:02:04 google-dataproc-startup[811]: <13>Oct 13 23:02:04 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:05 google-dataproc-startup[811]: <13>Oct 13 23:02:05 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:05 google-dataproc-startup[811]: <13>Oct 13 23:02:05 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:05 google-dataproc-startup[811]: <13>Oct 13 23:02:05 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:05 google-dataproc-startup[811]: <13>Oct 13 23:02:05 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 74.'
<13>Oct 13 23:02:05 google-dataproc-startup[811]: <13>Oct 13 23:02:05 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 74.'
<13>Oct 13 23:02:05 google-dataproc-startup[811]: <13>Oct 13 23:02:05 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 74.
<13>Oct 13 23:02:05 google-dataproc-startup[811]: <13>Oct 13 23:02:05 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:06 google-dataproc-startup[811]: <13>Oct 13 23:02:06 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:06 google-dataproc-startup[811]: <13>Oct 13 23:02:06 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:06 google-dataproc-startup[811]: <13>Oct 13 23:02:06 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:06 google-dataproc-startup[811]: <13>Oct 13 23:02:06 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 75.'
<13>Oct 13 23:02:06 google-dataproc-startup[811]: <13>Oct 13 23:02:06 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 75.'
<13>Oct 13 23:02:06 google-dataproc-startup[811]: <13>Oct 13 23:02:06 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 75.
<13>Oct 13 23:02:06 google-dataproc-startup[811]: <13>Oct 13 23:02:06 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:07 google-dataproc-startup[811]: <13>Oct 13 23:02:07 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:07 google-dataproc-startup[811]: <13>Oct 13 23:02:07 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:08 google-dataproc-startup[811]: <13>Oct 13 23:02:08 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:08 google-dataproc-startup[811]: <13>Oct 13 23:02:08 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 76.'
<13>Oct 13 23:02:08 google-dataproc-startup[811]: <13>Oct 13 23:02:08 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 76.'
<13>Oct 13 23:02:08 google-dataproc-startup[811]: <13>Oct 13 23:02:08 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 76.
<13>Oct 13 23:02:08 google-dataproc-startup[811]: <13>Oct 13 23:02:08 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:09 google-dataproc-startup[811]: <13>Oct 13 23:02:09 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:09 google-dataproc-startup[811]: <13>Oct 13 23:02:09 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:09 google-dataproc-startup[811]: <13>Oct 13 23:02:09 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:09 google-dataproc-startup[811]: <13>Oct 13 23:02:09 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 77.'
<13>Oct 13 23:02:09 google-dataproc-startup[811]: <13>Oct 13 23:02:09 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 77.'
<13>Oct 13 23:02:09 google-dataproc-startup[811]: <13>Oct 13 23:02:09 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 77.
<13>Oct 13 23:02:09 google-dataproc-startup[811]: <13>Oct 13 23:02:09 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 78.'
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 78.'
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 78.
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + update_succeeded=1
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + break
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + ((  1  ))
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + for SERVICE in "${DATAPROC_START_AFTER_HDFS_SERVICES[@]}"
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + run_with_retries systemctl start spark-history-server
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + local retry_backoff
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + cmd=("$@")
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + local -a cmd
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + loginfo 'About to run '\''systemctl start spark-history-server'\'' with retries...'
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + echo 'About to run '\''systemctl start spark-history-server'\'' with retries...'
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: About to run 'systemctl start spark-history-server' with retries...
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + local update_succeeded=0
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + (( i = 0 ))
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + (( i < 12 ))
<13>Oct 13 23:02:10 google-dataproc-startup[811]: <13>Oct 13 23:02:10 setup-hadoop-hdfs-namenode[1465]: + systemctl start spark-history-server
<13>Oct 13 23:02:11 google-dataproc-startup[811]: <13>Oct 13 23:02:11 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:11 google-dataproc-startup[811]: <13>Oct 13 23:02:11 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:11 google-dataproc-startup[811]: <13>Oct 13 23:02:11 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:11 google-dataproc-startup[811]: <13>Oct 13 23:02:11 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 79.'
<13>Oct 13 23:02:11 google-dataproc-startup[811]: <13>Oct 13 23:02:11 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 79.'
<13>Oct 13 23:02:11 google-dataproc-startup[811]: <13>Oct 13 23:02:11 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 79.
<13>Oct 13 23:02:11 google-dataproc-startup[811]: <13>Oct 13 23:02:11 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:12 google-dataproc-startup[811]: <13>Oct 13 23:02:12 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:12 google-dataproc-startup[811]: <13>Oct 13 23:02:12 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:12 google-dataproc-startup[811]: <13>Oct 13 23:02:12 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:12 google-dataproc-startup[811]: <13>Oct 13 23:02:12 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 80.'
<13>Oct 13 23:02:12 google-dataproc-startup[811]: <13>Oct 13 23:02:12 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 80.'
<13>Oct 13 23:02:12 google-dataproc-startup[811]: <13>Oct 13 23:02:12 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 80.
<13>Oct 13 23:02:12 google-dataproc-startup[811]: <13>Oct 13 23:02:12 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:13 google-dataproc-startup[811]: <13>Oct 13 23:02:13 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:13 google-dataproc-startup[811]: <13>Oct 13 23:02:13 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:13 google-dataproc-startup[811]: <13>Oct 13 23:02:13 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:13 google-dataproc-startup[811]: <13>Oct 13 23:02:13 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 81.'
<13>Oct 13 23:02:13 google-dataproc-startup[811]: <13>Oct 13 23:02:13 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 81.'
<13>Oct 13 23:02:13 google-dataproc-startup[811]: <13>Oct 13 23:02:13 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 81.
<13>Oct 13 23:02:13 google-dataproc-startup[811]: <13>Oct 13 23:02:13 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: + update_succeeded=1
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: + break
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: + ((  1  ))
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: ++ local property_name=am.primary_only
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: +++ local property_name=am.primary_only
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: ++++ tail -n 1
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: ++++ cut -d = -f 2-
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 82.'
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 82.'
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 82.
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: +++ local property_value=false
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: +++ echo false
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: ++ local property_value=false
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: ++ echo false
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:02:14 google-dataproc-startup[811]: <13>Oct 13 23:02:14 setup-hadoop-hdfs-namenode[1465]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:02:15 google-dataproc-startup[811]: <13>Oct 13 23:02:15 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:15 google-dataproc-startup[811]: <13>Oct 13 23:02:15 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:15 google-dataproc-startup[811]: <13>Oct 13 23:02:15 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:15 google-dataproc-startup[811]: <13>Oct 13 23:02:15 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 83.'
<13>Oct 13 23:02:15 google-dataproc-startup[811]: <13>Oct 13 23:02:15 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 83.'
<13>Oct 13 23:02:15 google-dataproc-startup[811]: <13>Oct 13 23:02:15 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 83.
<13>Oct 13 23:02:15 google-dataproc-startup[811]: <13>Oct 13 23:02:15 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:16 google-dataproc-startup[811]: <13>Oct 13 23:02:16 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:16 google-dataproc-startup[811]: <13>Oct 13 23:02:16 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:16 google-dataproc-startup[811]: <13>Oct 13 23:02:16 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:16 google-dataproc-startup[811]: <13>Oct 13 23:02:16 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 84.'
<13>Oct 13 23:02:16 google-dataproc-startup[811]: <13>Oct 13 23:02:16 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 84.'
<13>Oct 13 23:02:16 google-dataproc-startup[811]: <13>Oct 13 23:02:16 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 84.
<13>Oct 13 23:02:16 google-dataproc-startup[811]: <13>Oct 13 23:02:16 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:17 google-dataproc-startup[811]: <13>Oct 13 23:02:17 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:17 google-dataproc-startup[811]: <13>Oct 13 23:02:17 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:17 google-dataproc-startup[811]: <13>Oct 13 23:02:17 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:17 google-dataproc-startup[811]: <13>Oct 13 23:02:17 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 85.'
<13>Oct 13 23:02:17 google-dataproc-startup[811]: <13>Oct 13 23:02:17 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 85.'
<13>Oct 13 23:02:17 google-dataproc-startup[811]: <13>Oct 13 23:02:17 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 85.
<13>Oct 13 23:02:17 google-dataproc-startup[811]: <13>Oct 13 23:02:17 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:18 google-dataproc-startup[811]: <13>Oct 13 23:02:18 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:18 google-dataproc-startup[811]: <13>Oct 13 23:02:18 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:18 google-dataproc-startup[811]: <13>Oct 13 23:02:18 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:18 google-dataproc-startup[811]: <13>Oct 13 23:02:18 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 86.'
<13>Oct 13 23:02:18 google-dataproc-startup[811]: <13>Oct 13 23:02:18 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 86.'
<13>Oct 13 23:02:18 google-dataproc-startup[811]: <13>Oct 13 23:02:18 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 86.
<13>Oct 13 23:02:18 google-dataproc-startup[811]: <13>Oct 13 23:02:18 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:19 google-dataproc-startup[811]: <13>Oct 13 23:02:19 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:19 google-dataproc-startup[811]: <13>Oct 13 23:02:19 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:19 google-dataproc-startup[811]: <13>Oct 13 23:02:19 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 23:02:19 google-dataproc-startup[811]: <13>Oct 13 23:02:19 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 87.'
<13>Oct 13 23:02:19 google-dataproc-startup[811]: <13>Oct 13 23:02:19 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 87.'
<13>Oct 13 23:02:19 google-dataproc-startup[811]: <13>Oct 13 23:02:19 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 failed. Retry attempt: 87.
<13>Oct 13 23:02:19 google-dataproc-startup[811]: <13>Oct 13 23:02:19 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 9083
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: Connection to cluster-6d45-m 9083 port [tcp/*] succeeded!
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + update_succeeded=1
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 9083 succeeded.'
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 9083 succeeded.'
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 9083 succeeded.
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + break
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + ((  1  ))
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + loginfo 'Service up on host=cluster-6d45-m port=9083.'
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + echo 'Service up on host=cluster-6d45-m port=9083.'
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: Service up on host=cluster-6d45-m port=9083.
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + run_with_retries systemctl start hive-server2
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + local retry_backoff
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + cmd=("$@")
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + local -a cmd
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + loginfo 'About to run '\''systemctl start hive-server2'\'' with retries...'
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + echo 'About to run '\''systemctl start hive-server2'\'' with retries...'
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: About to run 'systemctl start hive-server2' with retries...
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + local update_succeeded=0
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + (( i = 0 ))
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + (( i < 12 ))
<13>Oct 13 23:02:20 google-dataproc-startup[811]: <13>Oct 13 23:02:20 setup-hive-metastore[1467]: + systemctl start hive-server2
<13>Oct 13 23:02:23 google-dataproc-startup[811]: <13>Oct 13 23:02:23 setup-hive-metastore[1467]: + update_succeeded=1
<13>Oct 13 23:02:23 google-dataproc-startup[811]: <13>Oct 13 23:02:23 setup-hive-metastore[1467]: + break
<13>Oct 13 23:02:23 google-dataproc-startup[811]: <13>Oct 13 23:02:23 setup-hive-metastore[1467]: + ((  1  ))
<13>Oct 13 23:02:23 google-dataproc-startup[811]: <13>Oct 13 23:02:23 setup-hive-metastore[1467]: ++ get_xml_property_or_default /etc/hive/conf/hive-site.xml hive.server2.thrift.port 10000
<13>Oct 13 23:02:23 google-dataproc-startup[811]: <13>Oct 13 23:02:23 setup-hive-metastore[1467]: ++ file=/etc/hive/conf/hive-site.xml
<13>Oct 13 23:02:23 google-dataproc-startup[811]: <13>Oct 13 23:02:23 setup-hive-metastore[1467]: ++ property=hive.server2.thrift.port
<13>Oct 13 23:02:23 google-dataproc-startup[811]: <13>Oct 13 23:02:23 setup-hive-metastore[1467]: ++ default_value=10000
<13>Oct 13 23:02:23 google-dataproc-startup[811]: <13>Oct 13 23:02:23 setup-hive-metastore[1467]: +++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.server2.thrift.port
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: ++ val=None
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: ++ [[ None = \N\o\n\e ]]
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: ++ val=10000
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: ++ echo 10000
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + thrift_port=10000
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + wait_for_port cluster-6d45-m 10000
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + local -r host=cluster-6d45-m
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + local -r port=10000
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + loginfo 'Waiting for service to come up on host=cluster-6d45-m port=10000.'
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + echo 'Waiting for service to come up on host=cluster-6d45-m port=10000.'
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: Waiting for service to come up on host=cluster-6d45-m port=10000.
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + retry_with_constant_backoff nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + local max_retry=300
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + cmd=("$@")
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + local -a cmd
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + local update_succeeded=0
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: ++ seq 1 300
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 1.'
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 1.'
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 1.
<13>Oct 13 23:02:24 google-dataproc-startup[811]: <13>Oct 13 23:02:24 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:25 google-dataproc-startup[811]: <13>Oct 13 23:02:25 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:25 google-dataproc-startup[811]: <13>Oct 13 23:02:25 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:25 google-dataproc-startup[811]: <13>Oct 13 23:02:25 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:25 google-dataproc-startup[811]: <13>Oct 13 23:02:25 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 2.'
<13>Oct 13 23:02:25 google-dataproc-startup[811]: <13>Oct 13 23:02:25 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 2.'
<13>Oct 13 23:02:25 google-dataproc-startup[811]: <13>Oct 13 23:02:25 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 2.
<13>Oct 13 23:02:25 google-dataproc-startup[811]: <13>Oct 13 23:02:25 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:26 google-dataproc-startup[811]: <13>Oct 13 23:02:26 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:26 google-dataproc-startup[811]: <13>Oct 13 23:02:26 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:26 google-dataproc-startup[811]: <13>Oct 13 23:02:26 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:26 google-dataproc-startup[811]: <13>Oct 13 23:02:26 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 3.'
<13>Oct 13 23:02:26 google-dataproc-startup[811]: <13>Oct 13 23:02:26 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 3.'
<13>Oct 13 23:02:26 google-dataproc-startup[811]: <13>Oct 13 23:02:26 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 3.
<13>Oct 13 23:02:26 google-dataproc-startup[811]: <13>Oct 13 23:02:26 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:27 google-dataproc-startup[811]: <13>Oct 13 23:02:27 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:27 google-dataproc-startup[811]: <13>Oct 13 23:02:27 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:27 google-dataproc-startup[811]: <13>Oct 13 23:02:27 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:27 google-dataproc-startup[811]: <13>Oct 13 23:02:27 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 4.'
<13>Oct 13 23:02:27 google-dataproc-startup[811]: <13>Oct 13 23:02:27 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 4.'
<13>Oct 13 23:02:27 google-dataproc-startup[811]: <13>Oct 13 23:02:27 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 4.
<13>Oct 13 23:02:27 google-dataproc-startup[811]: <13>Oct 13 23:02:27 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:28 google-dataproc-startup[811]: <13>Oct 13 23:02:28 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:28 google-dataproc-startup[811]: <13>Oct 13 23:02:28 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:28 google-dataproc-startup[811]: <13>Oct 13 23:02:28 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:28 google-dataproc-startup[811]: <13>Oct 13 23:02:28 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 5.'
<13>Oct 13 23:02:28 google-dataproc-startup[811]: <13>Oct 13 23:02:28 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 5.'
<13>Oct 13 23:02:28 google-dataproc-startup[811]: <13>Oct 13 23:02:28 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 5.
<13>Oct 13 23:02:28 google-dataproc-startup[811]: <13>Oct 13 23:02:28 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:29 google-dataproc-startup[811]: <13>Oct 13 23:02:29 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:29 google-dataproc-startup[811]: <13>Oct 13 23:02:29 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:29 google-dataproc-startup[811]: <13>Oct 13 23:02:29 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:29 google-dataproc-startup[811]: <13>Oct 13 23:02:29 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 6.'
<13>Oct 13 23:02:29 google-dataproc-startup[811]: <13>Oct 13 23:02:29 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 6.'
<13>Oct 13 23:02:29 google-dataproc-startup[811]: <13>Oct 13 23:02:29 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 6.
<13>Oct 13 23:02:29 google-dataproc-startup[811]: <13>Oct 13 23:02:29 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:30 google-dataproc-startup[811]: <13>Oct 13 23:02:30 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:30 google-dataproc-startup[811]: <13>Oct 13 23:02:30 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:30 google-dataproc-startup[811]: <13>Oct 13 23:02:30 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:30 google-dataproc-startup[811]: <13>Oct 13 23:02:30 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 7.'
<13>Oct 13 23:02:30 google-dataproc-startup[811]: <13>Oct 13 23:02:30 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 7.'
<13>Oct 13 23:02:30 google-dataproc-startup[811]: <13>Oct 13 23:02:30 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 7.
<13>Oct 13 23:02:30 google-dataproc-startup[811]: <13>Oct 13 23:02:30 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:31 google-dataproc-startup[811]: <13>Oct 13 23:02:31 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:31 google-dataproc-startup[811]: <13>Oct 13 23:02:31 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:31 google-dataproc-startup[811]: <13>Oct 13 23:02:31 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:31 google-dataproc-startup[811]: <13>Oct 13 23:02:31 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 8.'
<13>Oct 13 23:02:31 google-dataproc-startup[811]: <13>Oct 13 23:02:31 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 8.'
<13>Oct 13 23:02:31 google-dataproc-startup[811]: <13>Oct 13 23:02:31 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 8.
<13>Oct 13 23:02:31 google-dataproc-startup[811]: <13>Oct 13 23:02:31 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:32 google-dataproc-startup[811]: <13>Oct 13 23:02:32 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:32 google-dataproc-startup[811]: <13>Oct 13 23:02:32 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:32 google-dataproc-startup[811]: <13>Oct 13 23:02:32 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:32 google-dataproc-startup[811]: <13>Oct 13 23:02:32 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 9.'
<13>Oct 13 23:02:32 google-dataproc-startup[811]: <13>Oct 13 23:02:32 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 9.'
<13>Oct 13 23:02:32 google-dataproc-startup[811]: <13>Oct 13 23:02:32 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 9.
<13>Oct 13 23:02:32 google-dataproc-startup[811]: <13>Oct 13 23:02:32 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:33 google-dataproc-startup[811]: <13>Oct 13 23:02:33 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:33 google-dataproc-startup[811]: <13>Oct 13 23:02:33 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:33 google-dataproc-startup[811]: <13>Oct 13 23:02:33 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:33 google-dataproc-startup[811]: <13>Oct 13 23:02:33 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 10.'
<13>Oct 13 23:02:33 google-dataproc-startup[811]: <13>Oct 13 23:02:33 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 10.'
<13>Oct 13 23:02:33 google-dataproc-startup[811]: <13>Oct 13 23:02:33 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 10.
<13>Oct 13 23:02:33 google-dataproc-startup[811]: <13>Oct 13 23:02:33 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:34 google-dataproc-startup[811]: <13>Oct 13 23:02:34 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:34 google-dataproc-startup[811]: <13>Oct 13 23:02:34 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:34 google-dataproc-startup[811]: <13>Oct 13 23:02:34 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:34 google-dataproc-startup[811]: <13>Oct 13 23:02:34 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 11.'
<13>Oct 13 23:02:34 google-dataproc-startup[811]: <13>Oct 13 23:02:34 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 11.'
<13>Oct 13 23:02:34 google-dataproc-startup[811]: <13>Oct 13 23:02:34 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 11.
<13>Oct 13 23:02:34 google-dataproc-startup[811]: <13>Oct 13 23:02:34 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:35 google-dataproc-startup[811]: <13>Oct 13 23:02:35 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:35 google-dataproc-startup[811]: <13>Oct 13 23:02:35 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:35 google-dataproc-startup[811]: <13>Oct 13 23:02:35 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:35 google-dataproc-startup[811]: <13>Oct 13 23:02:35 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 12.'
<13>Oct 13 23:02:35 google-dataproc-startup[811]: <13>Oct 13 23:02:35 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 12.'
<13>Oct 13 23:02:35 google-dataproc-startup[811]: <13>Oct 13 23:02:35 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 12.
<13>Oct 13 23:02:35 google-dataproc-startup[811]: <13>Oct 13 23:02:35 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:36 google-dataproc-startup[811]: <13>Oct 13 23:02:36 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:36 google-dataproc-startup[811]: <13>Oct 13 23:02:36 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:36 google-dataproc-startup[811]: <13>Oct 13 23:02:36 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:36 google-dataproc-startup[811]: <13>Oct 13 23:02:36 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 13.'
<13>Oct 13 23:02:36 google-dataproc-startup[811]: <13>Oct 13 23:02:36 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 13.'
<13>Oct 13 23:02:36 google-dataproc-startup[811]: <13>Oct 13 23:02:36 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 13.
<13>Oct 13 23:02:36 google-dataproc-startup[811]: <13>Oct 13 23:02:36 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:37 google-dataproc-startup[811]: <13>Oct 13 23:02:37 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:37 google-dataproc-startup[811]: <13>Oct 13 23:02:37 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:37 google-dataproc-startup[811]: <13>Oct 13 23:02:37 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:37 google-dataproc-startup[811]: <13>Oct 13 23:02:37 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 14.'
<13>Oct 13 23:02:37 google-dataproc-startup[811]: <13>Oct 13 23:02:37 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 14.'
<13>Oct 13 23:02:37 google-dataproc-startup[811]: <13>Oct 13 23:02:37 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 14.
<13>Oct 13 23:02:37 google-dataproc-startup[811]: <13>Oct 13 23:02:37 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:38 google-dataproc-startup[811]: <13>Oct 13 23:02:38 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:38 google-dataproc-startup[811]: <13>Oct 13 23:02:38 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:38 google-dataproc-startup[811]: <13>Oct 13 23:02:38 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:38 google-dataproc-startup[811]: <13>Oct 13 23:02:38 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 15.'
<13>Oct 13 23:02:38 google-dataproc-startup[811]: <13>Oct 13 23:02:38 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 15.'
<13>Oct 13 23:02:38 google-dataproc-startup[811]: <13>Oct 13 23:02:38 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 15.
<13>Oct 13 23:02:38 google-dataproc-startup[811]: <13>Oct 13 23:02:38 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:39 google-dataproc-startup[811]: <13>Oct 13 23:02:39 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:39 google-dataproc-startup[811]: <13>Oct 13 23:02:39 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:39 google-dataproc-startup[811]: <13>Oct 13 23:02:39 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:39 google-dataproc-startup[811]: <13>Oct 13 23:02:39 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 16.'
<13>Oct 13 23:02:39 google-dataproc-startup[811]: <13>Oct 13 23:02:39 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 16.'
<13>Oct 13 23:02:39 google-dataproc-startup[811]: <13>Oct 13 23:02:39 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 16.
<13>Oct 13 23:02:39 google-dataproc-startup[811]: <13>Oct 13 23:02:39 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:40 google-dataproc-startup[811]: <13>Oct 13 23:02:40 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:40 google-dataproc-startup[811]: <13>Oct 13 23:02:40 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:40 google-dataproc-startup[811]: <13>Oct 13 23:02:40 setup-hive-metastore[1467]: nc: connect to cluster-6d45-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 23:02:40 google-dataproc-startup[811]: <13>Oct 13 23:02:40 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 17.'
<13>Oct 13 23:02:40 google-dataproc-startup[811]: <13>Oct 13 23:02:40 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 17.'
<13>Oct 13 23:02:40 google-dataproc-startup[811]: <13>Oct 13 23:02:40 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 failed. Retry attempt: 17.
<13>Oct 13 23:02:40 google-dataproc-startup[811]: <13>Oct 13 23:02:40 setup-hive-metastore[1467]: + sleep 1
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + nc -v -z -w 0 cluster-6d45-m 10000
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: Connection to cluster-6d45-m 10000 port [tcp/webmin] succeeded!
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + update_succeeded=1
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + loginfo 'nc -v -z -w 0 cluster-6d45-m 10000 succeeded.'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + echo 'nc -v -z -w 0 cluster-6d45-m 10000 succeeded.'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: nc -v -z -w 0 cluster-6d45-m 10000 succeeded.
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + break
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + ((  1  ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + loginfo 'Service up on host=cluster-6d45-m port=10000.'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + echo 'Service up on host=cluster-6d45-m port=10000.'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: Service up on host=cluster-6d45-m port=10000.
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: ++ get_dataproc_property am.primary_only
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: ++ local property_name=am.primary_only
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: +++ local property_name=am.primary_only
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: ++++ cut -d = -f 2-
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: ++++ tail -n 1
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: +++ local property_value=false
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: +++ echo false
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + pid=1466
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + cmd='setup_service hadoop-yarn-resourcemanager'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1466 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + echo 'Waiting on pid=1466 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: Waiting on pid=1466 cmd=[setup_service hadoop-yarn-resourcemanager]
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + wait 1466
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + pid=1465
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + cmd='setup_service hadoop-hdfs-namenode'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1465 cmd=[setup_service hadoop-hdfs-namenode]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + echo 'Waiting on pid=1465 cmd=[setup_service hadoop-hdfs-namenode]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: Waiting on pid=1465 cmd=[setup_service hadoop-hdfs-namenode]
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + wait 1465
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + pid=1408
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + cmd='uninstall_component proxy-agent'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1408 cmd=[uninstall_component proxy-agent]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + echo 'Waiting on pid=1408 cmd=[uninstall_component proxy-agent]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: Waiting on pid=1408 cmd=[uninstall_component proxy-agent]
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + wait 1408
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + pid=1407
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + cmd='uninstall_component presto'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1407 cmd=[uninstall_component presto]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + echo 'Waiting on pid=1407 cmd=[uninstall_component presto]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: Waiting on pid=1407 cmd=[uninstall_component presto]
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + wait 1407
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + pid=1406
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + cmd='uninstall_component kerberos'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1406 cmd=[uninstall_component kerberos]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + echo 'Waiting on pid=1406 cmd=[uninstall_component kerberos]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: Waiting on pid=1406 cmd=[uninstall_component kerberos]
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + wait 1406
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + pid=1405
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + cmd='uninstall_component jupyter'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1405 cmd=[uninstall_component jupyter]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + echo 'Waiting on pid=1405 cmd=[uninstall_component jupyter]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: Waiting on pid=1405 cmd=[uninstall_component jupyter]
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + wait 1405
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + pid=1404
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + cmd='uninstall_component anaconda'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1404 cmd=[uninstall_component anaconda]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + echo 'Waiting on pid=1404 cmd=[uninstall_component anaconda]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: Waiting on pid=1404 cmd=[uninstall_component anaconda]
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + wait 1404
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + pid=1403
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + cmd='bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + loginfo 'Waiting on pid=1403 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + echo 'Waiting on pid=1403 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: Waiting on pid=1403 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + status=0
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + wait 1403
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( status != 0 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( ++i  ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + (( i < 16 ))
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + BACKGROUND_PROCESSES=()
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + BACKGROUND_COMMANDS=()
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + systemctl daemon-reload
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: ++ local property_value=false
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: ++ echo false
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 23:02:41 google-dataproc-startup[811]: <13>Oct 13 23:02:41 setup-hive-metastore[1467]: + [[ hive-metastore == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + loginfo 'All done'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: + echo 'All done'
<13>Oct 13 23:02:41 google-dataproc-startup[811]: All done
