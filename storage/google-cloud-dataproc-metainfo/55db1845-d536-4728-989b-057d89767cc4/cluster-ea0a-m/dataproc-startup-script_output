+ run_with_logger --tag google-dataproc-startup
+ local tag=
+ local pid=837
+ [[ --tag == \-\-\t\a\g ]]
+ tag=google-dataproc-startup
+ shift 2
+ exec
++ logger -s -t 'google-dataproc-startup[837]'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=()
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=()
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + cd /tmp
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ INSTALL_GCS_CONNECTOR=1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ ENABLE_HDFS=1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ ENABLE_NFS_GCS_FILE_CACHE=1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ GCS_ADMIN=gcsadmin
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ GCS_CACHE_CLEANER_LOG_DIRECTORY=/var/log/hadoop-hdfs
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,DRFA
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,RFA
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ NUM_WORKERS=10
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ WORKERS=()
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ CORES_PER_MAP_TASK=1.0
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ CORES_PER_APP_MASTER=2.0
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ HDFS_DATA_DIRS_PERM=700
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ COMMON_JAVA_OPTS=("-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance")
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ DATAPROC_VERSION_KEY=1_3
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ declare -A DATAPROC_TO_GCS_CONNECTOR_VERSION
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ DATAPROC_TO_GCS_CONNECTOR_VERSION=(["1_0"]="1.6.10-hadoop2" ["1_1"]="1.6.10-hadoop2" ["1_2"]="1.6.10-hadoop2" ["1_3"]="hadoop2-1.9.17" ["1_4"]="hadoop2-1.9.17" ["1_5"]="hadoop2-1.9.17" ["2_0"]="hadoop3-1.9.17")
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ GCS_CONNECTOR_VERSION=hadoop2-1.9.17
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ GCS_CONNECTOR_JAR=https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-1.9.17.jar
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ /usr/share/google/get_metadata_value attributes/dataproc_gcs_connector_url
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ true
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ GCS_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ [[ ! -z '' ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ declare -A DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ DATAPROC_TO_BQ_CONNECTOR_VERSION=(["1_0"]="0.10.11-hadoop2" ["1_1"]="0.10.11-hadoop2" ["1_2"]="0.10.11-hadoop2")
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ BIGQUERY_CONNECTOR_VERSION=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ [[ -n '' ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ /usr/share/google/get_metadata_value attributes/dataproc_bq_connector_url
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ true
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ BIGQUERY_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ [[ ! -z '' ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ unset DATAPROC_VERSION_KEY GCS_CONNECTOR_VERSION BIGQUERY_CONNECTOR_VERSION DATAPROC_TO_GCS_CONNECTOR_VERSION DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ dpkg -s hive
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ HIVE_VERSION=2.3.5
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ dpkg -s spark-core
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ SPARK_VERSION=2.3.3
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ readonly APT_SENTINEL=apt.lastupdate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ APT_SENTINEL=apt.lastupdate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + source /usr/local/share/google/dataproc/bdutil/cluster_properties.sh
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + source /usr/local/share/google/dataproc/bdutil/components/startup-script-components.sh
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ readonly COMPONENTS_DIR=/var/lib/google/dataproc/components
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ COMPONENTS_DIR=/var/lib/google/dataproc/components
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + trap logstacktrace ERR
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + loginfo 'Starting Dataproc startup script'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + echo 'Starting Dataproc startup script'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: Starting Dataproc startup script
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ hostname -s
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + MY_HOSTNAME=cluster-ea0a-m
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ hostname -f
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ dnsdomainname
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + DOMAIN=us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ echo cluster-ea0a-m
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ sed -r 's/-([mw](-[0-9]*)?)$//'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + PREFIX=cluster-ea0a
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + merge_java_properties /tmp/cluster/properties/dataproc.properties /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + local dest=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/dataproc.properties ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + cat /tmp/cluster/properties/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: Merged /tmp/cluster/properties/dataproc.properties.
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ local property_name=dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_name=dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_value=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ echo ''
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ local property_value=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ echo ''
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + OPTIONAL_COMPONENTS_VALUE=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + is_version_at_least 1.3 1.4
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + local ver1=1.3.0.0.0.0
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + local ver2=1.4
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + local log=/tmp/tmp.1dMUgT1rN1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + err_code=1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.1dMUgT1rN1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + rm -f /tmp/tmp.1dMUgT1rN1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + DATAPROC_MASTER=cluster-ea0a-m
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master-additional
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + DATAPROC_MASTER_ADDITIONAL=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + MASTER_COUNT=1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + ((  1 > 1  ))
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + is_component_selected kafka-server
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + local component=kafka-server
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ local property_name=dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_name=dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_value=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ echo ''
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ local property_value=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ echo ''
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + local activated_components=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + [[ '' == *kafka-server* ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + COMPONENTS_TO_ACTIVATE=(${OPTIONAL_COMPONENTS_VALUE})
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + KERBEROS_ENABLED=false
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + is_component_selected kerberos
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + local component=kerberos
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ local property_name=dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_name=dataproc.components.activate
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_value=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ echo ''
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ local property_value=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ echo ''
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + local activated_components=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + [[ '' == *kerberos* ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + [[ false == \t\r\u\e ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ /usr/share/google/get_metadata_value ../project/project-id
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + PROJECT=lustrous-drake-255300
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ /usr/share/google/get_metadata_value attributes/dataproc-bucket
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + CONFIGBUCKET=dataproc-ed3c3d29-fb10-47bb-aca7-dcc358c68973-us-central1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ /usr/share/google/get_metadata_value attributes/dataproc-role
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + ROLE=Master
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-name
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + CLUSTER_NAME=cluster-ea0a
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-uuid
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + CLUSTER_UUID=55db1845-d536-4728-989b-057d89767cc4
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ /usr/share/google/get_metadata_value attributes/dataproc-worker-count
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + WORKER_COUNT=2
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + HBASE_CONF_DIR=/etc/hbase/conf
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + PIG_CONF_DIR=/etc/pig/conf
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + SPARK_CONF_DIR=/etc/spark/conf
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + ZOOKEEPER_CONF_DIR=/etc/zookeeper/conf
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + HADOOP_2_PORTS=(50010 50020 50070 50090)
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + ((  1 > 1  ))
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + HDFS_ROOT_URI=hdfs://cluster-ea0a-m
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + hostname=cluster-ea0a-m
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + [[ false == \t\r\u\e ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + for i in "${!MASTER_HOSTNAMES[@]}"
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + [[ cluster-ea0a-m == \c\l\u\s\t\e\r\-\e\a\0\a\-\m ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + MASTER_INDEX=0
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + break
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + ((  2 == 0  ))
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + ((  1 > 1  ))
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + PACKAGES_TO_UNINSTALL=(${DATAPROC_MASTER_HA_SERVICES} ${DATAPROC_WORKER_SERVICES})
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + SERVICES=(${DATAPROC_MASTER_SERVICES} ${DATAPROC_MASTER_EXCLUSIVE_SERVICES} ${DATAPROC_MASTER_STANDALONE_SERVICES})
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + loginfo 'Generating helper scripts'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + echo 'Generating helper scripts'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: Generating helper scripts
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + sudo -u hdfs hdfs namenode -genclusterid
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ cat /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ (( i=0 ))
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ (( i<1 ))
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ echo MASTER_HOSTNAME_0=cluster-ea0a-m
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ (( i++  ))
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ (( i<1 ))
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ cat /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ cat /usr/local/share/google/dataproc/bdutil/setup_master_nfs.sh /usr/local/share/google/dataproc/bdutil/setup_client_nfs.sh
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ cat /usr/local/share/google/dataproc/bdutil/configure_hadoop.sh /usr/local/share/google/dataproc/bdutil/configure_mysql.sh /usr/local/share/google/dataproc/bdutil/configure_hive.sh /usr/local/share/google/dataproc/bdutil/configure_hdfs.sh /usr/local/share/google/dataproc/bdutil/configure_connectors.sh /usr/local/share/google/dataproc/bdutil/configure_spark.sh /usr/local/share/google/dataproc/bdutil/configure_tez.sh /usr/local/share/google/dataproc/bdutil/configure_zookeeper.sh
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + cp -r /usr/local/share/google/dataproc/bdutil/conf/bq-mapred-template.xml /usr/local/share/google/dataproc/bdutil/conf/capacity-scheduler-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-template.xml /usr/local/share/google/dataproc/bdutil/conf/distcp-template.xml /usr/local/share/google/dataproc/bdutil/conf/gcs-core-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-template.xml /usr/local/share/google/dataproc/bdutil/conf/hive-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hive-template.xml /usr/local/share/google/dataproc/bdutil/conf/knox /usr/local/share/google/dataproc/bdutil/conf/mapred-template.xml /usr/local/share/google/dataproc/b
<13>Oct 13 16:53:24 google-dataproc-startup[837]: dutil/conf/yarn-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-template.xml /tmp
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + cp /usr/local/share/google/dataproc/bdutil/configure_mrv2_mem.py /tmp
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + chmod +x configure_mrv2_mem.py
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + loginfo 'Running helper scripts'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + echo 'Running helper scripts'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: Running helper scripts
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ get_dataproc_property dataproc.localssd.mount.enable
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ local property_name=dataproc.localssd.mount.enable
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.localssd.mount.enable
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_name=dataproc.localssd.mount.enable
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ grep '^dataproc.localssd.mount.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ local property_value=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: +++ echo ''
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ local property_value=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: ++ echo ''
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + MOUNT_DISKS_ENABLED=
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + [[ '' == \f\a\l\s\e ]]
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + DATAPROC_MOUNT_SERVICE_FILE=/usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + chmod +x /usr/local/share/google/dataproc/bdutil/mount_disks.sh
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + chmod 644 /usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Oct 13 16:53:24 google-dataproc-startup[837]: + systemctl enable google-dataproc-disk-mount
<13>Oct 13 16:53:24 google-dataproc-startup[837]: Created symlink /etc/systemd/system/multi-user.target.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 16:53:24 google-dataproc-startup[837]: Created symlink /etc/systemd/system/hadoop-hdfs-namenode.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 16:53:24 google-dataproc-startup[837]: Created symlink /etc/systemd/system/hadoop-hdfs-datanode.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 16:53:24 google-dataproc-startup[837]: Created symlink /etc/systemd/system/hadoop-yarn-resourcemanager.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 16:53:24 google-dataproc-startup[837]: Created symlink /etc/systemd/system/hadoop-yarn-nodemanager.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + systemctl start google-dataproc-disk-mount
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + in_array nfs-kernel-server DATAPROC_MASTER_EXCLUSIVE_SERVICES
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + local value=nfs-kernel-server
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + local -n values=DATAPROC_MASTER_EXCLUSIVE_SERVICES
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + [[ !  hadoop-mapreduce-historyserver spark-history-server hive-webhcat-server jupyter knox proxy-agent zeppelin hadoop-yarn-timelineserver mariadb-server  =~  nfs-kernel-server  ]]
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + bash configuration_script.sh
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ INSTALL_GCS_CONNECTOR=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ ENABLE_HDFS=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ ENABLE_NFS_GCS_FILE_CACHE=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ GCS_ADMIN=gcsadmin
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ GCS_CACHE_CLEANER_LOG_DIRECTORY=/var/log/hadoop-hdfs
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,DRFA
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,RFA
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ NUM_WORKERS=10
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ WORKERS=()
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ CORES_PER_MAP_TASK=1.0
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ CORES_PER_APP_MASTER=2.0
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ HDFS_DATA_DIRS_PERM=700
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ COMMON_JAVA_OPTS=("-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance")
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ DATAPROC_VERSION_KEY=1_3
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ declare -A DATAPROC_TO_GCS_CONNECTOR_VERSION
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ DATAPROC_TO_GCS_CONNECTOR_VERSION=(["1_0"]="1.6.10-hadoop2" ["1_1"]="1.6.10-hadoop2" ["1_2"]="1.6.10-hadoop2" ["1_3"]="hadoop2-1.9.17" ["1_4"]="hadoop2-1.9.17" ["1_5"]="hadoop2-1.9.17" ["2_0"]="hadoop3-1.9.17")
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ GCS_CONNECTOR_VERSION=hadoop2-1.9.17
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ GCS_CONNECTOR_JAR=https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-1.9.17.jar
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ /usr/share/google/get_metadata_value attributes/dataproc_gcs_connector_url
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ true
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ GCS_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ [[ ! -z '' ]]
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ declare -A DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ DATAPROC_TO_BQ_CONNECTOR_VERSION=(["1_0"]="0.10.11-hadoop2" ["1_1"]="0.10.11-hadoop2" ["1_2"]="0.10.11-hadoop2")
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ BIGQUERY_CONNECTOR_VERSION=
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ [[ -n '' ]]
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ /usr/share/google/get_metadata_value attributes/dataproc_bq_connector_url
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ true
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ BIGQUERY_CONNECTOR_JAR_OVERRIDE=
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ [[ ! -z '' ]]
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ unset DATAPROC_VERSION_KEY GCS_CONNECTOR_VERSION BIGQUERY_CONNECTOR_VERSION DATAPROC_TO_GCS_CONNECTOR_VERSION DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ dpkg -s hive
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ HIVE_VERSION=2.3.5
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ dpkg -s spark-core
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ SPARK_VERSION=2.3.3
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ CLUSTER_NAME=cluster-ea0a
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ CLUSTER_UUID=55db1845-d536-4728-989b-057d89767cc4
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ CONFIGBUCKET=dataproc-ed3c3d29-fb10-47bb-aca7-dcc358c68973-us-central1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ HDFS_ROOT_URI=hdfs://cluster-ea0a-m
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ MASTER_HOSTNAME_0=cluster-ea0a-m
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ MASTER_HOSTNAMES=(cluster-ea0a-m)
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ NUM_MASTERS=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ NUM_WORKERS=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ PREFIX=cluster-ea0a
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ PROJECT=lustrous-drake-255300
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ ROLE=Master
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ set +a
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ readonly APT_SENTINEL=apt.lastupdate
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ APT_SENTINEL=apt.lastupdate
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + set -e
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + loginfo 'Running configure_hadoop.sh'
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + echo 'Running configure_hadoop.sh'
<13>Oct 13 16:53:25 google-dataproc-startup[837]: Running configure_hadoop.sh
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + export HADOOP_TMP_DIR=/hadoop/tmp
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + HADOOP_TMP_DIR=/hadoop/tmp
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + mkdir -p /hadoop/tmp
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + export DEFAULT_NUM_MAPS=20
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + DEFAULT_NUM_MAPS=20
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + export DEFAULT_NUM_REDUCES=8
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + DEFAULT_NUM_REDUCES=8
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ grep -c processor /proc/cpuinfo
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + export NUM_CORES=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + NUM_CORES=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ python -c 'print int(2 //     1.0)'
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + export MAP_SLOTS=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + MAP_SLOTS=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ python -c 'print int(2 //     2.0)'
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + export REDUCE_SLOTS=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + REDUCE_SLOTS=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ awk '/^Mem:/{print $2}'
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ free -m
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + TOTAL_MEM=7483
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ python -c 'print int(7483 *     0.4)'
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + HADOOP_MR_MASTER_MEM_MB=2993
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + [[ -x configure_mrv2_mem.py ]]
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ mktemp /tmp/mrv2_XXX_tmp_env.sh
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + TEMP_ENV_FILE=/tmp/mrv2_ptf_tmp_env.sh
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + ./configure_mrv2_mem.py --output_file /tmp/mrv2_ptf_tmp_env.sh --total_memory 7483 --available_memory_ratio 0.8 --total_cores 2 --cores_per_map 1.0 --cores_per_reduce 2.0 --cores_per_app_master 2.0
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + source /tmp/mrv2_ptf_tmp_env.sh
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export YARN_MIN_MEM_MB=512
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ YARN_MIN_MEM_MB=512
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export YARN_MAX_MEM_MB=5632
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ YARN_MAX_MEM_MB=5632
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export NODEMANAGER_MEM_MB=5632
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ NODEMANAGER_MEM_MB=5632
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export APP_MASTER_MEM_MB=5632
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ APP_MASTER_MEM_MB=5632
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export MAP_MEM_MB=2560
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ MAP_MEM_MB=2560
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ CORES_PER_MAP_ROUNDED=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export REDUCE_MEM_MB=5632
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ REDUCE_MEM_MB=5632
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export CORES_PER_REDUCE_ROUNDED=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ CORES_PER_REDUCE_ROUNDED=2
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ export REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ python -c 'print int(7483 / 4)'
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + HADOOP_CLIENT_MEM_MB=1870
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + is_version_at_least 1.3 1.4
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + local ver1=1.3.0.0.0.0
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + local ver2=1.4
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + local log=/tmp/tmp.BuNMedDTrF
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + err_code=1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.BuNMedDTrF
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + rm -f /tmp/tmp.BuNMedDTrF
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + DATA_DIRS=($(get_data_dirs))
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ get_data_dirs
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ mount_points=($(find /mnt/[0-9]*/ -maxdepth 0 || true))
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Oct 13 16:53:25 google-dataproc-startup[837]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ true
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ local mount_points
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ ((  0  ))
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ echo /
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ return
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + mkdir -p /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + chgrp hadoop -L -R /hadoop /hadoop/tmp /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + chmod g+rwx -R /hadoop /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + chmod 777 -R /hadoop/tmp
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + export MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + export NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + YARN_ENV_FILE=/etc/hadoop/conf/yarn-env.sh
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + [[ -f /etc/hadoop/conf/yarn-env.sh ]]
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ get_dataproc_property simplified.scaling.enable
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ local property_name=simplified.scaling.enable
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ local property_name=simplified.scaling.enable
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ local property_value=
<13>Oct 13 16:53:25 google-dataproc-startup[837]: +++ echo ''
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ local property_value=
<13>Oct 13 16:53:25 google-dataproc-startup[837]: ++ echo ''
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + [[ '' == \t\r\u\e ]]
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + touch /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + chown root:hadoop /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + [[ 1 -gt 1 ]]
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + CORE_TEMPLATE=core-template.xml
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + YARN_TEMPLATE=yarn-template.xml
<13>Oct 13 16:53:25 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file core-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file mapred-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file yarn-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file capacity-scheduler-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file distcp-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + is_version_at_least 1.3 1.4
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + local ver1=1.3.0.0.0.0
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + local ver2=1.4
<13>Oct 13 16:53:26 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + local log=/tmp/tmp.UsU4TzjbSw
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + err_code=1
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.UsU4TzjbSw
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + rm -f /tmp/tmp.UsU4TzjbSw
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value false --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + ZK_QUORUM=cluster-ea0a-m:2181,:2181,:2181
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + [[ 1 -gt 1 ]]
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + is_version_at_least 1.3 1.2
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + local ver1=1.3.0.0.0.0
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + local ver2=1.2
<13>Oct 13 16:53:26 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + local log=/tmp/tmp.XrNfSUszu6
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.2
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + err_code=0
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.XrNfSUszu6
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + rm -f /tmp/tmp.XrNfSUszu6
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + return 0
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.file.close.num-committed-allowed --value 1 --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + is_version_at_least 1.3 1.3
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + local ver1=1.3.0.0.0.0
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + local ver2=1.3
<13>Oct 13 16:53:26 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + local log=/tmp/tmp.chTHNI6ZeO
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + err_code=0
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.chTHNI6ZeO
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + rm -f /tmp/tmp.chTHNI6ZeO
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + return 0
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/core-site.xml --name hadoop.http.filter.initializers --value org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.webapp.cross-origin.enabled --value true --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.http-cross-origin.enabled --value true --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.enabled --value true --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.hostname --value cluster-ea0a-m --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.bind-host --value 0.0.0.0 --clobber
<13>Oct 13 16:53:26 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.system-metrics-publisher.enabled --value true --clobber
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.generic-application-history.enabled --value true --clobber
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ get_dataproc_property am.primary_only
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ local property_name=am.primary_only
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ local property_name=am.primary_only
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ local property_value=false
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ echo false
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ local property_value=false
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ echo false
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ /usr/share/google/get_metadata_value attributes/dataproc-datanode-enabled
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + DATAPROC_DATANODE_ENABLED=true
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ false == \t\r\u\e ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ '' == \t\r\u\e ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + set -e -x
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + grep -lr bind-address /etc/mysql
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + xargs -n1 sed -i 's/^\(bind-address\)\s*=.*/\1 = 0.0.0.0/'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + set -e -x
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + is_version_at_least 1.3 1.3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver1=1.3.0.0.0.0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver2=1.3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local log=/tmp/tmp.Qw9hM7yYgg
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + err_code=0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.Qw9hM7yYgg
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + rm -f /tmp/tmp.Qw9hM7yYgg
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + return 0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + is_version_at_least 1.3 1.4
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver1=1.3.0.0.0.0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver2=1.4
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local log=/tmp/tmp.DAsaWinJFY
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + err_code=1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.DAsaWinJFY
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + rm -f /tmp/tmp.DAsaWinJFY
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ 1 -gt 1 ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + METASTORE_URIS=thrift://cluster-ea0a-m:9083
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris --value thrift://cluster-ea0a-m:9083 --clobber
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + METADATA_STORE=jdbc:mysql://cluster-ea0a-m/metastore
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name javax.jdo.option.ConnectionURL --value jdbc:mysql://cluster-ea0a-m/metastore --clobber
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ 1 -gt 1 ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + set -e
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + loginfo 'Running configure_hdfs.sh'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + echo 'Running configure_hdfs.sh'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: Running configure_hdfs.sh
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + HDFS_ADMIN=hdfs
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + export HDFS_NAME_DIR=/hadoop/dfs/name
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + HDFS_NAME_DIR=/hadoop/dfs/name
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + export HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + DATA_DIRS=($(get_data_dirs))
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ get_data_dirs
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ mount_points=($(find /mnt/[0-9]*/ -maxdepth 0 || true))
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ true
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ local mount_points
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ ((  0  ))
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ echo /
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ return
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + mkdir -p /hadoop/dfs /hadoop/dfs/data
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + chown hdfs:hadoop -L -R /hadoop/dfs /hadoop/dfs/data
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + chmod 700 /hadoop/dfs/data
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ awk '/^Mem:/{print $2}'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ free -m
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + TOTAL_MEM=7483
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ python -c 'print int(7483 *     0.4 / 2)'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + NAMENODE_MEM_MB=1496
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SECONDARYNAMENODE_MEM_MB=1496
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + export HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ 1 -gt 1 ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + TEMPLATE=hdfs-template.xml
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file hdfs-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + ((  2 == 0  ))
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ get_java_property /tmp/cluster/properties/dataproc.properties simplified.scaling.enable
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ local property_file=/tmp/cluster/properties/dataproc.properties
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ local property_name=simplified.scaling.enable
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ cut -d = -f 2-
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ tail -n 1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ grep '^simplified.scaling.enable=' /tmp/cluster/properties/dataproc.properties
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ local property_value=
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ echo ''
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ '' == \t\r\u\e ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + set -e
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + loginfo 'Running configure_connectors.sh'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + echo 'Running configure_connectors.sh'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: Running configure_connectors.sh
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + ((  1  ))
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + export GCS_METADATA_CACHE_TYPE=FILESYSTEM_BACKED
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + GCS_METADATA_CACHE_TYPE=FILESYSTEM_BACKED
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ get_nfs_mount_point
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ echo /hadoop_gcs_connector_metadata_cache
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + export GCS_FILE_CACHE_DIRECTORY=/hadoop_gcs_connector_metadata_cache
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + GCS_FILE_CACHE_DIRECTORY=/hadoop_gcs_connector_metadata_cache
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file gcs-core-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ hostname -s
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ cluster-ea0a-m == \c\l\u\s\t\e\r\-\e\a\0\a\-\m ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ 1 -ne 0 ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + setup_cache_cleaner
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + mkdir -p /usr/lib/hadoop/google
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local cleaner=/usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + make_cache_cleaner_script
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local gc_cleaner=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemCacheCleaner
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local etab=/var/lib/nfs/etab
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + chmod 755 /usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + make_cleaner_crontab /usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local cleaner=/usr/lib/hadoop/google/clean-caches.sh
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file bq-mapred-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + set -e
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + set -o nounset
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + loginfo 'Running configure_spark.sh'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + echo 'Running configure_spark.sh'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: Running configure_spark.sh
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_EVENTLOG_DIR=hdfs://cluster-ea0a-m/user/spark/eventlog
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_TMPDIR=/hadoop/spark/tmp
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_WORKDIR=/hadoop/spark/work
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_LOG_DIR=/var/log/spark
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + mkdir -p /hadoop/spark/tmp /hadoop/spark/work /var/log/spark
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + chown spark:spark -R /hadoop/spark /var/log/spark
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + chmod 1777 -R /hadoop/spark /var/log/spark
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + is_version_at_least 2.3.3 2
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver1=2.3.3.0.0.0.0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver2=2
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local log=/tmp/tmp.Ww7Kfe7sg3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + dpkg --compare-versions 2.3.3.0.0.0.0 '>=' 2
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + err_code=0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.Ww7Kfe7sg3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + rm -f /tmp/tmp.Ww7Kfe7sg3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + return 0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + RPC_SIZE_KEY=spark.rpc.message.maxSize
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + NUM_INITIAL_EXECUTORS_KEY=spark.executor.instances
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_YARN_DIR=/usr/lib/spark/yarn
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARKR_LIB_DIR=/usr/lib/spark/R/lib
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ -f /usr/lib/spark/R/lib/sparkr.zip ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ free -m
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ awk '/^Mem:/{print $2}'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + TOTAL_MEM=7483
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ python -c 'print int(7483 * 0.15)'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_DAEMON_MEMORY=1122
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ python -c 'print int(7483 / 4)'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_DRIVER_MEM_MB=1870
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ python -c 'print int(1870 / 2)'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_DRIVER_MAX_RESULT_MB=935
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ head -1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ find /tmp/mrv2_ptf_tmp_env.sh
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + YARN_MEMORY_ENV=/tmp/mrv2_ptf_tmp_env.sh
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + source /tmp/mrv2_ptf_tmp_env.sh
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export YARN_MIN_MEM_MB=512
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ YARN_MIN_MEM_MB=512
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export YARN_MAX_MEM_MB=5632
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ YARN_MAX_MEM_MB=5632
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export NODEMANAGER_MEM_MB=5632
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ NODEMANAGER_MEM_MB=5632
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export APP_MASTER_MEM_MB=5632
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ APP_MASTER_MEM_MB=5632
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export MAP_MEM_MB=2560
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ MAP_MEM_MB=2560
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ CORES_PER_MAP_ROUNDED=1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export REDUCE_MEM_MB=5632
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ REDUCE_MEM_MB=5632
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export CORES_PER_REDUCE_ROUNDED=2
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ CORES_PER_REDUCE_ROUNDED=2
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ export REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ python
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ cat
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_EXECUTOR_MEMORY=2560
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ python -c 'print max(1,     2 / 2)'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_EXECUTOR_CORES=1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ python -c 'print int(max(     2560 / 11, 384))'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_YARN_EXECUTOR_MEMORY_OVERHEAD=384
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + SPARK_EXECUTOR_MEMORY=2176
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + is_version_at_least 1.3 1.4
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver1=1.3.0.0.0.0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver2=1.4
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local log=/tmp/tmp.8qOc8tW6yd
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + err_code=1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.8qOc8tW6yd
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + rm -f /tmp/tmp.8qOc8tW6yd
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + is_version_at_least 1.3 1.3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver1=1.3.0.0.0.0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver2=1.3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local log=/tmp/tmp.jzu8mOxJe1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + err_code=0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.jzu8mOxJe1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + rm -f /tmp/tmp.jzu8mOxJe1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + return 0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + set -e
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + loginfo 'Running configure_tez.sh'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + echo 'Running configure_tez.sh'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: Running configure_tez.sh
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + readonly CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + is_version_at_least 1.3 1.3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver1=1.3.0.0.0.0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local ver2=1.3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local log=/tmp/tmp.VE6aiq4f97
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + err_code=0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.VE6aiq4f97
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + rm -f /tmp/tmp.VE6aiq4f97
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + return 0
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ ls /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + configure_war /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local -r tez_war=/usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ mktemp -d
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local -r tmp_dir=/tmp/tmp.OEpDm4AP6p
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + unzip -q /usr/lib/tez/tez-ui-0.9.2.war -d /tmp/tmp.OEpDm4AP6p
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local -r tez_configs=/tmp/tmp.OEpDm4AP6p/config/configs.env
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ cut -d ' ' -f 1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ md5sum /tmp/tmp.OEpDm4AP6p/config/configs.env
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ 23fbfca8f7b8e142395c6bb4676427ae != \2\3\f\b\f\c\a\8\f\7\b\8\e\1\4\2\3\9\5\c\6\b\b\4\6\7\6\4\2\7\a\e ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ -f /tmp/tmp.OEpDm4AP6p/config/configs.env ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + sed -i 's#\(.*\)//timeline: "http://localhost:8188"\(.*\)#\1timeline: "http://cluster-ea0a-m:8188"\2#' /tmp/tmp.OEpDm4AP6p/config/configs.env
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + sed -i 's#\(.*\)//rm: "http://localhost:8088"\(.*\)#\1rm: "http://cluster-ea0a-m:8088"\2#' /tmp/tmp.OEpDm4AP6p/config/configs.env
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ local property_name=dataproc.components.activate
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ local property_name=dataproc.components.activate
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ local property_value=
<13>Oct 13 16:53:27 google-dataproc-startup[837]: +++ echo ''
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ local property_value=
<13>Oct 13 16:53:27 google-dataproc-startup[837]: ++ echo ''
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + local -r optional_components_value=
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + [[ '' == *\k\n\o\x* ]]
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + cd /tmp/tmp.OEpDm4AP6p
<13>Oct 13 16:53:27 google-dataproc-startup[837]: + zip -q /usr/lib/tez/tez-ui-0.9.2.war -r ./assets ./config ./fonts ./index.html ./META-INF ./WEB-INF
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + cd ..
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + rm -rf /tmp/tmp.OEpDm4AP6p
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++ stat /usr/lib/tez/tez-common-0.9.2.jar --format=%Y
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + touch -d @1568819629 /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-names --value tez --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-on-disk-path.tez --value /usr/lib/tez/tez-ui-0.9.2.war --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-web-path.tez --value /tez-ui --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.history.logging.service.class --value org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.tez-ui.history-url.base --value http://cluster-ea0a-m:8188/tez-ui/ --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.am.node-blacklisting.enabled --value false --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + set -e
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + declare -r ZOOKEEPER_CONFIG=/etc/zookeeper/conf/zoo.cfg
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + declare -r ZOOKEEPER_DATA_DIR=/var/lib/zookeeper/
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + (( i=0 ))
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + (( i<1 ))
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo server.0=cluster-ea0a-m:2888:3888
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + (( i++  ))
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + (( i<1 ))
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo autopurge.purgeInterval=168
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++ sed -e 's/.*-m-//'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++ uname -n
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + declare -r MY_ID=cluster-ea0a-m
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo cluster-ea0a-m
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ false == \t\r\u\e ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + loginfo 'Populating initial cluster member list'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo 'Populating initial cluster member list'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: Populating initial cluster member list
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++ get_dataproc_property dataproc.worker.custom.init.actions.mode
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++ local property_name=dataproc.worker.custom.init.actions.mode
<13>Oct 13 16:53:28 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.worker.custom.init.actions.mode
<13>Oct 13 16:53:28 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:28 google-dataproc-startup[837]: +++ local property_name=dataproc.worker.custom.init.actions.mode
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++++ grep '^dataproc.worker.custom.init.actions.mode=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:28 google-dataproc-startup[837]: +++ local property_value=
<13>Oct 13 16:53:28 google-dataproc-startup[837]: +++ echo ''
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++ local property_value=
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++ echo ''
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + WORKER_CUSTOM_INIT_ACTIONS_MODE=
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + WORKER_COUNT=2
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++ get_dataproc_property simplified.scaling.enable
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++ local property_name=simplified.scaling.enable
<13>Oct 13 16:53:28 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Oct 13 16:53:28 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:28 google-dataproc-startup[837]: +++ local property_name=simplified.scaling.enable
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:28 google-dataproc-startup[837]: +++ local property_value=
<13>Oct 13 16:53:28 google-dataproc-startup[837]: +++ echo ''
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++ local property_value=
<13>Oct 13 16:53:28 google-dataproc-startup[837]: ++ echo ''
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ '' != \t\r\u\e ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + ((  2 == 0  ))
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ '' == \R\U\N\_\B\E\F\O\R\E\_\S\E\R\V\I\C\E\S ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + MEMBERSHIP_FILE=/etc/hadoop/conf/nodes_include
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + (( i=0 ))
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + (( i<2 ))
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo cluster-ea0a-w-0.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + (( i++  ))
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + (( i<2 ))
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo cluster-ea0a-w-1.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + (( i++  ))
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + (( i<2 ))
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + loginfo 'Merging user-specified cluster properties'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo 'Merging user-specified cluster properties'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: Merging user-specified cluster properties
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + merge_xml_properties /tmp/cluster/properties/capacity-scheduler.xml /etc/hadoop/conf/capacity-scheduler.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/capacity-scheduler.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local dest=/etc/hadoop/conf/capacity-scheduler.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/capacity-scheduler.xml ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file /tmp/cluster/properties/capacity-scheduler.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: Merged /tmp/cluster/properties/capacity-scheduler.xml.
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + merge_xml_properties /tmp/cluster/properties/core.xml /etc/hadoop/conf/core-site.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/core.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local dest=/etc/hadoop/conf/core-site.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/core.xml ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file /tmp/cluster/properties/core.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/core.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/core.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: Merged /tmp/cluster/properties/core.xml.
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + merge_xml_properties /tmp/cluster/properties/distcp.xml /etc/hadoop/conf/distcp-default.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/distcp.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local dest=/etc/hadoop/conf/distcp-default.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/distcp.xml ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file /tmp/cluster/properties/distcp.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: Merged /tmp/cluster/properties/distcp.xml.
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + merge_xml_properties /tmp/cluster/properties/hdfs.xml /etc/hadoop/conf/hdfs-site.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/hdfs.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local dest=/etc/hadoop/conf/hdfs-site.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/hdfs.xml ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /tmp/cluster/properties/hdfs.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: Merged /tmp/cluster/properties/hdfs.xml.
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + merge_xml_properties /tmp/cluster/properties/mapred.xml /etc/hadoop/conf/mapred-site.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/mapred.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local dest=/etc/hadoop/conf/mapred-site.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/mapred.xml ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file /tmp/cluster/properties/mapred.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: Merged /tmp/cluster/properties/mapred.xml.
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + merge_xml_properties /tmp/cluster/properties/yarn.xml /etc/hadoop/conf/yarn-site.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/yarn.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local dest=/etc/hadoop/conf/yarn-site.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/yarn.xml ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /tmp/cluster/properties/yarn.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Oct 13 16:53:28 google-dataproc-startup[837]: Merged /tmp/cluster/properties/yarn.xml.
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + merge_xml_properties /tmp/cluster/properties/hive.xml /etc/hive/conf/hive-site.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/hive.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + local dest=/etc/hive/conf/hive-site.xml
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/hive.xml ]]
<13>Oct 13 16:53:28 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/hive/conf/hive-site.xml --source_configuration_file /tmp/cluster/properties/hive.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Merged /tmp/cluster/properties/hive.xml.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + merge_java_properties /tmp/cluster/properties/pig.properties /etc/pig/conf/pig.properties
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/pig.properties
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local dest=/etc/pig/conf/pig.properties
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/pig.properties ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + cat /tmp/cluster/properties/pig.properties
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Merged /tmp/cluster/properties/pig.properties.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + merge_java_properties /tmp/cluster/properties/spark.properties /etc/spark/conf/spark-defaults.conf
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/spark.properties
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local dest=/etc/spark/conf/spark-defaults.conf
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/spark.properties ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + cat /tmp/cluster/properties/spark.properties
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Merged /tmp/cluster/properties/spark.properties.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + merge_java_properties /tmp/cluster/properties/zookeeper.properties /etc/zookeeper/conf/zoo.cfg
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/zookeeper.properties
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local dest=/etc/zookeeper/conf/zoo.cfg
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/zookeeper.properties ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + cat /tmp/cluster/properties/zookeeper.properties
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Merged /tmp/cluster/properties/zookeeper.properties.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + merge_sh_env_vars /tmp/cluster/properties/hadoop-env.sh /etc/hadoop/conf/hadoop-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/hadoop-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local dest=/etc/hadoop/conf/hadoop-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/hadoop-env.sh ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + cat /tmp/cluster/properties/hadoop-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Merged /tmp/cluster/properties/hadoop-env.sh.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + merge_sh_env_vars /tmp/cluster/properties/mapred-env.sh /etc/hadoop/conf/mapred-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/mapred-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local dest=/etc/hadoop/conf/mapred-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/mapred-env.sh ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + cat /tmp/cluster/properties/mapred-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Merged /tmp/cluster/properties/mapred-env.sh.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + merge_sh_env_vars /tmp/cluster/properties/spark-env.sh /etc/spark/conf/spark-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/spark-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local dest=/etc/spark/conf/spark-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/spark-env.sh ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + cat /tmp/cluster/properties/spark-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Merged /tmp/cluster/properties/spark-env.sh.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + merge_sh_env_vars /tmp/cluster/properties/yarn-env.sh /etc/hadoop/conf/yarn-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/yarn-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local dest=/etc/hadoop/conf/yarn-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/yarn-env.sh ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo -e '\n# User-supplied properties.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + cat /tmp/cluster/properties/yarn-env.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Merged /tmp/cluster/properties/yarn-env.sh.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ -f /etc/hbase/conf/hbase-site.xml ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ -f /etc/tez/conf/tez-site.xml ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + merge_xml_properties /tmp/cluster/properties/tez.xml /etc/tez/conf/tez-site.xml
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local src=/tmp/cluster/properties/tez.xml
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local dest=/etc/tez/conf/tez-site.xml
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ ! -f /tmp/cluster/properties/tez.xml ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + bdconfig merge_configurations --configuration_file /etc/tez/conf/tez-site.xml --source_configuration_file /tmp/cluster/properties/tez.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Merged /tmp/cluster/properties/tez.xml.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ false == \t\r\u\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + ACTIVATABLE_COMPONENTS=(${DATAPROC_OPTIONAL_COMPONENTS})
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + DATAPROC_NON_DEBIAN_COMPONENTS=(${DATAPROC_NON_DEBIAN_COMPONENTS})
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PACKAGES_TO_KEEP=("${SERVICES[@]}" ${DATAPROC_COMMON_PACKAGES})
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + DATAPROC_START_AFTER_HDFS_SERVICES=(${DATAPROC_START_AFTER_HDFS_SERVICES})
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + COMPONENTS_TO_ACTIVATE=($(intersection COMPONENTS_TO_ACTIVATE PACKAGES_TO_KEEP))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ intersection COMPONENTS_TO_ACTIVATE PACKAGES_TO_KEEP
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local -n values=COMPONENTS_TO_ACTIVATE
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local -n filter=PACKAGES_TO_KEEP
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ sort -u
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ printf '%s\n' hadoop-hdfs-namenode hadoop-yarn-resourcemanager hive-metastore hive-server2 zookeeper-server solr-server hadoop-mapreduce-historyserver spark-history-server hive-webhcat-server jupyter knox proxy-agent zeppelin hadoop-yarn-timelineserver mariadb-server hadoop-hdfs-secondarynamenode openjdk-8-jdk openjdk-8-dbg libjansi-java python-numpy libmysql-java hadoop-client hive pig spark-core spark-python spark-r autofs libhdfs0 libsnappy-dev libzstd-dev libatlas3-base libopenblas-base libapr1 vim git bash-completion spark-yarn-shuffle spark-datanucleus spark-extras hadoop-lzo python-setuptools anaconda druid kafka-server kerberos presto openssl tez hive-hcatalog
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ printf '%s\n'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ sort -u
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + NON_ACTIVATED_COMPONENTS=($(difference ACTIVATABLE_COMPONENTS COMPONENTS_TO_ACTIVATE))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ difference ACTIVATABLE_COMPONENTS COMPONENTS_TO_ACTIVATE
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local -n filter=COMPONENTS_TO_ACTIVATE
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ printf '%s\n'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ sort -u
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ printf '%s\n' zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ sort -u
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PACKAGES_TO_UNINSTALL+=($(difference NON_ACTIVATED_COMPONENTS PACKAGES_TO_UNINSTALL))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ difference NON_ACTIVATED_COMPONENTS PACKAGES_TO_UNINSTALL
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local -n values=NON_ACTIVATED_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local -n filter=PACKAGES_TO_UNINSTALL
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ sort -u
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ printf '%s\n' anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ sort -u
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + NON_DEBIAN_COMPONENTS_TO_UNINSTALL=($(intersection PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ intersection PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local -n values=PACKAGES_TO_UNINSTALL
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local -n filter=DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ sort -u
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ printf '%s\n' anaconda jupyter kerberos presto proxy-agent
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ sort -u
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + DEBIAN_COMPONENTS_TO_UNINSTALL=($(difference PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ difference PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local -n values=PACKAGES_TO_UNINSTALL
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local -n filter=DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ sort -u
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ printf '%s\n' anaconda jupyter kerberos presto proxy-agent
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ sort -u
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ false != \t\r\u\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + DEBIAN_COMPONENTS_TO_UNINSTALL+=('krb5-kpropd' 'krb5-kdc' 'krb5-admin-server' 'krb5-user' 'krb5-config' 'xinetd')
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + uninstall_packages
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_retries set_selections
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local retry_backoff
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + cmd=("$@")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -a cmd
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'About to run '\''set_selections'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'About to run '\''set_selections'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: About to run 'set_selections' with retries...
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local update_succeeded=0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + (( i = 0 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + (( i < 12 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + set_selections
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + cat
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + debconf-set-selections
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + update_succeeded=1
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + break
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + ((  1  ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Uninstalling un-needed daemons'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Uninstalling un-needed daemons'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Uninstalling un-needed daemons
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag uninstall bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1431
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true] as pid 1431'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true] as pid 1431
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag uninstall-component-anaconda uninstall_component anaconda
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag uninstall bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1432
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1431
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=uninstall
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [uninstall_component anaconda] as pid 1432'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [uninstall_component anaconda] as pid 1432
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag uninstall-component-jupyter uninstall_component jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1433
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [uninstall_component jupyter] as pid 1433'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [uninstall_component jupyter] as pid 1433
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag uninstall-component-kerberos uninstall_component kerberos
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1435
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [uninstall_component kerberos] as pid 1435'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [uninstall_component kerberos] as pid 1435
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag uninstall-component-presto uninstall_component presto
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1437
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [uninstall_component presto] as pid 1437'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [uninstall_component presto] as pid 1437
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag uninstall-component-proxy-agent uninstall_component proxy-agent
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1438
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [uninstall_component proxy-agent] as pid 1438'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [uninstall_component proxy-agent] as pid 1438
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + is_version_at_least 1.3 1.3
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local ver1=1.3.0.0.0.0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local ver2=1.3
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'uninstall[1431]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag uninstall-component-presto uninstall_component presto
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1437
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=uninstall-component-presto
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ mktemp
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'uninstall-component-presto[1437]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall[1431]: + bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag uninstall-component-proxy-agent uninstall_component proxy-agent
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1438
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=uninstall-component-proxy-agent
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local log=/tmp/tmp.KCOe9jDPGt
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag uninstall-component-kerberos uninstall_component kerberos
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1435
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=uninstall-component-kerberos
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'uninstall-component-proxy-agent[1438]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'uninstall-component-kerberos[1435]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + uninstall_component presto
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + local component=presto
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + err_code=0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + grep -C 10 -i warning /tmp/tmp.KCOe9jDPGt
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag uninstall-component-jupyter uninstall_component jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1433
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=uninstall-component-jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: + uninstall_component proxy-agent
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: + local component=proxy-agent
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: + set -exo pipefail
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag uninstall-component-anaconda uninstall_component anaconda
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1432
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=uninstall-component-anaconda
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + set -exo pipefail
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-kerberos[1435]: + uninstall_component kerberos
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-kerberos[1435]: + local component=kerberos
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-kerberos[1435]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'uninstall-component-jupyter[1433]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-kerberos[1435]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-kerberos[1435]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-kerberos[1435]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-kerberos[1435]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-kerberos[1435]: + set -euxo pipefail
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/proxy-agent.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + rm -f /tmp/tmp.KCOe9jDPGt
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'uninstall-component-anaconda[1432]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: ++ readonly PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: ++ PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: ++ readonly PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: ++ PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: ++ readonly PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: ++ PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: + rm -f /usr/bin/proxy-forwarding-agent
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/presto.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ readonly HTTP_PORT=8060
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ HTTP_PORT=8060
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ readonly PRESTO_BIN_PATH=/usr/bin/presto
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ PRESTO_BIN_PATH=/usr/bin/presto
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ readonly PRESTO_CLI_PATH=/opt/presto-cli
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ PRESTO_CLI_PATH=/opt/presto-cli
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ readonly PRESTO_DATA_DIRECTORY=/var/presto/data
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ PRESTO_DATA_DIRECTORY=/var/presto/data
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ readonly PRESTO_INSTALL_DIRECTORY=/opt/presto-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ PRESTO_INSTALL_DIRECTORY=/opt/presto-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ readonly INIT_SCRIPT=/usr/lib/systemd/system/presto.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ INIT_SCRIPT=/usr/lib/systemd/system/presto.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ readonly PRESTO_VERSION=0.215
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: ++ PRESTO_VERSION=0.215
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + rm -Rf /opt/presto-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + return 0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: + uninstall_component jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: + local component=jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: + uninstall_component anaconda
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: + local component=anaconda
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: + set -exo pipefail
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: + set -euxo pipefail
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/jupyter.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ export WHEEL_DIR=/opt/dataproc/jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ WHEEL_DIR=/opt/dataproc/jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ export JGSCM_WHEEL=/opt/dataproc/jupyter/jgscm
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ JGSCM_WHEEL=/opt/dataproc/jupyter/jgscm
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ export JGSCM_REQUIREMENTS=/opt/dataproc/jupyter/jgscm.requirements
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ JGSCM_REQUIREMENTS=/opt/dataproc/jupyter/jgscm.requirements
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ export JUPYTER_ETC_DIR=/etc/jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ JUPYTER_ETC_DIR=/etc/jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ export JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ export JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ export PYSPARK_KERNELSPEC=/opt/conda/anaconda/share/jupyter/kernels/pyspark/kernel.json
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ PYSPARK_KERNELSPEC=/opt/conda/anaconda/share/jupyter/kernels/pyspark/kernel.json
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ export JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ export JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: ++ JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: + rm -Rf /etc/jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-proxy-agent[1438]: + rm -f /usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ get_dataproc_property dataproc.monitoring.stackdriver.enable
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local property_name=dataproc.monitoring.stackdriver.enable
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/anaconda.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: ++ export ANACONDA_INSTALL_PATH=/opt/conda/anaconda
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: ++ ANACONDA_INSTALL_PATH=/opt/conda/anaconda
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: ++ export ANACONDA_BIN_DIR=/opt/conda/anaconda/bin
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: ++ ANACONDA_BIN_DIR=/opt/conda/anaconda/bin
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: ++ export PATH=/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: ++ PATH=/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-anaconda[1432]: + rm -Rf /opt/conda/anaconda
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.monitoring.stackdriver.enable
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ local property_name=dataproc.monitoring.stackdriver.enable
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-jupyter[1433]: + rm -Rf /opt/dataproc/jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++++ grep '^dataproc.monitoring.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ local property_value=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: +++ echo false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local property_value=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ echo false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + STACKDRIVER_MONITORING_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ false == \t\r\u\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Stackdriver monitoring disabled.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Stackdriver monitoring disabled.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Stackdriver monitoring disabled.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Replace dataproc plugin instance_name label with gce instance name.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Replace dataproc plugin instance_name label with gce instance name.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Replace dataproc plugin instance_name label with gce instance name.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for PLUGIN_FILE in /opt/stackdriver/collectd/etc/collectd.d/dataproc*
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ hostname
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + sed -i 's/"label:instance_name".*$/"label:instance_name" "cluster-ea0a-m"/g' /opt/stackdriver/collectd/etc/collectd.d/dataproc_collectd_default-20170324-133642.conf
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + chmod +x /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Running verify_setup.sh'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Running verify_setup.sh'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Running verify_setup.sh
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + rm -Rf /var/presto/data
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.warehouse.dir
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + rm -f /usr/bin/presto
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + rm -f /opt/presto-cli
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 uninstall-component-presto[1437]: + rm -f /usr/lib/systemd/system/presto.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + hive_warehouse_dir=None
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ None == \g\s\:\/\/* ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Starting services'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Starting services'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Starting services
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array hadoop-hdfs-namenode ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=hadoop-hdfs-namenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-hdfs-namenode  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + case "${SERVICE}" in
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag setup-hadoop-hdfs-namenode setup_service hadoop-hdfs-namenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1493
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [setup_service hadoop-hdfs-namenode] as pid 1493'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [setup_service hadoop-hdfs-namenode] as pid 1493
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array hadoop-yarn-resourcemanager ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=hadoop-yarn-resourcemanager
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-yarn-resourcemanager  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + case "${SERVICE}" in
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1494
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [setup_service hadoop-yarn-resourcemanager] as pid 1494'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [setup_service hadoop-yarn-resourcemanager] as pid 1494
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array hive-metastore ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=hive-metastore
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-metastore  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + case "${SERVICE}" in
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag setup-hive-metastore setup_service hive-metastore
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1495
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [setup_service hive-metastore] as pid 1495'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [setup_service hive-metastore] as pid 1495
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array hive-server2 ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=hive-server2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-server2  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + case "${SERVICE}" in
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag setup-hive-server2 setup_service hive-server2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1496
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [setup_service hive-server2] as pid 1496'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [setup_service hive-server2] as pid 1496
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array zookeeper-server ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=zookeeper-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  zookeeper-server  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + continue
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array solr-server ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=solr-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  solr-server  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + continue
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array hadoop-mapreduce-historyserver ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=hadoop-mapreduce-historyserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-mapreduce-historyserver  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + case "${SERVICE}" in
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1497
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [setup_service hadoop-mapreduce-historyserver] as pid 1497'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [setup_service hadoop-mapreduce-historyserver] as pid 1497
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array spark-history-server ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=spark-history-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  spark-history-server  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + case "${SERVICE}" in
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag setup-spark-history-server setup_service spark-history-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1498
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [setup_service spark-history-server] as pid 1498'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [setup_service spark-history-server] as pid 1498
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array hive-webhcat-server ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=hive-webhcat-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-webhcat-server  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + continue
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array jupyter ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=jupyter
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  jupyter  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + continue
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array knox ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=knox
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  knox  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + continue
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array proxy-agent ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=proxy-agent
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  proxy-agent  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + continue
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array zeppelin ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=zeppelin
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  zeppelin  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + continue
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array hadoop-yarn-timelineserver ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=hadoop-yarn-timelineserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-yarn-timelineserver  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + case "${SERVICE}" in
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1494
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=setup-hadoop-yarn-resourcemanager
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag setup-hadoop-hdfs-namenode setup_service hadoop-hdfs-namenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1499
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag setup-spark-history-server setup_service spark-history-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1493
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=setup-hadoop-hdfs-namenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [setup_service hadoop-yarn-timelineserver] as pid 1499'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1498
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [setup_service hadoop-yarn-timelineserver] as pid 1499
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=setup-spark-history-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag setup-hive-server2 setup_service hive-server2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag setup-hive-metastore setup_service hive-metastore
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1496
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=setup-hive-server2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array mariadb-server ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=mariadb-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  mariadb-server  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + case "${SERVICE}" in
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag setup-mariadb setup_service mariadb
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1507
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [setup_service mariadb] as pid 1507'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [setup_service mariadb] as pid 1507
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1499
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=setup-hadoop-yarn-timelineserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + in_array hadoop-hdfs-secondarynamenode ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local value=hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-hdfs-secondarynamenode  ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + return 1
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + case "${SERVICE}" in
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_in_background --tag setup-hadoop-hdfs-secondarynamenode setup_service hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + PID=1509
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'setup-hadoop-yarn-resourcemanager[1494]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Started background process [setup_service hadoop-hdfs-secondarynamenode] as pid 1509'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1497
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=setup-hadoop-mapreduce-historyserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'setup-hadoop-hdfs-namenode[1493]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1495
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=setup-hive-metastore
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'setup-hive-server2[1496]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'setup-hadoop-yarn-timelineserver[1499]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + setup_service hadoop-yarn-resourcemanager
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + export KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + export -f login_through_keytab_if_necessary
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + export MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + local service=hadoop-yarn-resourcemanager
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + enable_service hadoop-yarn-resourcemanager
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + local service=hadoop-yarn-resourcemanager
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + local unit=hadoop-yarn-resourcemanager.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + run_with_retries systemctl enable hadoop-yarn-resourcemanager.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + local retry_backoff
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + cmd=("$@")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + local -a cmd
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + loginfo 'About to run '\''systemctl enable hadoop-yarn-resourcemanager.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + echo 'About to run '\''systemctl enable hadoop-yarn-resourcemanager.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: About to run 'systemctl enable hadoop-yarn-resourcemanager.service' with retries...
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + local update_succeeded=0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + (( i = 0 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + (( i < 12 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: + systemctl enable hadoop-yarn-resourcemanager.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + setup_service hadoop-hdfs-namenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + export KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + export -f login_through_keytab_if_necessary
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + export MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + local service=hadoop-hdfs-namenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + case "${MASTER_INDEX?}" in
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + loginfo 'Formatting NameNode'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + echo 'Formatting NameNode'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: Formatting NameNode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + run_with_retries su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + local retry_backoff
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + cmd=("$@")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + local -a cmd
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + loginfo 'About to run '\''su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + echo 'About to run '\''su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: About to run 'su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive' with retries...
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + local update_succeeded=0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + (( i = 0 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + (( i < 12 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-namenode[1493]: + su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag setup-mariadb setup_service mariadb
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1507
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=setup-mariadb
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + run_with_logger --tag setup-hadoop-hdfs-secondarynamenode setup_service hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + local pid=1509
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + tag=setup-hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'setup-hadoop-mapreduce-historyserver[1497]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'setup-spark-history-server[1498]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'setup-hive-metastore[1495]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + setup_service hive-server2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + export KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + export -f login_through_keytab_if_necessary
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + export MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + local service=hive-server2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + [[ hive-server2 == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + enable_service hive-server2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + local service=hive-server2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + local unit=hive-server2.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + run_with_retries systemctl enable hive-server2.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + local retry_backoff
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + cmd=("$@")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + local -a cmd
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + loginfo 'About to run '\''systemctl enable hive-server2.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + echo 'About to run '\''systemctl enable hive-server2.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: About to run 'systemctl enable hive-server2.service' with retries...
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + local update_succeeded=0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + (( i = 0 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + (( i < 12 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: + systemctl enable hive-server2.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: hive-server2.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-server2[1496]: Executing: /lib/systemd/systemd-sysv-install enable hive-server2
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: hadoop-yarn-resourcemanager.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-resourcemanager[1494]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-resourcemanager
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Started background process [setup_service hadoop-hdfs-secondarynamenode] as pid 1509
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + loginfo 'Configuring optional components'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: + echo 'Configuring optional components'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: Configuring optional components
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + setup_service hadoop-yarn-timelineserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + export KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + export -f login_through_keytab_if_necessary
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + export MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + local service=hadoop-yarn-timelineserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + enable_service hadoop-yarn-timelineserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + local service=hadoop-yarn-timelineserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'setup-mariadb[1507]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + setup_service hadoop-mapreduce-historyserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + export KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + export -f login_through_keytab_if_necessary
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + export MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + local service=hadoop-mapreduce-historyserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + [[ hadoop-mapreduce-historyserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + enable_service hadoop-mapreduce-historyserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + local service=hadoop-mapreduce-historyserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + local unit=hadoop-mapreduce-historyserver.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + run_with_retries systemctl enable hadoop-mapreduce-historyserver.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + local retry_backoff
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + cmd=("$@")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + local -a cmd
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + loginfo 'About to run '\''systemctl enable hadoop-mapreduce-historyserver.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + echo 'About to run '\''systemctl enable hadoop-mapreduce-historyserver.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: About to run 'systemctl enable hadoop-mapreduce-historyserver.service' with retries...
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + local update_succeeded=0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + (( i = 0 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + (( i < 12 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: + systemctl enable hadoop-mapreduce-historyserver.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: hadoop-mapreduce-historyserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-mapreduce-historyserver[1497]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-mapreduce-historyserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ logger -s -t 'setup-hadoop-hdfs-secondarynamenode[1509]'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + setup_service spark-history-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + export KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + export -f login_through_keytab_if_necessary
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + export MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + local service=spark-history-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + [[ spark-history-server == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + enable_service spark-history-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + local service=spark-history-server
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + local unit=spark-history-server.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + run_with_retries systemctl enable spark-history-server.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + local retry_backoff
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + cmd=("$@")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + local -a cmd
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + loginfo 'About to run '\''systemctl enable spark-history-server.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + echo 'About to run '\''systemctl enable spark-history-server.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: About to run 'systemctl enable spark-history-server.service' with retries...
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + local update_succeeded=0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + (( i = 0 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + (( i < 12 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: + systemctl enable spark-history-server.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + local unit=hadoop-yarn-timelineserver.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + run_with_retries systemctl enable hadoop-yarn-timelineserver.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + setup_service hive-metastore
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + local retry_backoff
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + export KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + cmd=("$@")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + local -a cmd
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + loginfo 'About to run '\''systemctl enable hadoop-yarn-timelineserver.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + echo 'About to run '\''systemctl enable hadoop-yarn-timelineserver.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + export -f login_through_keytab_if_necessary
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: About to run 'systemctl enable hadoop-yarn-timelineserver.service' with retries...
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + export MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + local update_succeeded=0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + (( i = 0 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + local service=hive-metastore
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + (( i < 12 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: + systemctl enable hadoop-yarn-timelineserver.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + [[ hive-metastore == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: hadoop-yarn-timelineserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + wait_for_port cluster-ea0a-m 3306
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-yarn-timelineserver[1499]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-timelineserver
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + local -r host=cluster-ea0a-m
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + local -r port=3306
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + loginfo 'Waiting for service to come up on host=cluster-ea0a-m port=3306.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + echo 'Waiting for service to come up on host=cluster-ea0a-m port=3306.'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: Waiting for service to come up on host=cluster-ea0a-m port=3306.
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + retry_with_constant_backoff nc -v -z -w 0 cluster-ea0a-m 3306
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + local max_retry=300
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + cmd=("$@")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + local -a cmd
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + local update_succeeded=0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: ++ seq 1 300
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 3306
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + setup_service mariadb
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + export KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + export -f login_through_keytab_if_necessary
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + export MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + local service=mariadb
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + [[ mariadb == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + enable_service mariadb
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + local service=mariadb
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + local unit=mariadb.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + run_with_retries systemctl enable mariadb.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + local retry_backoff
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + cmd=("$@")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + local -a cmd
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + loginfo 'About to run '\''systemctl enable mariadb.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + echo 'About to run '\''systemctl enable mariadb.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: About to run 'systemctl enable mariadb.service' with retries...
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + local update_succeeded=0
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + (( i = 0 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + (( i < 12 ))
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + systemctl enable mariadb.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ get_dataproc_property dataproc.conscrypt.provider.enable
<13>Oct 13 16:53:29 google-dataproc-startup[837]: ++ local property_name=dataproc.conscrypt.provider.enable
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + setup_service hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + export KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + KERBEROS_ENABLED=false
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + export -f login_through_keytab_if_necessary
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + export MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + local service=hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + enable_service hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + local service=hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + local unit=hadoop-hdfs-secondarynamenode.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + run_with_retries systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + local retry_backoff
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + cmd=("$@")
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + local -a cmd
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + loginfo 'About to run '\''systemctl enable hadoop-hdfs-secondarynamenode.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + echo 'About to run '\''systemctl enable hadoop-hdfs-secondarynamenode.service'\'' with retries...'
<13>Oct 13 16:53:29 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: About to run 'systemctl enable hadoop-hdfs-secondarynamenode.service' with retries...
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + local update_succeeded=0
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + (( i = 0 ))
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + (( i < 12 ))
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: + systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: hadoop-hdfs-secondarynamenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hadoop-hdfs-secondarynamenode[1509]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: spark-history-server.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-spark-history-server[1498]: Executing: /lib/systemd/systemd-sysv-install enable spark-history-server
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: Created symlink /etc/systemd/system/mysql.service → /lib/systemd/system/mariadb.service.
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: Created symlink /etc/systemd/system/mysqld.service → /lib/systemd/system/mariadb.service.
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: Created symlink /etc/systemd/system/multi-user.target.wants/mariadb.service → /lib/systemd/system/mariadb.service.
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.conscrypt.provider.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ local property_name=dataproc.conscrypt.provider.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 1.'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 1.'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 1.
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ grep '^dataproc.conscrypt.provider.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ local property_value=true
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ echo true
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ local property_value=true
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ echo true
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + CONSCRYPT_ENABLED=true
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + [[ true == \t\r\u\e ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt.jar /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/libconscrypt.jar
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni.so /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/libconscrypt_openjdk_jni.so
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + cp /usr/local/share/google/dataproc/java.security.conscrypt /etc/java-8-openjdk/security/java.security
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ get_dataproc_property dataproc.logging.stackdriver.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ local property_name=dataproc.logging.stackdriver.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ local property_name=dataproc.logging.stackdriver.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + update_succeeded=1
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + break
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: + ((  1  ))
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:29 setup-mariadb[1507]: ++ systemctl show mariadb.service -p Restart,RemainAfterExit
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ grep '^dataproc.logging.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ tail -n 1
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ local property_value=
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ echo ''
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ local property_value=
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ echo ''
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + STACKDRIVER_LOGGING_ENABLED=
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + [[ '' == \f\a\l\s\e ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + loginfo 'Stackdriver enabled; enabling google-fluentd.'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + echo 'Stackdriver enabled; enabling google-fluentd.'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: Stackdriver enabled; enabling google-fluentd.
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + source /usr/local/share/google/dataproc/bdutil/configure_fluentd.sh
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ set -e
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ set -u
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ loginfo 'Running configure_fluentd.sh'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ echo 'Running configure_fluentd.sh'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: Running configure_fluentd.sh
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ FLUENTD_BASE_DIR=/etc/google-fluentd
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ FLUENTD_CONF_DIR=/etc/google-fluentd/config.d
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ FLUENTD_PLUGIN_DIR=/etc/google-fluentd/plugin
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ cp /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/monitoring.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/out_google_cloud.rb /etc/google-fluentd/plugin
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ get_dataproc_property dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ local property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ local property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++++ cut -d = -f 2-
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++++ grep '^dataproc.logging.stackdriver.job.driver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++++ tail -n 1
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ local property_value=
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ echo ''
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ local property_value=
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ echo ''
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ JOB_DRIVER_LOGGING_ENABLED=
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ [[ '' == \t\r\u\e ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ get_dataproc_property dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ local property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ local property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++++ tail -n 1
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++++ grep '^dataproc.logging.stackdriver.job.yarn.container.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++++ cut -d = -f 2-
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ local property_value=
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++++ echo ''
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ local property_value=
<13>Oct 13 16:53:30 google-dataproc-startup[837]: +++ echo ''
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ CONTAINER_LOGGING_ENABLED=
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ [[ '' == \t\r\u\e ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + run_in_background --tag setup-google-fluentd setup_service google-fluentd
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + PID=1611
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + echo 'Started background process [setup_service google-fluentd] as pid 1611'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: Started background process [setup_service google-fluentd] as pid 1611
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + wait_on_async_processes
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + loginfo 'Waiting on async proccesses'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + echo 'Waiting on async proccesses'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: Waiting on async proccesses
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + (( i = 0 ))
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + pid=1611
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + cmd='setup_service google-fluentd'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1611 cmd=[setup_service google-fluentd]'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + echo 'Waiting on pid=1611 cmd=[setup_service google-fluentd]'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: Waiting on pid=1611 cmd=[setup_service google-fluentd]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + wait 1611
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + run_with_logger --tag setup-google-fluentd setup_service google-fluentd
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + local tag=
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + local pid=1611
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + tag=setup-google-fluentd
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + shift 2
<13>Oct 13 16:53:30 google-dataproc-startup[837]: + exec
<13>Oct 13 16:53:30 google-dataproc-startup[837]: ++ logger -s -t 'setup-google-fluentd[1611]'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + setup_service google-fluentd
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + export KERBEROS_ENABLED=false
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + KERBEROS_ENABLED=false
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + export -f login_through_keytab_if_necessary
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + export MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + MY_FULL_HOSTNAME=cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + local service=google-fluentd
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + [[ google-fluentd == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + enable_service google-fluentd
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + local service=google-fluentd
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + local unit=google-fluentd.service
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + run_with_retries systemctl enable google-fluentd.service
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + local retry_backoff
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + cmd=("$@")
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + local -a cmd
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + loginfo 'About to run '\''systemctl enable google-fluentd.service'\'' with retries...'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + echo 'About to run '\''systemctl enable google-fluentd.service'\'' with retries...'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: About to run 'systemctl enable google-fluentd.service' with retries...
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + local update_succeeded=0
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + (( i = 0 ))
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + (( i < 12 ))
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: + systemctl enable google-fluentd.service
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: google-fluentd.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-google-fluentd[1611]: Executing: /lib/systemd/systemd-sysv-install enable google-fluentd
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 3306
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 2.'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 2.'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 2.
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + local 'props=Restart=on-abort
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: RemainAfterExit=no'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + [[ Restart=on-abort
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + in_array mariadb DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + local value=mariadb
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  mariadb  ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + return 1
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + [[ mariadb == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + run_with_retries systemctl start mariadb
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + local retry_backoff
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + cmd=("$@")
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + local -a cmd
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + loginfo 'About to run '\''systemctl start mariadb'\'' with retries...'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + echo 'About to run '\''systemctl start mariadb'\'' with retries...'
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: About to run 'systemctl start mariadb' with retries...
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + local update_succeeded=0
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + (( i = 0 ))
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + (( i < 12 ))
<13>Oct 13 16:53:30 google-dataproc-startup[837]: <13>Oct 13 16:53:30 setup-mariadb[1507]: + systemctl start mariadb
<13>Oct 13 16:53:31 google-dataproc-startup[837]: <13>Oct 13 16:53:31 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:31 google-dataproc-startup[837]: <13>Oct 13 16:53:31 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 3306
<13>Oct 13 16:53:31 google-dataproc-startup[837]: <13>Oct 13 16:53:31 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 16:53:31 google-dataproc-startup[837]: <13>Oct 13 16:53:31 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 3.'
<13>Oct 13 16:53:31 google-dataproc-startup[837]: <13>Oct 13 16:53:31 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 3.'
<13>Oct 13 16:53:31 google-dataproc-startup[837]: <13>Oct 13 16:53:31 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 3.
<13>Oct 13 16:53:31 google-dataproc-startup[837]: <13>Oct 13 16:53:31 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-mapreduce-historyserver[1497]: + update_succeeded=1
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-mapreduce-historyserver[1497]: + break
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-mapreduce-historyserver[1497]: + ((  1  ))
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-mapreduce-historyserver[1497]: ++ systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-yarn-timelineserver[1499]: + update_succeeded=1
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-yarn-timelineserver[1499]: + break
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-yarn-timelineserver[1499]: + ((  1  ))
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-yarn-timelineserver[1499]: ++ systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hive-server2[1496]: + update_succeeded=1
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hive-server2[1496]: + break
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hive-server2[1496]: + ((  1  ))
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hive-server2[1496]: ++ systemctl show hive-server2.service -p Restart,RemainAfterExit
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 uninstall[1431]: Reading package lists...
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-yarn-resourcemanager[1494]: + update_succeeded=1
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-yarn-resourcemanager[1494]: + break
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-yarn-resourcemanager[1494]: + ((  1  ))
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-yarn-resourcemanager[1494]: ++ systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 3306
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 4.'
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 4.'
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 4.
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-hdfs-secondarynamenode[1509]: + update_succeeded=1
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-hdfs-secondarynamenode[1509]: + break
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-hdfs-secondarynamenode[1509]: + ((  1  ))
<13>Oct 13 16:53:32 google-dataproc-startup[837]: <13>Oct 13 16:53:32 setup-hadoop-hdfs-secondarynamenode[1509]: ++ systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + update_succeeded=1
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + break
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + ((  1  ))
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: ++ systemctl show spark-history-server.service -p Restart,RemainAfterExit
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + update_succeeded=1
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + break
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + ((  1  ))
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: ++ systemctl show google-fluentd.service -p Restart,RemainAfterExit
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: + local 'props=Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: RemainAfterExit=no'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: + local drop_in_dir=/etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: + mkdir /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + local 'props=Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: RemainAfterExit=no'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + local drop_in_dir=/etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + mkdir /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: + in_array hadoop-mapreduce-historyserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: + local value=hadoop-mapreduce-historyserver
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-mapreduce-historyserver  ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-mapreduce-historyserver[1497]: + return
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]: Building dependency tree...
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + in_array hadoop-yarn-timelineserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + local value=hadoop-yarn-timelineserver
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-yarn-timelineserver  ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + return 1
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + run_with_retries systemctl start hadoop-yarn-timelineserver
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + local retry_backoff
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + cmd=("$@")
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + local -a cmd
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + loginfo 'About to run '\''systemctl start hadoop-yarn-timelineserver'\'' with retries...'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + echo 'About to run '\''systemctl start hadoop-yarn-timelineserver'\'' with retries...'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: About to run 'systemctl start hadoop-yarn-timelineserver' with retries...
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + local update_succeeded=0
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + (( i = 0 ))
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + (( i < 12 ))
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: + systemctl start hadoop-yarn-timelineserver
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]: Reading state information...
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + local 'props=Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: RemainAfterExit=no'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + local drop_in_dir=/etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + mkdir /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + local 'props=Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: RemainAfterExit=no'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + local drop_in_dir=/etc/systemd/system/hive-server2.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + mkdir /etc/systemd/system/hive-server2.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-server2.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + in_array hive-server2 DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + local value=hive-server2
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hive-server2  ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + return 1
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + [[ hive-server2 == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-server2[1496]: + return
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + in_array hadoop-yarn-resourcemanager DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + local value=hadoop-yarn-resourcemanager
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-yarn-resourcemanager  ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + return 1
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + run_with_retries systemctl start hadoop-yarn-resourcemanager
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + local retry_backoff
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + cmd=("$@")
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + local -a cmd
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + loginfo 'About to run '\''systemctl start hadoop-yarn-resourcemanager'\'' with retries...'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + echo 'About to run '\''systemctl start hadoop-yarn-resourcemanager'\'' with retries...'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: About to run 'systemctl start hadoop-yarn-resourcemanager' with retries...
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + local update_succeeded=0
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + (( i = 0 ))
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + (( i < 12 ))
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: + systemctl start hadoop-yarn-resourcemanager
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + local 'props=Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: RemainAfterExit=no'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + local drop_in_dir=/etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + mkdir /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + local 'props=Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: RemainAfterExit=no'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + local drop_in_dir=/etc/systemd/system/spark-history-server.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + mkdir /etc/systemd/system/spark-history-server.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + local 'props=Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: RemainAfterExit=yes'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + [[ Restart=no
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + in_array google-fluentd DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + local value=google-fluentd
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  google-fluentd  ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + return 1
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + [[ google-fluentd == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + run_with_retries systemctl start google-fluentd
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + local retry_backoff
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + cmd=("$@")
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + local -a cmd
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + loginfo 'About to run '\''systemctl start google-fluentd'\'' with retries...'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + echo 'About to run '\''systemctl start google-fluentd'\'' with retries...'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: About to run 'systemctl start google-fluentd' with retries...
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + local update_succeeded=0
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + (( i = 0 ))
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + (( i < 12 ))
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-google-fluentd[1611]: + systemctl start google-fluentd
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/spark-history-server.service.d
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + in_array hadoop-hdfs-secondarynamenode DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + local value=hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-hdfs-secondarynamenode  ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + return 1
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + run_with_retries systemctl start hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + local retry_backoff
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + cmd=("$@")
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + local -a cmd
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + loginfo 'About to run '\''systemctl start hadoop-hdfs-secondarynamenode'\'' with retries...'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + echo 'About to run '\''systemctl start hadoop-hdfs-secondarynamenode'\'' with retries...'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: About to run 'systemctl start hadoop-hdfs-secondarynamenode' with retries...
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + local update_succeeded=0
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + (( i = 0 ))
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + (( i < 12 ))
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: + systemctl start hadoop-hdfs-secondarynamenode
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + in_array spark-history-server DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + local value=spark-history-server
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  spark-history-server  ]]
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-spark-history-server[1498]: + return
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-timelineserver[1499]: Warning: hadoop-yarn-timelineserver.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-yarn-resourcemanager[1494]: Warning: hadoop-yarn-resourcemanager.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hadoop-hdfs-secondarynamenode[1509]: Warning: hadoop-hdfs-secondarynamenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]: The following packages will be REMOVED:
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   bind9-host* druid* fonts-font-awesome* fonts-mathjax* geoip-database*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   hadoop-hdfs-datanode* hadoop-hdfs-journalnode* hadoop-hdfs-zkfc*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   hadoop-yarn-nodemanager* hive-webhcat* hive-webhcat-server*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   javascript-common* kafka* kafka-server* knox* krb5-admin-server*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   krb5-config* krb5-kdc* krb5-kpropd* krb5-user* libbind9-140* libc-ares2*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   libdns162* libev4* libfile-copy-recursive-perl* libgeoip1* libgssrpc4*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   libhttp-parser2.8* libisc160* libisccc140* libisccfg140* libjs-bootstrap*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   libjs-d3* libjs-es5-shim* libjs-highlight.js* libjs-jquery*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   libjs-jquery-datatables* libjs-jquery-metadata* libjs-jquery-selectize.js*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   libjs-jquery-tablesorter* libjs-jquery-ui* libjs-json* libjs-mathjax*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   libjs-microplugin.js* libjs-modernizr* libjs-prettify* libjs-sifter.js*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   libjs-twitter-bootstrap* libjs-twitter-bootstrap-datepicker*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   libkadm5clnt-mit11* libkadm5srv-mit11* libkdb5-8* liblua5.1-0*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   libluajit-5.1-2* libluajit-5.1-common* liblwres141* libuv1* libverto-libev1*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   libverto1* libyaml-0-2* littler* node-highlight.js* node-normalize.css*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   nodejs* nodejs-doc* pandoc* pandoc-data* r-cran-assertthat*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-base64enc* r-cran-bindr* r-cran-bindrcpp* r-cran-bit* r-cran-bit64*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-cli* r-cran-colorspace* r-cran-crayon* r-cran-data.table* r-cran-dbi*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-digest* r-cran-dplyr* r-cran-evaluate* r-cran-fansi* r-cran-filehash*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-ggplot2* r-cran-glue* r-cran-googlevis* r-cran-gtable* r-cran-hexbin*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-highr* r-cran-hms* r-cran-htmltools* r-cran-htmlwidgets*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-httpuv* r-cran-jsonlite* r-cran-knitr* r-cran-labeling* r-cran-later*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-lazyeval* r-cran-littler* r-cran-magrittr* r-cran-mapproj*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-maps* r-cran-markdown* r-cran-memoise* r-cran-mime* r-cran-munsell*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-pillar* r-cran-pkgconfig* r-cran-pkgkitten* r-cran-plyr* r-cran-png*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-praise* r-cran-promises* r-cran-purrr* r-cran-r6*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-rcolorbrewer* r-cran-rcpp* r-cran-reshape2* r-cran-rlang*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-rmarkdown* r-cran-rsqlite* r-cran-scales* r-cran-shiny*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-sourcetools* r-cran-sp* r-cran-stringi* r-cran-stringr*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-testit* r-cran-testthat* r-cran-tibble* r-cran-tidyselect*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-tikzdevice* r-cran-tinytex* r-cran-utf8* r-cran-viridislite*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   r-cran-withr* r-cran-xfun* r-cran-xml2* r-cran-xtable* r-cran-yaml* solr*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 uninstall[1431]:   solr-server* update-inetd* xinetd* zeppelin* zookeeper-server*
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 3306
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 5.'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 5.'
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 5.
<13>Oct 13 16:53:33 google-dataproc-startup[837]: <13>Oct 13 16:53:33 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:34 google-dataproc-startup[837]: <13>Oct 13 16:53:34 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:34 google-dataproc-startup[837]: <13>Oct 13 16:53:34 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 3306
<13>Oct 13 16:53:34 google-dataproc-startup[837]: <13>Oct 13 16:53:34 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 16:53:34 google-dataproc-startup[837]: <13>Oct 13 16:53:34 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 6.'
<13>Oct 13 16:53:34 google-dataproc-startup[837]: <13>Oct 13 16:53:34 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 6.'
<13>Oct 13 16:53:34 google-dataproc-startup[837]: <13>Oct 13 16:53:34 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 6.
<13>Oct 13 16:53:34 google-dataproc-startup[837]: <13>Oct 13 16:53:34 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:35 google-dataproc-startup[837]: <13>Oct 13 16:53:35 uninstall[1431]: 0 upgraded, 0 newly installed, 146 to remove and 1 not upgraded.
<13>Oct 13 16:53:35 google-dataproc-startup[837]: <13>Oct 13 16:53:35 uninstall[1431]: After this operation, 2,006 MB disk space will be freed.
<13>Oct 13 16:53:35 google-dataproc-startup[837]: <13>Oct 13 16:53:35 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:35 google-dataproc-startup[837]: <13>Oct 13 16:53:35 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 3306
<13>Oct 13 16:53:35 google-dataproc-startup[837]: <13>Oct 13 16:53:35 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 3306 (tcp) failed: Connection refused
<13>Oct 13 16:53:35 google-dataproc-startup[837]: <13>Oct 13 16:53:35 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 7.'
<13>Oct 13 16:53:35 google-dataproc-startup[837]: <13>Oct 13 16:53:35 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 7.'
<13>Oct 13 16:53:35 google-dataproc-startup[837]: <13>Oct 13 16:53:35 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 3306 failed. Retry attempt: 7.
<13>Oct 13 16:53:35 google-dataproc-startup[837]: <13>Oct 13 16:53:35 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 3306
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: Connection to cluster-ea0a-m 3306 port [tcp/mysql] succeeded!
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + update_succeeded=1
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 3306 succeeded.'
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 3306 succeeded.'
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 3306 succeeded.
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + break
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + ((  1  ))
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + loginfo 'Service up on host=cluster-ea0a-m port=3306.'
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + echo 'Service up on host=cluster-ea0a-m port=3306.'
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: Service up on host=cluster-ea0a-m port=3306.
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + enable_service hive-metastore
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + local service=hive-metastore
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + local unit=hive-metastore.service
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + run_with_retries systemctl enable hive-metastore.service
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + local retry_backoff
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + cmd=("$@")
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + local -a cmd
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + loginfo 'About to run '\''systemctl enable hive-metastore.service'\'' with retries...'
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + echo 'About to run '\''systemctl enable hive-metastore.service'\'' with retries...'
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: About to run 'systemctl enable hive-metastore.service' with retries...
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + local update_succeeded=0
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + (( i = 0 ))
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + (( i < 12 ))
<13>Oct 13 16:53:36 google-dataproc-startup[837]: <13>Oct 13 16:53:36 setup-hive-metastore[1495]: + systemctl enable hive-metastore.service
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 uninstall[1431]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 119619 files and directories currently installed.)
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 uninstall[1431]: Removing krb5-admin-server (1.15-1+deb9u1) ...
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-hive-metastore[1495]: hive-metastore.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-hive-metastore[1495]: Executing: /lib/systemd/systemd-sysv-install enable hive-metastore
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: + update_succeeded=1
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: + break
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: + ((  1  ))
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: + [[ mariadb == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: ++ get_dataproc_property am.primary_only
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: ++ local property_name=am.primary_only
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: +++ local property_name=am.primary_only
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: ++++ tail -n 1
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: +++ local property_value=false
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: +++ echo false
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: ++ local property_value=false
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: ++ echo false
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 16:53:37 google-dataproc-startup[837]: <13>Oct 13 16:53:37 setup-mariadb[1507]: + [[ mariadb == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:38 INFO namenode.NameNode: STARTUP_MSG: 
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: /************************************************************
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: STARTUP_MSG: Starting NameNode
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: STARTUP_MSG:   host = cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal/10.128.0.8
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: STARTUP_MSG:   args = [-format, -nonInteractive]
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: STARTUP_MSG:   version = 2.9.2
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/hadoop-lzo-0.4.20.jar:/usr/lib/hadoop/lib/httpclient-4.5.2.jar:/usr/lib/hadoop/lib/gcs-connector-hadoop2-1.9.17.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/gcs-connector.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.1
<13>Oct 13 16:53:38 google-dataproc-startup[837]: 3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jetty-6.1.26.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0
<13>Oct 13 16:53:38 google-dataproc-startup[837]: .4.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/u
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: sr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/httpcore-4.4.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/li
<13>Oct 13 16:53:38 google-dataproc-startup[837]: b/jetty-sslengine-6.1.26.jar:/usr/lib/hadoop/lib/m
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: ockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.9.2.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-common-2.9.2-tests.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-2.9.2.jar:/usr/lib/hadoop/.//hadoop-nfs-2.9.2.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth-2.9.2.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib
<13>Oct 13 16:53:38 google-dataproc-startup[837]: /hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoo
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: p-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2
<13>Oct 13 16:53:38 google-dataproc-startup[837]: .7.8.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: .jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.9.2-tests.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/l
<13>Oct 13 16:53:38 google-dataproc-startup[837]: ib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: /hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/api-as
<13>Oct 13 16:53:38 google-dataproc-startup[837]: n1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/comm
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: ons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hado
<13>Oct 13 16:53:38 google-dataproc-startup[837]: op-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/ha
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: doop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/ht
<13>Oct 13 16:53:38 google-dataproc-startup[837]: tpcore-4.4.4.jar:/usr/lib/hadoop-yarn/lib/jetty-ut
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: il-6.1.26.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.9.2.jar:/usr
<13>Oct 13 16:53:38 google-dataproc-startup[837]: /lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/li
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: b/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-y
<13>Oct 13 16:53:38 google-dataproc-startup[837]: arn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-ya
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: rn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.9.2.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce
<13>Oct 13 16:53:38 google-dataproc-startup[837]: /lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapredu
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: ce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//l
<13>Oct 13 16:53:38 google-dataproc-startup[837]: eveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/./
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: /jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ar
<13>Oct 13 16:53:38 google-dataproc-startup[837]: chive-logs.jar:/usr/lib/hadoop-mapreduce/.//log4j-
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: 1.2.17.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//h
<13>Oct 13 16:53:38 google-dataproc-startup[837]: adoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hado
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: op-archives-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.199.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2
<13>Oct 13 16:53:38 google-dataproc-startup[837]: .0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//mssql-jd
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: bc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client
<13>Oct 13 16:53:38 google-dataproc-startup[837]: -hs-plugins-2.9.2.jar:/usr/lib/hadoop-mapreduce/./
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: /hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.9.2-tests.jar:/usr
<13>Oct 13 16:53:38 google-dataproc-startup[837]: /lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: /lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.9.2.jar:/usr/lib/hadoop-ma
<13>Oct 13 16:53:38 google-dataproc-startup[837]: preduce/.//json-20170516.jar:/usr/lib/hadoop-mapre
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: duce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.5.j
<13>Oct 13 16:53:38 google-dataproc-startup[837]: ar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: .jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sd
<13>Oct 13 16:53:38 google-dataproc-startup[837]: k-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ar
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: chive-logs-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.13.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: STARTUP_MSG:   build = https://bigdataoss-internal.googlesource.com/third_party/apache/hadoop -r 849ee9eda72c7e8b1eb9fc5a830432c887914111; compiled by 'bigtop' on 2019-09-18T10:58Z
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: STARTUP_MSG:   java = 1.8.0_222
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: ************************************************************/
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:38 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
<13>Oct 13 16:53:38 google-dataproc-startup[837]: <13>Oct 13 16:53:38 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:38 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + update_succeeded=1
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + break
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + ((  1  ))
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: ++ systemctl show hive-metastore.service -p Restart,RemainAfterExit
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + local 'props=Restart=no
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: RemainAfterExit=no'
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + [[ Restart=no
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + [[ Restart=no
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + local drop_in_dir=/etc/systemd/system/hive-metastore.service.d
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + mkdir /etc/systemd/system/hive-metastore.service.d
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-metastore.service.d
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + in_array hive-metastore DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + local value=hive-metastore
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hive-metastore  ]]
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + return 1
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + [[ hive-metastore == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + run_with_retries systemctl start hive-metastore
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + local retry_backoff
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + cmd=("$@")
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + local -a cmd
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + loginfo 'About to run '\''systemctl start hive-metastore'\'' with retries...'
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + echo 'About to run '\''systemctl start hive-metastore'\'' with retries...'
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: About to run 'systemctl start hive-metastore' with retries...
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + local update_succeeded=0
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + (( i = 0 ))
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + (( i < 12 ))
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: + systemctl start hive-metastore
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hadoop-hdfs-namenode[1493]: 2019-10-13T16:53:39.594+0000: 5.428: [GC (Allocation Failure) 2019-10-13T16:53:39.594+0000: 5.428: [ParNew: 32320K->3968K(36288K), 0.0409418 secs] 32320K->5320K(116864K), 0.0410501 secs] [Times: user=0.02 sys=0.01, real=0.04 secs] 
<13>Oct 13 16:53:39 google-dataproc-startup[837]: <13>Oct 13 16:53:39 setup-hive-metastore[1495]: Warning: hive-metastore.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: + update_succeeded=1
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: + break
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: + ((  1  ))
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: + update_succeeded=1
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: + break
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: + ((  1  ))
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: ++ get_dataproc_property am.primary_only
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: ++ get_dataproc_property am.primary_only
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: ++ local property_name=am.primary_only
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: ++ local property_name=am.primary_only
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: +++ local property_name=am.primary_only
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: +++ local property_name=am.primary_only
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: ++++ tail -n 1
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: +++ local property_value=false
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: +++ echo false
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: ++++ tail -n 1
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: ++ local property_value=false
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: ++ echo false
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: + [[ 0 -eq 0 ]]
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-resourcemanager[1494]: + [[ false == \t\r\u\e ]]
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: +++ local property_value=false
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: +++ echo false
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: ++ local property_value=false
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: ++ echo false
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 16:53:40 google-dataproc-startup[837]: <13>Oct 13 16:53:40 setup-hadoop-yarn-timelineserver[1499]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 16:53:41 google-dataproc-startup[837]: <13>Oct 13 16:53:41 uninstall[1431]: Removing krb5-kpropd (1.15-1+deb9u1) ...
<13>Oct 13 16:53:41 google-dataproc-startup[837]: <13>Oct 13 16:53:41 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:41 WARN common.Util: Path /hadoop/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
<13>Oct 13 16:53:41 google-dataproc-startup[837]: <13>Oct 13 16:53:41 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:41 WARN common.Util: Path /hadoop/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
<13>Oct 13 16:53:41 google-dataproc-startup[837]: <13>Oct 13 16:53:41 setup-hadoop-hdfs-namenode[1493]: Formatting using clusterid: CID-56a4745e-4a99-49ad-8a43-5a9433ed4a2a
<13>Oct 13 16:53:42 google-dataproc-startup[837]: <13>Oct 13 16:53:42 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:42 INFO namenode.FSEditLog: Edit logging is async:true
<13>Oct 13 16:53:42 google-dataproc-startup[837]: <13>Oct 13 16:53:42 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:42 INFO namenode.FSNamesystem: KeyProvider: null
<13>Oct 13 16:53:42 google-dataproc-startup[837]: <13>Oct 13 16:53:42 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:42 INFO namenode.FSNamesystem: fsLock is fair: true
<13>Oct 13 16:53:42 google-dataproc-startup[837]: <13>Oct 13 16:53:42 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:42 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
<13>Oct 13 16:53:42 google-dataproc-startup[837]: <13>Oct 13 16:53:42 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:42 INFO namenode.FSNamesystem: fsOwner             = hdfs (auth:SIMPLE)
<13>Oct 13 16:53:42 google-dataproc-startup[837]: <13>Oct 13 16:53:42 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:42 INFO namenode.FSNamesystem: supergroup          = hadoop
<13>Oct 13 16:53:42 google-dataproc-startup[837]: <13>Oct 13 16:53:42 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:42 INFO namenode.FSNamesystem: isPermissionEnabled = false
<13>Oct 13 16:53:42 google-dataproc-startup[837]: <13>Oct 13 16:53:42 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:42 INFO namenode.FSNamesystem: HA Enabled: false
<13>Oct 13 16:53:42 google-dataproc-startup[837]: <13>Oct 13 16:53:42 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:42 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
<13>Oct 13 16:53:42 google-dataproc-startup[837]: <13>Oct 13 16:53:42 setup-hadoop-hdfs-namenode[1493]: 2019-10-13T16:53:42.484+0000: 8.318: [GC (Allocation Failure) 2019-10-13T16:53:42.484+0000: 8.318: [ParNew: 36288K->3967K(36288K), 0.0654569 secs] 37640K->8258K(116864K), 0.0655488 secs] [Times: user=0.03 sys=0.01, real=0.07 secs] 
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + update_succeeded=1
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + break
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + ((  1  ))
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + [[ hive-metastore == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + wait_for_port cluster-ea0a-m 9083
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + local -r host=cluster-ea0a-m
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + local -r port=9083
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + loginfo 'Waiting for service to come up on host=cluster-ea0a-m port=9083.'
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + echo 'Waiting for service to come up on host=cluster-ea0a-m port=9083.'
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: Waiting for service to come up on host=cluster-ea0a-m port=9083.
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + retry_with_constant_backoff nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + local max_retry=300
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + cmd=("$@")
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + local -a cmd
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + local update_succeeded=0
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: ++ seq 1 300
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 1.'
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 1.'
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 1.
<13>Oct 13 16:53:44 google-dataproc-startup[837]: <13>Oct 13 16:53:44 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: + update_succeeded=1
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: + break
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: + ((  1  ))
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: ++ get_dataproc_property am.primary_only
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: ++ local property_name=am.primary_only
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: +++ local property_name=am.primary_only
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO util.HostsFileReader: Adding a node "cluster-ea0a-w-0.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO util.HostsFileReader: Adding a node "cluster-ea0a-w-1.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: ++++ tail -n 1
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: ++++ cut -d = -f 2-
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: +++ local property_value=false
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: +++ echo false
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: ++ local property_value=false
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: ++ echo false
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-secondarynamenode[1509]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.retry-hostname-dns-lookup=true
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Oct 13 16:53:45
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO util.GSet: Computing capacity for map BlocksMap
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO util.GSet: 2.0% max memory 1.4 GB = 29.6 MB
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO util.GSet: capacity      = 2^22 = 4194304 entries
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 2019-10-13T16:53:45.259+0000: 11.093: [GC (Allocation Failure) 2019-10-13T16:53:45.259+0000: 11.093: [ParNew: 35420K->3967K(36288K), 0.0175661 secs] 39711K->9682K(116864K), 0.0176410 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] 
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManager: defaultReplication         = 2
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManager: maxReplication             = 512
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManager: minReplication             = 1
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO namenode.FSNamesystem: Append Enabled: true
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 2.'
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 2.'
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 2.
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 uninstall[1431]: Removing krb5-kdc (1.15-1+deb9u1) ...
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO util.GSet: Computing capacity for map INodeMap
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO util.GSet: 1.0% max memory 1.4 GB = 14.8 MB
<13>Oct 13 16:53:45 google-dataproc-startup[837]: <13>Oct 13 16:53:45 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:45 INFO util.GSet: capacity      = 2^21 = 2097152 entries
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 2019-10-13T16:53:45.923+0000: 11.757: [GC (Allocation Failure) 2019-10-13T16:53:45.923+0000: 11.757: [ParNew: 28810K->1253K(36288K), 0.2087516 secs] 34524K->23916K(116864K), 0.2088083 secs] [Times: user=0.08 sys=0.01, real=0.21 secs] 
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO namenode.FSDirectory: ACLs enabled? false
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO namenode.FSDirectory: XAttrs enabled? true
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO namenode.NameNode: Caching file names occurring more than 10 times
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO util.GSet: Computing capacity for map cachedBlocks
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO util.GSet: 0.25% max memory 1.4 GB = 3.7 MB
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO util.GSet: capacity      = 2^19 = 524288 entries
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO util.GSet: Computing capacity for map NameNodeRetryCache
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO util.GSet: VM type       = 64-bit
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 454.5 KB
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO util.GSet: capacity      = 2^16 = 65536 entries
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1686650189-10.128.0.8-1570985626394
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO common.Storage: Storage directory /hadoop/dfs/name has been successfully formatted.
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 3.'
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 3.'
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 3.
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:46 google-dataproc-startup[837]: <13>Oct 13 16:53:46 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:46 INFO namenode.FSImageFormatProtobuf: Saving image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:47 INFO namenode.FSImageFormatProtobuf: Image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 318 bytes saved in 0 seconds .
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 4.'
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 4.'
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 4.
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:47 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: 19/10/13 16:53:47 INFO namenode.NameNode: SHUTDOWN_MSG: 
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: /************************************************************
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: SHUTDOWN_MSG: Shutting down NameNode at cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal/10.128.0.8
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: ************************************************************/
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: Heap
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]:  par new generation   total 36288K, used 30503K [0x00000000a2800000, 0x00000000a4f50000, 0x00000000ace60000)
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]:   eden space 32320K,  90% used [0x00000000a2800000, 0x00000000a4490980, 0x00000000a4790000)
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]:   from space 3968K,  31% used [0x00000000a4790000, 0x00000000a48c9610, 0x00000000a4b70000)
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]:   to   space 3968K,   0% used [0x00000000a4b70000, 0x00000000a4b70000, 0x00000000a4f50000)
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]:  concurrent mark-sweep generation total 80576K, used 22663K [0x00000000ace60000, 0x00000000b1d10000, 0x0000000100000000)
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]:  Metaspace       used 19020K, capacity 19212K, committed 19456K, reserved 1067008K
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]:   class space    used 2133K, capacity 2192K, committed 2304K, reserved 1048576K
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + update_succeeded=1
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + break
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + ((  1  ))
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + enable_service hadoop-hdfs-namenode
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + local service=hadoop-hdfs-namenode
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + local unit=hadoop-hdfs-namenode.service
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + run_with_retries systemctl enable hadoop-hdfs-namenode.service
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + local retry_backoff
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + cmd=("$@")
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + local -a cmd
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + loginfo 'About to run '\''systemctl enable hadoop-hdfs-namenode.service'\'' with retries...'
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + echo 'About to run '\''systemctl enable hadoop-hdfs-namenode.service'\'' with retries...'
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: About to run 'systemctl enable hadoop-hdfs-namenode.service' with retries...
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + local update_succeeded=0
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + (( i = 0 ))
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + (( i < 12 ))
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: + systemctl enable hadoop-hdfs-namenode.service
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: hadoop-hdfs-namenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 13 16:53:47 google-dataproc-startup[837]: <13>Oct 13 16:53:47 setup-hadoop-hdfs-namenode[1493]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-namenode
<13>Oct 13 16:53:48 google-dataproc-startup[837]: <13>Oct 13 16:53:48 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:48 google-dataproc-startup[837]: <13>Oct 13 16:53:48 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:48 google-dataproc-startup[837]: <13>Oct 13 16:53:48 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:48 google-dataproc-startup[837]: <13>Oct 13 16:53:48 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 5.'
<13>Oct 13 16:53:48 google-dataproc-startup[837]: <13>Oct 13 16:53:48 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 5.'
<13>Oct 13 16:53:48 google-dataproc-startup[837]: <13>Oct 13 16:53:48 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 5.
<13>Oct 13 16:53:48 google-dataproc-startup[837]: <13>Oct 13 16:53:48 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + update_succeeded=1
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + break
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + ((  1  ))
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: ++ systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + local 'props=Restart=no
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: RemainAfterExit=no'
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + [[ Restart=no
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + [[ Restart=no
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + local drop_in_dir=/etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + mkdir /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + in_array hadoop-hdfs-namenode DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + local value=hadoop-hdfs-namenode
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-hdfs-namenode  ]]
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + return 1
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + run_with_retries systemctl start hadoop-hdfs-namenode
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + local retry_backoff
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + cmd=("$@")
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + local -a cmd
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + loginfo 'About to run '\''systemctl start hadoop-hdfs-namenode'\'' with retries...'
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + echo 'About to run '\''systemctl start hadoop-hdfs-namenode'\'' with retries...'
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: About to run 'systemctl start hadoop-hdfs-namenode' with retries...
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + local update_succeeded=0
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + (( i = 0 ))
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + (( i < 12 ))
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: + systemctl start hadoop-hdfs-namenode
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 6.'
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 6.'
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 6.
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:49 google-dataproc-startup[837]: <13>Oct 13 16:53:49 setup-hadoop-hdfs-namenode[1493]: Warning: hadoop-hdfs-namenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 13 16:53:50 google-dataproc-startup[837]: <13>Oct 13 16:53:50 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:50 google-dataproc-startup[837]: <13>Oct 13 16:53:50 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:50 google-dataproc-startup[837]: <13>Oct 13 16:53:50 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:50 google-dataproc-startup[837]: <13>Oct 13 16:53:50 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 7.'
<13>Oct 13 16:53:50 google-dataproc-startup[837]: <13>Oct 13 16:53:50 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 7.'
<13>Oct 13 16:53:50 google-dataproc-startup[837]: <13>Oct 13 16:53:50 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 7.
<13>Oct 13 16:53:50 google-dataproc-startup[837]: <13>Oct 13 16:53:50 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:50 google-dataproc-startup[837]: <13>Oct 13 16:53:50 uninstall[1431]: Removing krb5-user (1.15-1+deb9u1) ...
<13>Oct 13 16:53:50 google-dataproc-startup[837]: <13>Oct 13 16:53:50 uninstall[1431]: Removing krb5-config (2.6) ...
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 uninstall[1431]: Removing bind9-host (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 uninstall[1431]: Removing druid (0.13.0-incubating-1) ...
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 uninstall[1431]: dpkg: warning: while removing druid, directory '/usr/lib/druid/extensions/mysql-metadata-storage' not empty so not removed
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 uninstall[1431]: dpkg: warning: while removing druid, directory '/usr/lib/druid/conf/druid/_common' not empty so not removed
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 uninstall[1431]: Removing r-cran-rmarkdown (1.11+dfsg-1~bpo9+1) ...
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 uninstall[1431]: Removing r-cran-shiny (1.2.0+dfsg-1~bpo9+1) ...
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 8.'
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 8.'
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 8.
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 uninstall[1431]: Removing fonts-font-awesome (4.7.0~dfsg-1) ...
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 uninstall[1431]: Removing r-cran-knitr (1.21+dfsg-2~bpo9+1) ...
<13>Oct 13 16:53:51 google-dataproc-startup[837]: <13>Oct 13 16:53:51 uninstall[1431]: Removing r-cran-markdown (0.9+dfsg-1~bpo9+1) ...
<13>Oct 13 16:53:52 google-dataproc-startup[837]: <13>Oct 13 16:53:52 uninstall[1431]: Removing libjs-mathjax (2.7.0-2) ...
<13>Oct 13 16:53:52 google-dataproc-startup[837]: <13>Oct 13 16:53:52 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:52 google-dataproc-startup[837]: <13>Oct 13 16:53:52 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:52 google-dataproc-startup[837]: <13>Oct 13 16:53:52 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:52 google-dataproc-startup[837]: <13>Oct 13 16:53:52 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 9.'
<13>Oct 13 16:53:52 google-dataproc-startup[837]: <13>Oct 13 16:53:52 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 9.'
<13>Oct 13 16:53:52 google-dataproc-startup[837]: <13>Oct 13 16:53:52 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 9.
<13>Oct 13 16:53:52 google-dataproc-startup[837]: <13>Oct 13 16:53:52 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:53 google-dataproc-startup[837]: <13>Oct 13 16:53:53 uninstall[1431]: Removing fonts-mathjax (2.7.0-2) ...
<13>Oct 13 16:53:53 google-dataproc-startup[837]: <13>Oct 13 16:53:53 uninstall[1431]: Removing geoip-database (20170512-1) ...
<13>Oct 13 16:53:53 google-dataproc-startup[837]: <13>Oct 13 16:53:53 uninstall[1431]: Removing hadoop-hdfs-datanode (2.9.2-1) ...
<13>Oct 13 16:53:53 google-dataproc-startup[837]: <13>Oct 13 16:53:53 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:53 google-dataproc-startup[837]: <13>Oct 13 16:53:53 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:53 google-dataproc-startup[837]: <13>Oct 13 16:53:53 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:53 google-dataproc-startup[837]: <13>Oct 13 16:53:53 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 10.'
<13>Oct 13 16:53:53 google-dataproc-startup[837]: <13>Oct 13 16:53:53 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 10.'
<13>Oct 13 16:53:53 google-dataproc-startup[837]: <13>Oct 13 16:53:53 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 10.
<13>Oct 13 16:53:53 google-dataproc-startup[837]: <13>Oct 13 16:53:53 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:54 google-dataproc-startup[837]: <13>Oct 13 16:53:54 uninstall[1431]: Removing hadoop-hdfs-journalnode (2.9.2-1) ...
<13>Oct 13 16:53:54 google-dataproc-startup[837]: <13>Oct 13 16:53:54 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:54 google-dataproc-startup[837]: <13>Oct 13 16:53:54 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:54 google-dataproc-startup[837]: <13>Oct 13 16:53:54 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:54 google-dataproc-startup[837]: <13>Oct 13 16:53:54 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 11.'
<13>Oct 13 16:53:54 google-dataproc-startup[837]: <13>Oct 13 16:53:54 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 11.'
<13>Oct 13 16:53:54 google-dataproc-startup[837]: <13>Oct 13 16:53:54 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 11.
<13>Oct 13 16:53:54 google-dataproc-startup[837]: <13>Oct 13 16:53:54 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:55 google-dataproc-startup[837]: <13>Oct 13 16:53:55 uninstall[1431]: Removing hadoop-hdfs-zkfc (2.9.2-1) ...
<13>Oct 13 16:53:55 google-dataproc-startup[837]: <13>Oct 13 16:53:55 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:55 google-dataproc-startup[837]: <13>Oct 13 16:53:55 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:55 google-dataproc-startup[837]: <13>Oct 13 16:53:55 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:55 google-dataproc-startup[837]: <13>Oct 13 16:53:55 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 12.'
<13>Oct 13 16:53:55 google-dataproc-startup[837]: <13>Oct 13 16:53:55 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 12.'
<13>Oct 13 16:53:55 google-dataproc-startup[837]: <13>Oct 13 16:53:55 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 12.
<13>Oct 13 16:53:55 google-dataproc-startup[837]: <13>Oct 13 16:53:55 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:56 google-dataproc-startup[837]: <13>Oct 13 16:53:56 uninstall[1431]: Removing hadoop-yarn-nodemanager (2.9.2-1) ...
<13>Oct 13 16:53:56 google-dataproc-startup[837]: <13>Oct 13 16:53:56 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:56 google-dataproc-startup[837]: <13>Oct 13 16:53:56 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:56 google-dataproc-startup[837]: <13>Oct 13 16:53:56 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:56 google-dataproc-startup[837]: <13>Oct 13 16:53:56 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 13.'
<13>Oct 13 16:53:56 google-dataproc-startup[837]: <13>Oct 13 16:53:56 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 13.'
<13>Oct 13 16:53:56 google-dataproc-startup[837]: <13>Oct 13 16:53:56 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 13.
<13>Oct 13 16:53:56 google-dataproc-startup[837]: <13>Oct 13 16:53:56 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:57 google-dataproc-startup[837]: <13>Oct 13 16:53:57 uninstall[1431]: Removing hive-webhcat-server (2.3.5-1) ...
<13>Oct 13 16:53:57 google-dataproc-startup[837]: <13>Oct 13 16:53:57 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:57 google-dataproc-startup[837]: <13>Oct 13 16:53:57 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:57 google-dataproc-startup[837]: <13>Oct 13 16:53:57 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:57 google-dataproc-startup[837]: <13>Oct 13 16:53:57 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 14.'
<13>Oct 13 16:53:57 google-dataproc-startup[837]: <13>Oct 13 16:53:57 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 14.'
<13>Oct 13 16:53:57 google-dataproc-startup[837]: <13>Oct 13 16:53:57 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 14.
<13>Oct 13 16:53:57 google-dataproc-startup[837]: <13>Oct 13 16:53:57 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:58 google-dataproc-startup[837]: <13>Oct 13 16:53:58 uninstall[1431]: Removing hive-webhcat (2.3.5-1) ...
<13>Oct 13 16:53:58 google-dataproc-startup[837]: <13>Oct 13 16:53:58 uninstall[1431]: Removing javascript-common (11) ...
<13>Oct 13 16:53:58 google-dataproc-startup[837]: <13>Oct 13 16:53:58 uninstall[1431]: Removing kafka-server (1.1.1-1) ...
<13>Oct 13 16:53:58 google-dataproc-startup[837]: <13>Oct 13 16:53:58 uninstall[1431]: Removing kafka (1.1.1-1) ...
<13>Oct 13 16:53:58 google-dataproc-startup[837]: <13>Oct 13 16:53:58 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:58 google-dataproc-startup[837]: <13>Oct 13 16:53:58 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 15.'
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 15.'
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 15.
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 uninstall[1431]: Removing knox (1.1.0-1) ...
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 uninstall[1431]: Removing libbind9-140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + update_succeeded=1
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + break
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + ((  1  ))
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + [[ 0 -eq 0 ]]
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + loginfo 'Waiting for namenode to listen on rpc port'
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + echo 'Waiting for namenode to listen on rpc port'
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: Waiting for namenode to listen on rpc port
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + wait_for_port cluster-ea0a-m 8020
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + local -r host=cluster-ea0a-m
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + local -r port=8020
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + loginfo 'Waiting for service to come up on host=cluster-ea0a-m port=8020.'
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + echo 'Waiting for service to come up on host=cluster-ea0a-m port=8020.'
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: Waiting for service to come up on host=cluster-ea0a-m port=8020.
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + retry_with_constant_backoff nc -v -z -w 0 cluster-ea0a-m 8020
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + local max_retry=300
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + cmd=("$@")
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + local -a cmd
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + local update_succeeded=0
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: ++ seq 1 300
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + nc -v -z -w 0 cluster-ea0a-m 8020
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: nc: connect to cluster-ea0a-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 1.'
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + echo 'nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 1.'
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 1.
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 setup-hadoop-hdfs-namenode[1493]: + sleep 1
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 uninstall[1431]: Removing node-highlight.js (8.2+ds-5) ...
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 uninstall[1431]: Removing nodejs (8.11.1~dfsg-2~bpo9+1) ...
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 uninstall[1431]: Removing libc-ares2:amd64 (1.14.0-1~bpo9+1) ...
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 uninstall[1431]: Removing libisccfg140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 uninstall[1431]: Removing libdns162:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 16:53:59 google-dataproc-startup[837]: <13>Oct 13 16:53:59 uninstall[1431]: Removing update-inetd (4.44) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 16.'
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 16.'
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 16.
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libfile-copy-recursive-perl (0.38-1) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing liblwres141:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libisccc140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libkadm5srv-mit11:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libkdb5-8:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libkadm5clnt-mit11:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libgssrpc4:amd64 (1.15-1+deb9u1) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libhttp-parser2.8:amd64 (2.8.1-1~bpo9+1) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libisc160:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hadoop-hdfs-namenode[1493]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hadoop-hdfs-namenode[1493]: + nc -v -z -w 0 cluster-ea0a-m 8020
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hadoop-hdfs-namenode[1493]: nc: connect to cluster-ea0a-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hadoop-hdfs-namenode[1493]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 2.'
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hadoop-hdfs-namenode[1493]: + echo 'nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 2.'
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hadoop-hdfs-namenode[1493]: nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 2.
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 setup-hadoop-hdfs-namenode[1493]: + sleep 1
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libjs-bootstrap (3.3.7+dfsg-2+deb9u2) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libjs-d3 (3.5.17-2) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libjs-es5-shim (4.5.9-1) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing r-cran-highr (0.6-1) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libjs-highlight.js (8.2+ds-5) ...
<13>Oct 13 16:54:00 google-dataproc-startup[837]: <13>Oct 13 16:54:00 uninstall[1431]: Removing libjs-jquery-selectize.js (0.12.4+dfsg-1~bpo9+1) ...
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 uninstall[1431]: Removing libjs-twitter-bootstrap-datepicker (1.3.1+dfsg1-1) ...
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 17.'
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 17.'
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 17.
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 uninstall[1431]: Removing libjs-twitter-bootstrap (2.0.2+dfsg-10) ...
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 uninstall[1431]: Removing libjs-jquery-tablesorter (1:2.31.1+dfsg1-1~bpo9+1) ...
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 uninstall[1431]: Removing libjs-jquery-datatables (1.10.13+dfsg-2) ...
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 uninstall[1431]: Removing libjs-jquery-metadata (11-3) ...
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 uninstall[1431]: Removing libjs-jquery-ui (1.12.1+dfsg-4) ...
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hadoop-hdfs-namenode[1493]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hadoop-hdfs-namenode[1493]: + nc -v -z -w 0 cluster-ea0a-m 8020
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hadoop-hdfs-namenode[1493]: nc: connect to cluster-ea0a-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hadoop-hdfs-namenode[1493]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 3.'
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hadoop-hdfs-namenode[1493]: + echo 'nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 3.'
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hadoop-hdfs-namenode[1493]: nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 3.
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 setup-hadoop-hdfs-namenode[1493]: + sleep 1
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 uninstall[1431]: Removing libjs-json (0~20160510-1) ...
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 uninstall[1431]: Removing libjs-microplugin.js (0.0.3+dfsg-1) ...
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 uninstall[1431]: Removing libjs-modernizr (2.6.2+ds1-1) ...
<13>Oct 13 16:54:01 google-dataproc-startup[837]: <13>Oct 13 16:54:01 uninstall[1431]: Removing libjs-prettify (2013.03.04+dfsg-4) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing libjs-sifter.js (0.5.1+dfsg-2) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing pandoc (1.17.2~dfsg-3) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 18.'
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 18.'
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 18.
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing liblua5.1-0:amd64 (5.1.5-8.1+b2) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing libluajit-5.1-2:amd64 (2.1.0~beta3+dfsg-5.1~bpo9+1) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing libluajit-5.1-common (2.1.0~beta3+dfsg-5.1~bpo9+1) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing r-cran-httpuv (1.4.5.1+dfsg-1~bpo9+1) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing libuv1:amd64 (1.18.0-3~bpo9+1) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing libyaml-0-2:amd64 (0.2.1-1~bpo9+1) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing r-cran-dplyr (0.7.8-1~bpo9+1) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing r-cran-tidyselect (0.2.5-1~bpo9+1) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing r-cran-ggplot2 (3.1.0-1~bpo9+1) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hadoop-hdfs-namenode[1493]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hadoop-hdfs-namenode[1493]: + nc -v -z -w 0 cluster-ea0a-m 8020
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hadoop-hdfs-namenode[1493]: nc: connect to cluster-ea0a-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hadoop-hdfs-namenode[1493]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 4.'
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hadoop-hdfs-namenode[1493]: + echo 'nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 4.'
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hadoop-hdfs-namenode[1493]: nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 4.
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 setup-hadoop-hdfs-namenode[1493]: + sleep 1
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing r-cran-scales (1.0.0-1~bpo9+1) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing littler (0.3.1-1) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing node-normalize.css (8.0.0-3~bpo9+1) ...
<13>Oct 13 16:54:02 google-dataproc-startup[837]: <13>Oct 13 16:54:02 uninstall[1431]: Removing nodejs-doc (8.11.1~dfsg-2~bpo9+1) ...
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 uninstall[1431]: Removing pandoc-data (1.17.2~dfsg-3) ...
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 uninstall[1431]: Removing r-cran-tibble (2.0.1-1~bpo9+1) ...
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 19.'
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 19.'
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 19.
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 uninstall[1431]: Removing r-cran-pillar (1.3.1-1~bpo9+1) ...
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 uninstall[1431]: Removing r-cran-base64enc (0.1-3-1) ...
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 uninstall[1431]: Removing r-cran-bindrcpp (0.2.2-2~bpo9+1) ...
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 uninstall[1431]: Removing r-cran-bindr (0.1.1-2~bpo9+1) ...
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 uninstall[1431]: Removing r-cran-bit64 (0.9-7-2~bpo9+1) ...
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 uninstall[1431]: Removing r-cran-bit (1.1-14-1~bpo9+1) ...
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hadoop-hdfs-namenode[1493]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hadoop-hdfs-namenode[1493]: + nc -v -z -w 0 cluster-ea0a-m 8020
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 uninstall[1431]: Removing r-cran-munsell (0.5.0-1~bpo9+1) ...
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hadoop-hdfs-namenode[1493]: nc: connect to cluster-ea0a-m port 8020 (tcp) failed: Connection refused
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hadoop-hdfs-namenode[1493]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 5.'
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hadoop-hdfs-namenode[1493]: + echo 'nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 5.'
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hadoop-hdfs-namenode[1493]: nc -v -z -w 0 cluster-ea0a-m 8020 failed. Retry attempt: 5.
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 setup-hadoop-hdfs-namenode[1493]: + sleep 1
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 uninstall[1431]: Removing r-cran-colorspace (1.3-2-1) ...
<13>Oct 13 16:54:03 google-dataproc-startup[837]: <13>Oct 13 16:54:03 uninstall[1431]: Removing r-cran-testthat (2.0.1-1~bpo9+1) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-data.table (1.10.0-1) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-rsqlite (1.1-2-1) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 20.'
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 20.'
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 20.
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-dbi (1.0.0-1~bpo9+2) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-memoise (1.1.0-1~bpo9+1) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-htmlwidgets (1.3+dfsg-1~bpo9+1) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-htmltools (0.3.6-2~bpo9+1) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-digest (0.6.11-1) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-evaluate (0.10-1) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-fansi (0.4.0-1~bpo9+1) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + nc -v -z -w 0 cluster-ea0a-m 8020
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: Connection to cluster-ea0a-m 8020 port [tcp/*] succeeded!
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + update_succeeded=1
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 8020 succeeded.'
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + echo 'nc -v -z -w 0 cluster-ea0a-m 8020 succeeded.'
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: nc -v -z -w 0 cluster-ea0a-m 8020 succeeded.
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + break
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + ((  1  ))
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + loginfo 'Service up on host=cluster-ea0a-m port=8020.'
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + echo 'Service up on host=cluster-ea0a-m port=8020.'
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: Service up on host=cluster-ea0a-m port=8020.
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + loginfo 'Initializing HDFS directories'
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + echo 'Initializing HDFS directories'
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: Initializing HDFS directories
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + HADOOP_USERS=(hdfs mapred yarn spark pig hive hbase zookeeper)
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + REAL_USERS=($(getent passwd | awk -F: '1000 < $3 && $3 < 6000 { print $1}'))
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: ++ getent passwd
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: ++ awk -F: '1000 < $3 && $3 < 6000 { print $1}'
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + HDFS_USERS=("${HADOOP_USERS[@]}" "${REAL_USERS[@]}")
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + HDFS_USER_DIRS=("${HDFS_USERS[@]/#//user/}")
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.eventLog.dir
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: ++ local property_file=/etc/spark/conf/spark-defaults.conf
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: ++ local property_name=spark.eventLog.dir
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: +++ grep '^spark.eventLog.dir=' /etc/spark/conf/spark-defaults.conf
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: +++ cut -d = -f 2-
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-tikzdevice (0.10-1-1) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: +++ tail -n 1
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: ++ local property_value=hdfs://cluster-ea0a-m/user/spark/eventlog
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: ++ echo hdfs://cluster-ea0a-m/user/spark/eventlog
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + SPARK_EVENTLOG_DIR=hdfs://cluster-ea0a-m/user/spark/eventlog
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 setup-hadoop-hdfs-namenode[1493]: + su -s /bin/bash hdfs -c 'login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-ea0a-m.us-central1-a.c.lustrous-drake-255300.internal &&              hadoop fs -mkdir -p              /tmp/hadoop-yarn/staging/history /user/hdfs /user/mapred /user/yarn /user/spark /user/pig /user/hive /user/hbase /user/zookeeper hdfs://cluster-ea0a-m/user/spark/eventlog'
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-filehash (2.3-1) ...
<13>Oct 13 16:54:04 google-dataproc-startup[837]: <13>Oct 13 16:54:04 uninstall[1431]: Removing r-cran-reshape2 (1.4.2-1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-stringr (1.4.0-1~bpo9+1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-glue (1.3.0-1~bpo9+1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 21.'
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 21.'
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 21.
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-googlevis (0.6.2-1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-gtable (0.2.0-1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-hexbin (1.27.1-1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-hms (0.4.2-1~bpo9+1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-jsonlite (1.6+dfsg-1~bpo9+1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-labeling (0.3-1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-promises (1.0.1-2~bpo9+1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-later (0.7.5+dfsg-2~bpo9+1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-lazyeval (0.2.0-1) ...
<13>Oct 13 16:54:05 google-dataproc-startup[837]: <13>Oct 13 16:54:05 uninstall[1431]: Removing r-cran-purrr (0.3.0-1~bpo9+1) ...
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 uninstall[1431]: Removing r-cran-magrittr (1.5-3) ...
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 22.'
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 22.'
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 22.
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 uninstall[1431]: Removing r-cran-mapproj (1.2-4-1) ...
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 uninstall[1431]: Removing r-cran-maps (3.1.1-1) ...
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 uninstall[1431]: Removing r-cran-mime (0.5-1) ...
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 uninstall[1431]: Removing r-cran-pkgconfig (2.0.2-1~bpo9+1) ...
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 uninstall[1431]: Removing r-cran-plyr (1.8.4-1) ...
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 uninstall[1431]: Removing r-cran-png (0.1-7-1) ...
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 uninstall[1431]: Removing r-cran-praise (1.0.0-1) ...
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 uninstall[1431]: Removing r-cran-r6 (2.4.0-1~bpo9+1) ...
<13>Oct 13 16:54:06 google-dataproc-startup[837]: <13>Oct 13 16:54:06 uninstall[1431]: Removing r-cran-rcolorbrewer (1.1-2-1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-rlang (0.3.1-2~bpo9+1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-sourcetools (0.1.5-1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-sp (1:1.2-4-1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-stringi (1.2.4-2~bpo9+1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 23.'
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 23.'
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 23.
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-testit (0.6-1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-tinytex (0.10-1~bpo9+1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-utf8 (1.1.4-1~bpo9+1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-viridislite (0.3.0-3~bpo9+1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-withr (2.1.2-1~bpo9+1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-xfun (0.4-1~bpo9+1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-xml2 (1.1.0-1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-xtable (1:1.8-2-1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing r-cran-yaml (2.2.0-1~bpo9+1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing solr-server (6.6.5-1) ...
<13>Oct 13 16:54:07 google-dataproc-startup[837]: <13>Oct 13 16:54:07 uninstall[1431]: Removing solr (6.6.5-1) ...
<13>Oct 13 16:54:08 google-dataproc-startup[837]: <13>Oct 13 16:54:08 uninstall[1431]: Removing xinetd (1:2.3.15-7) ...
<13>Oct 13 16:54:08 google-dataproc-startup[837]: <13>Oct 13 16:54:08 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:08 google-dataproc-startup[837]: <13>Oct 13 16:54:08 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:08 google-dataproc-startup[837]: <13>Oct 13 16:54:08 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:08 google-dataproc-startup[837]: <13>Oct 13 16:54:08 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 24.'
<13>Oct 13 16:54:08 google-dataproc-startup[837]: <13>Oct 13 16:54:08 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 24.'
<13>Oct 13 16:54:08 google-dataproc-startup[837]: <13>Oct 13 16:54:08 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 24.
<13>Oct 13 16:54:08 google-dataproc-startup[837]: <13>Oct 13 16:54:08 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:08 google-dataproc-startup[837]: <13>Oct 13 16:54:08 uninstall[1431]: Removing zeppelin (0.8.0-1) ...
<13>Oct 13 16:54:09 google-dataproc-startup[837]: <13>Oct 13 16:54:09 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:09 google-dataproc-startup[837]: <13>Oct 13 16:54:09 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:09 google-dataproc-startup[837]: <13>Oct 13 16:54:09 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:09 google-dataproc-startup[837]: <13>Oct 13 16:54:09 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 25.'
<13>Oct 13 16:54:09 google-dataproc-startup[837]: <13>Oct 13 16:54:09 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 25.'
<13>Oct 13 16:54:09 google-dataproc-startup[837]: <13>Oct 13 16:54:09 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 25.
<13>Oct 13 16:54:09 google-dataproc-startup[837]: <13>Oct 13 16:54:09 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:09 google-dataproc-startup[837]: <13>Oct 13 16:54:09 uninstall[1431]: Removing zookeeper-server (3.4.13-1) ...
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 26.'
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 26.'
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 26.
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 uninstall[1431]: Removing libgeoip1:amd64 (1.6.9-4) ...
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 uninstall[1431]: Removing libjs-jquery (3.1.1-2+deb9u1) ...
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 uninstall[1431]: Removing r-cran-rcpp (1.0.0-1~bpo9+1) ...
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 uninstall[1431]: Removing r-cran-cli (1.0.1-1~bpo9+1) ...
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: + run_with_retries sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: + local retry_backoff
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: + cmd=("$@")
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: + local -a cmd
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: + loginfo 'About to run '\''sudo -u hdfs hadoop fs -chmod -R 1777 /'\'' with retries...'
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: + echo 'About to run '\''sudo -u hdfs hadoop fs -chmod -R 1777 /'\'' with retries...'
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: About to run 'sudo -u hdfs hadoop fs -chmod -R 1777 /' with retries...
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: + local update_succeeded=0
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: + (( i = 0 ))
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: + (( i < 12 ))
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 setup-hadoop-hdfs-namenode[1493]: + sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 uninstall[1431]: Removing r-cran-assertthat (0.2.0-1~bpo9+1) ...
<13>Oct 13 16:54:10 google-dataproc-startup[837]: <13>Oct 13 16:54:10 uninstall[1431]: Removing r-cran-crayon (1.3.4-2~bpo9+1) ...
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 uninstall[1431]: Removing r-cran-littler (0.3.1-1) ...
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 uninstall[1431]: Removing r-cran-pkgkitten (0.1.4-1) ...
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 uninstall[1431]: Removing libverto1:amd64 (0.2.4-2.1) ...
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 uninstall[1431]: Removing libverto-libev1:amd64 (0.2.4-2.1) ...
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 uninstall[1431]: Removing libev4 (1:4.22-1+b1) ...
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 27.'
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 27.'
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 27.
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 uninstall[1431]: Processing triggers for libc-bin (2.24-11+deb9u4) ...
<13>Oct 13 16:54:11 google-dataproc-startup[837]: <13>Oct 13 16:54:11 uninstall[1431]: Processing triggers for man-db (2.7.6.1-2) ...
<13>Oct 13 16:54:12 google-dataproc-startup[837]: <13>Oct 13 16:54:12 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:12 google-dataproc-startup[837]: <13>Oct 13 16:54:12 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:12 google-dataproc-startup[837]: <13>Oct 13 16:54:12 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:12 google-dataproc-startup[837]: <13>Oct 13 16:54:12 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 28.'
<13>Oct 13 16:54:12 google-dataproc-startup[837]: <13>Oct 13 16:54:12 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 28.'
<13>Oct 13 16:54:12 google-dataproc-startup[837]: <13>Oct 13 16:54:12 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 28.
<13>Oct 13 16:54:12 google-dataproc-startup[837]: <13>Oct 13 16:54:12 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:13 google-dataproc-startup[837]: <13>Oct 13 16:54:13 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:13 google-dataproc-startup[837]: <13>Oct 13 16:54:13 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:13 google-dataproc-startup[837]: <13>Oct 13 16:54:13 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:13 google-dataproc-startup[837]: <13>Oct 13 16:54:13 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 29.'
<13>Oct 13 16:54:13 google-dataproc-startup[837]: <13>Oct 13 16:54:13 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 29.'
<13>Oct 13 16:54:13 google-dataproc-startup[837]: <13>Oct 13 16:54:13 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 29.
<13>Oct 13 16:54:13 google-dataproc-startup[837]: <13>Oct 13 16:54:13 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:14 google-dataproc-startup[837]: <13>Oct 13 16:54:14 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:14 google-dataproc-startup[837]: <13>Oct 13 16:54:14 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:14 google-dataproc-startup[837]: <13>Oct 13 16:54:14 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:14 google-dataproc-startup[837]: <13>Oct 13 16:54:14 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 30.'
<13>Oct 13 16:54:14 google-dataproc-startup[837]: <13>Oct 13 16:54:14 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 30.'
<13>Oct 13 16:54:14 google-dataproc-startup[837]: <13>Oct 13 16:54:14 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 30.
<13>Oct 13 16:54:14 google-dataproc-startup[837]: <13>Oct 13 16:54:14 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:15 google-dataproc-startup[837]: <13>Oct 13 16:54:15 uninstall[1431]: Processing triggers for fontconfig (2.11.0-6.7+b1) ...
<13>Oct 13 16:54:15 google-dataproc-startup[837]: <13>Oct 13 16:54:15 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:15 google-dataproc-startup[837]: <13>Oct 13 16:54:15 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:15 google-dataproc-startup[837]: <13>Oct 13 16:54:15 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:15 google-dataproc-startup[837]: <13>Oct 13 16:54:15 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 31.'
<13>Oct 13 16:54:15 google-dataproc-startup[837]: <13>Oct 13 16:54:15 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 31.'
<13>Oct 13 16:54:15 google-dataproc-startup[837]: <13>Oct 13 16:54:15 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 31.
<13>Oct 13 16:54:15 google-dataproc-startup[837]: <13>Oct 13 16:54:15 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 32.'
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 32.'
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 32.
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + update_succeeded=1
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + break
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + ((  1  ))
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + run_with_retries sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-ea0a-m/user/spark/eventlog
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + local retry_backoff
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + cmd=("$@")
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + local -a cmd
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + loginfo 'About to run '\''sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-ea0a-m/user/spark/eventlog'\'' with retries...'
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + echo 'About to run '\''sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-ea0a-m/user/spark/eventlog'\'' with retries...'
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: About to run 'sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-ea0a-m/user/spark/eventlog' with retries...
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + local update_succeeded=0
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + (( i = 0 ))
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + (( i < 12 ))
<13>Oct 13 16:54:16 google-dataproc-startup[837]: <13>Oct 13 16:54:16 setup-hadoop-hdfs-namenode[1493]: + sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-ea0a-m/user/spark/eventlog
<13>Oct 13 16:54:17 google-dataproc-startup[837]: <13>Oct 13 16:54:17 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:17 google-dataproc-startup[837]: <13>Oct 13 16:54:17 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:17 google-dataproc-startup[837]: <13>Oct 13 16:54:17 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:17 google-dataproc-startup[837]: <13>Oct 13 16:54:17 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 33.'
<13>Oct 13 16:54:17 google-dataproc-startup[837]: <13>Oct 13 16:54:17 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 33.'
<13>Oct 13 16:54:17 google-dataproc-startup[837]: <13>Oct 13 16:54:17 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 33.
<13>Oct 13 16:54:17 google-dataproc-startup[837]: <13>Oct 13 16:54:17 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:17 google-dataproc-startup[837]: <13>Oct 13 16:54:17 uninstall[1431]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 102578 files and directories currently installed.)
<13>Oct 13 16:54:17 google-dataproc-startup[837]: <13>Oct 13 16:54:17 uninstall[1431]: Purging configuration files for update-inetd (4.44) ...
<13>Oct 13 16:54:18 google-dataproc-startup[837]: <13>Oct 13 16:54:18 uninstall[1431]: Purging configuration files for hive-webhcat-server (2.3.5-1) ...
<13>Oct 13 16:54:18 google-dataproc-startup[837]: <13>Oct 13 16:54:18 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:18 google-dataproc-startup[837]: <13>Oct 13 16:54:18 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:18 google-dataproc-startup[837]: <13>Oct 13 16:54:18 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:18 google-dataproc-startup[837]: <13>Oct 13 16:54:18 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 34.'
<13>Oct 13 16:54:18 google-dataproc-startup[837]: <13>Oct 13 16:54:18 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 34.'
<13>Oct 13 16:54:18 google-dataproc-startup[837]: <13>Oct 13 16:54:18 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 34.
<13>Oct 13 16:54:18 google-dataproc-startup[837]: <13>Oct 13 16:54:18 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:18 google-dataproc-startup[837]: <13>Oct 13 16:54:18 uninstall[1431]: Purging configuration files for xinetd (1:2.3.15-7) ...
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 uninstall[1431]: Purging configuration files for zookeeper-server (3.4.13-1) ...
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 35.'
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 35.'
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 35.
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 uninstall[1431]: Purging configuration files for hadoop-hdfs-journalnode (2.9.2-1) ...
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: + update_succeeded=1
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: + break
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: + ((  1  ))
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: + [[ google-fluentd == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: ++ get_dataproc_property am.primary_only
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: ++ local property_name=am.primary_only
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: +++ local property_name=am.primary_only
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:54:19 google-dataproc-startup[837]: <13>Oct 13 16:54:19 setup-google-fluentd[1611]: ++++ tail -n 1
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-google-fluentd[1611]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-google-fluentd[1611]: ++++ cut -d = -f 2-
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-google-fluentd[1611]: +++ local property_value=false
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-google-fluentd[1611]: +++ echo false
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-google-fluentd[1611]: ++ local property_value=false
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-google-fluentd[1611]: ++ echo false
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-google-fluentd[1611]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-google-fluentd[1611]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + pid=1509
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + cmd='setup_service hadoop-hdfs-secondarynamenode'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1509 cmd=[setup_service hadoop-hdfs-secondarynamenode]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + echo 'Waiting on pid=1509 cmd=[setup_service hadoop-hdfs-secondarynamenode]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: Waiting on pid=1509 cmd=[setup_service hadoop-hdfs-secondarynamenode]
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + wait 1509
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + pid=1507
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + cmd='setup_service mariadb'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1507 cmd=[setup_service mariadb]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + echo 'Waiting on pid=1507 cmd=[setup_service mariadb]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: Waiting on pid=1507 cmd=[setup_service mariadb]
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + wait 1507
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + pid=1499
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + cmd='setup_service hadoop-yarn-timelineserver'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1499 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + echo 'Waiting on pid=1499 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: Waiting on pid=1499 cmd=[setup_service hadoop-yarn-timelineserver]
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + wait 1499
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + pid=1498
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + cmd='setup_service spark-history-server'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1498 cmd=[setup_service spark-history-server]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + echo 'Waiting on pid=1498 cmd=[setup_service spark-history-server]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: Waiting on pid=1498 cmd=[setup_service spark-history-server]
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + wait 1498
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + pid=1497
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + cmd='setup_service hadoop-mapreduce-historyserver'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1497 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + echo 'Waiting on pid=1497 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: Waiting on pid=1497 cmd=[setup_service hadoop-mapreduce-historyserver]
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + wait 1497
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + pid=1496
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + cmd='setup_service hive-server2'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1496 cmd=[setup_service hive-server2]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + echo 'Waiting on pid=1496 cmd=[setup_service hive-server2]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: Waiting on pid=1496 cmd=[setup_service hive-server2]
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + wait 1496
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + pid=1495
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + cmd='setup_service hive-metastore'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1495 cmd=[setup_service hive-metastore]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + echo 'Waiting on pid=1495 cmd=[setup_service hive-metastore]'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: Waiting on pid=1495 cmd=[setup_service hive-metastore]
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:20 google-dataproc-startup[837]: + wait 1495
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 uninstall[1431]: Purging configuration files for solr (6.6.5-1) ...
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 uninstall[1431]: Purging configuration files for krb5-admin-server (1.15-1+deb9u1) ...
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 36.'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 36.'
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 36.
<13>Oct 13 16:54:20 google-dataproc-startup[837]: <13>Oct 13 16:54:20 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:21 google-dataproc-startup[837]: <13>Oct 13 16:54:21 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:21 google-dataproc-startup[837]: <13>Oct 13 16:54:21 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:21 google-dataproc-startup[837]: <13>Oct 13 16:54:21 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:21 google-dataproc-startup[837]: <13>Oct 13 16:54:21 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 37.'
<13>Oct 13 16:54:21 google-dataproc-startup[837]: <13>Oct 13 16:54:21 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 37.'
<13>Oct 13 16:54:21 google-dataproc-startup[837]: <13>Oct 13 16:54:21 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 37.
<13>Oct 13 16:54:21 google-dataproc-startup[837]: <13>Oct 13 16:54:21 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:22 google-dataproc-startup[837]: <13>Oct 13 16:54:22 uninstall[1431]: Purging configuration files for kafka (1.1.1-1) ...
<13>Oct 13 16:54:22 google-dataproc-startup[837]: <13>Oct 13 16:54:22 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:22 google-dataproc-startup[837]: <13>Oct 13 16:54:22 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:22 google-dataproc-startup[837]: <13>Oct 13 16:54:22 uninstall[1431]: Purging configuration files for hadoop-hdfs-zkfc (2.9.2-1) ...
<13>Oct 13 16:54:22 google-dataproc-startup[837]: <13>Oct 13 16:54:22 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:22 google-dataproc-startup[837]: <13>Oct 13 16:54:22 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 38.'
<13>Oct 13 16:54:22 google-dataproc-startup[837]: <13>Oct 13 16:54:22 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 38.'
<13>Oct 13 16:54:22 google-dataproc-startup[837]: <13>Oct 13 16:54:22 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 38.
<13>Oct 13 16:54:22 google-dataproc-startup[837]: <13>Oct 13 16:54:22 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + update_succeeded=1
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + break
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + ((  1  ))
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + for SERVICE in "${DATAPROC_START_AFTER_HDFS_SERVICES[@]}"
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + run_with_retries systemctl start hadoop-mapreduce-historyserver
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + local retry_backoff
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + cmd=("$@")
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + local -a cmd
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + loginfo 'About to run '\''systemctl start hadoop-mapreduce-historyserver'\'' with retries...'
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + echo 'About to run '\''systemctl start hadoop-mapreduce-historyserver'\'' with retries...'
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: About to run 'systemctl start hadoop-mapreduce-historyserver' with retries...
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + local update_succeeded=0
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + (( i = 0 ))
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + (( i < 12 ))
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hadoop-hdfs-namenode[1493]: + systemctl start hadoop-mapreduce-historyserver
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 uninstall[1431]: Purging configuration files for krb5-kdc (1.15-1+deb9u1) ...
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 39.'
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 39.'
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 39.
<13>Oct 13 16:54:23 google-dataproc-startup[837]: <13>Oct 13 16:54:23 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:24 google-dataproc-startup[837]: <13>Oct 13 16:54:24 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:24 google-dataproc-startup[837]: <13>Oct 13 16:54:24 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:24 google-dataproc-startup[837]: <13>Oct 13 16:54:24 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:24 google-dataproc-startup[837]: <13>Oct 13 16:54:24 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 40.'
<13>Oct 13 16:54:24 google-dataproc-startup[837]: <13>Oct 13 16:54:24 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 40.'
<13>Oct 13 16:54:24 google-dataproc-startup[837]: <13>Oct 13 16:54:24 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 40.
<13>Oct 13 16:54:24 google-dataproc-startup[837]: <13>Oct 13 16:54:24 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:25 google-dataproc-startup[837]: <13>Oct 13 16:54:25 uninstall[1431]: Purging configuration files for libjs-jquery-ui (1.12.1+dfsg-4) ...
<13>Oct 13 16:54:25 google-dataproc-startup[837]: <13>Oct 13 16:54:25 uninstall[1431]: Purging configuration files for krb5-kpropd (1.15-1+deb9u1) ...
<13>Oct 13 16:54:25 google-dataproc-startup[837]: <13>Oct 13 16:54:25 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:25 google-dataproc-startup[837]: <13>Oct 13 16:54:25 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:25 google-dataproc-startup[837]: <13>Oct 13 16:54:25 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:25 google-dataproc-startup[837]: <13>Oct 13 16:54:25 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 41.'
<13>Oct 13 16:54:25 google-dataproc-startup[837]: <13>Oct 13 16:54:25 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 41.'
<13>Oct 13 16:54:25 google-dataproc-startup[837]: <13>Oct 13 16:54:25 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 41.
<13>Oct 13 16:54:25 google-dataproc-startup[837]: <13>Oct 13 16:54:25 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:26 google-dataproc-startup[837]: <13>Oct 13 16:54:26 uninstall[1431]: Purging configuration files for hadoop-hdfs-datanode (2.9.2-1) ...
<13>Oct 13 16:54:26 google-dataproc-startup[837]: <13>Oct 13 16:54:26 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:26 google-dataproc-startup[837]: <13>Oct 13 16:54:26 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:26 google-dataproc-startup[837]: <13>Oct 13 16:54:26 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:26 google-dataproc-startup[837]: <13>Oct 13 16:54:26 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 42.'
<13>Oct 13 16:54:26 google-dataproc-startup[837]: <13>Oct 13 16:54:26 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 42.'
<13>Oct 13 16:54:26 google-dataproc-startup[837]: <13>Oct 13 16:54:26 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 42.
<13>Oct 13 16:54:26 google-dataproc-startup[837]: <13>Oct 13 16:54:26 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:26 google-dataproc-startup[837]: <13>Oct 13 16:54:26 uninstall[1431]: Purging configuration files for javascript-common (11) ...
<13>Oct 13 16:54:27 google-dataproc-startup[837]: <13>Oct 13 16:54:27 uninstall[1431]: Purging configuration files for krb5-config (2.6) ...
<13>Oct 13 16:54:27 google-dataproc-startup[837]: <13>Oct 13 16:54:27 uninstall[1431]: Purging configuration files for knox (1.1.0-1) ...
<13>Oct 13 16:54:27 google-dataproc-startup[837]: <13>Oct 13 16:54:27 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:27 google-dataproc-startup[837]: <13>Oct 13 16:54:27 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:27 google-dataproc-startup[837]: <13>Oct 13 16:54:27 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:27 google-dataproc-startup[837]: <13>Oct 13 16:54:27 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 43.'
<13>Oct 13 16:54:27 google-dataproc-startup[837]: <13>Oct 13 16:54:27 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 43.'
<13>Oct 13 16:54:27 google-dataproc-startup[837]: <13>Oct 13 16:54:27 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 43.
<13>Oct 13 16:54:27 google-dataproc-startup[837]: <13>Oct 13 16:54:27 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 uninstall[1431]: Purging configuration files for r-cran-rmarkdown (1.11+dfsg-1~bpo9+1) ...
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 uninstall[1431]: Purging configuration files for hive-webhcat (2.3.5-1) ...
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 uninstall[1431]: Purging configuration files for solr-server (6.6.5-1) ...
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 uninstall[1431]: Purging configuration files for kafka-server (1.1.1-1) ...
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 44.'
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 44.'
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 44.
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:28 google-dataproc-startup[837]: <13>Oct 13 16:54:28 uninstall[1431]: Purging configuration files for zeppelin (0.8.0-1) ...
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + update_succeeded=1
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + break
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + ((  1  ))
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + for SERVICE in "${DATAPROC_START_AFTER_HDFS_SERVICES[@]}"
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + run_with_retries systemctl start spark-history-server
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + local retry_backoff
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + cmd=("$@")
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + local -a cmd
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + loginfo 'About to run '\''systemctl start spark-history-server'\'' with retries...'
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + echo 'About to run '\''systemctl start spark-history-server'\'' with retries...'
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: About to run 'systemctl start spark-history-server' with retries...
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + local update_succeeded=0
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + (( i = 0 ))
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + (( i < 12 ))
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hadoop-hdfs-namenode[1493]: + systemctl start spark-history-server
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 uninstall[1431]: Purging configuration files for hadoop-yarn-nodemanager (2.9.2-1) ...
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 45.'
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 45.'
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 45.
<13>Oct 13 16:54:29 google-dataproc-startup[837]: <13>Oct 13 16:54:29 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:30 google-dataproc-startup[837]: <13>Oct 13 16:54:30 uninstall[1431]: Processing triggers for systemd (232-25+deb9u12) ...
<13>Oct 13 16:54:30 google-dataproc-startup[837]: <13>Oct 13 16:54:30 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:30 google-dataproc-startup[837]: <13>Oct 13 16:54:30 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:30 google-dataproc-startup[837]: <13>Oct 13 16:54:30 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:30 google-dataproc-startup[837]: <13>Oct 13 16:54:30 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 46.'
<13>Oct 13 16:54:30 google-dataproc-startup[837]: <13>Oct 13 16:54:30 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 46.'
<13>Oct 13 16:54:30 google-dataproc-startup[837]: <13>Oct 13 16:54:30 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 46.
<13>Oct 13 16:54:30 google-dataproc-startup[837]: <13>Oct 13 16:54:30 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:31 google-dataproc-startup[837]: <13>Oct 13 16:54:31 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:31 google-dataproc-startup[837]: <13>Oct 13 16:54:31 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:32 google-dataproc-startup[837]: <13>Oct 13 16:54:32 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:32 google-dataproc-startup[837]: <13>Oct 13 16:54:32 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 47.'
<13>Oct 13 16:54:32 google-dataproc-startup[837]: <13>Oct 13 16:54:32 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 47.'
<13>Oct 13 16:54:32 google-dataproc-startup[837]: <13>Oct 13 16:54:32 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 47.
<13>Oct 13 16:54:32 google-dataproc-startup[837]: <13>Oct 13 16:54:32 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 9083 (tcp) failed: Connection refused
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 48.'
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 48.'
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 failed. Retry attempt: 48.
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: + update_succeeded=1
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: + break
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: + ((  1  ))
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: ++ get_dataproc_property am.primary_only
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: ++ local property_name=am.primary_only
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: +++ local property_name=am.primary_only
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: ++++ tail -n 1
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: ++++ cut -d = -f 2-
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: +++ local property_value=false
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: +++ echo false
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: ++ local property_value=false
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: ++ echo false
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 16:54:33 google-dataproc-startup[837]: <13>Oct 13 16:54:33 setup-hadoop-hdfs-namenode[1493]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 9083
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: Connection to cluster-ea0a-m 9083 port [tcp/*] succeeded!
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + update_succeeded=1
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 9083 succeeded.'
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 9083 succeeded.'
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 9083 succeeded.
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + break
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + ((  1  ))
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + loginfo 'Service up on host=cluster-ea0a-m port=9083.'
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + echo 'Service up on host=cluster-ea0a-m port=9083.'
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: Service up on host=cluster-ea0a-m port=9083.
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + run_with_retries systemctl start hive-server2
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + local retry_backoff
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + cmd=("$@")
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + local -a cmd
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + loginfo 'About to run '\''systemctl start hive-server2'\'' with retries...'
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + echo 'About to run '\''systemctl start hive-server2'\'' with retries...'
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: About to run 'systemctl start hive-server2' with retries...
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + local update_succeeded=0
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + (( i = 0 ))
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + (( i < 12 ))
<13>Oct 13 16:54:34 google-dataproc-startup[837]: <13>Oct 13 16:54:34 setup-hive-metastore[1495]: + systemctl start hive-server2
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + update_succeeded=1
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + break
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + ((  1  ))
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: ++ get_xml_property_or_default /etc/hive/conf/hive-site.xml hive.server2.thrift.port 10000
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: ++ file=/etc/hive/conf/hive-site.xml
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: ++ property=hive.server2.thrift.port
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: ++ default_value=10000
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: +++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.server2.thrift.port
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: ++ val=None
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: ++ [[ None = \N\o\n\e ]]
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: ++ val=10000
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: ++ echo 10000
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + thrift_port=10000
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + wait_for_port cluster-ea0a-m 10000
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + local -r host=cluster-ea0a-m
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + local -r port=10000
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + loginfo 'Waiting for service to come up on host=cluster-ea0a-m port=10000.'
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + echo 'Waiting for service to come up on host=cluster-ea0a-m port=10000.'
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: Waiting for service to come up on host=cluster-ea0a-m port=10000.
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + retry_with_constant_backoff nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + local max_retry=300
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + cmd=("$@")
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + local -a cmd
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + local update_succeeded=0
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: ++ seq 1 300
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 1.'
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 1.'
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 1.
<13>Oct 13 16:54:37 google-dataproc-startup[837]: <13>Oct 13 16:54:37 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:38 google-dataproc-startup[837]: <13>Oct 13 16:54:38 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:38 google-dataproc-startup[837]: <13>Oct 13 16:54:38 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:38 google-dataproc-startup[837]: <13>Oct 13 16:54:38 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:38 google-dataproc-startup[837]: <13>Oct 13 16:54:38 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 2.'
<13>Oct 13 16:54:38 google-dataproc-startup[837]: <13>Oct 13 16:54:38 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 2.'
<13>Oct 13 16:54:38 google-dataproc-startup[837]: <13>Oct 13 16:54:38 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 2.
<13>Oct 13 16:54:38 google-dataproc-startup[837]: <13>Oct 13 16:54:38 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:40 google-dataproc-startup[837]: <13>Oct 13 16:54:40 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:40 google-dataproc-startup[837]: <13>Oct 13 16:54:40 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:40 google-dataproc-startup[837]: <13>Oct 13 16:54:40 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:40 google-dataproc-startup[837]: <13>Oct 13 16:54:40 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 3.'
<13>Oct 13 16:54:40 google-dataproc-startup[837]: <13>Oct 13 16:54:40 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 3.'
<13>Oct 13 16:54:40 google-dataproc-startup[837]: <13>Oct 13 16:54:40 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 3.
<13>Oct 13 16:54:40 google-dataproc-startup[837]: <13>Oct 13 16:54:40 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:41 google-dataproc-startup[837]: <13>Oct 13 16:54:41 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:41 google-dataproc-startup[837]: <13>Oct 13 16:54:41 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:41 google-dataproc-startup[837]: <13>Oct 13 16:54:41 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:41 google-dataproc-startup[837]: <13>Oct 13 16:54:41 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 4.'
<13>Oct 13 16:54:41 google-dataproc-startup[837]: <13>Oct 13 16:54:41 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 4.'
<13>Oct 13 16:54:41 google-dataproc-startup[837]: <13>Oct 13 16:54:41 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 4.
<13>Oct 13 16:54:41 google-dataproc-startup[837]: <13>Oct 13 16:54:41 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:42 google-dataproc-startup[837]: <13>Oct 13 16:54:42 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:42 google-dataproc-startup[837]: <13>Oct 13 16:54:42 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:42 google-dataproc-startup[837]: <13>Oct 13 16:54:42 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:42 google-dataproc-startup[837]: <13>Oct 13 16:54:42 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 5.'
<13>Oct 13 16:54:42 google-dataproc-startup[837]: <13>Oct 13 16:54:42 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 5.'
<13>Oct 13 16:54:42 google-dataproc-startup[837]: <13>Oct 13 16:54:42 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 5.
<13>Oct 13 16:54:42 google-dataproc-startup[837]: <13>Oct 13 16:54:42 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:43 google-dataproc-startup[837]: <13>Oct 13 16:54:43 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:43 google-dataproc-startup[837]: <13>Oct 13 16:54:43 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:43 google-dataproc-startup[837]: <13>Oct 13 16:54:43 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:43 google-dataproc-startup[837]: <13>Oct 13 16:54:43 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 6.'
<13>Oct 13 16:54:43 google-dataproc-startup[837]: <13>Oct 13 16:54:43 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 6.'
<13>Oct 13 16:54:43 google-dataproc-startup[837]: <13>Oct 13 16:54:43 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 6.
<13>Oct 13 16:54:43 google-dataproc-startup[837]: <13>Oct 13 16:54:43 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:44 google-dataproc-startup[837]: <13>Oct 13 16:54:44 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:44 google-dataproc-startup[837]: <13>Oct 13 16:54:44 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:44 google-dataproc-startup[837]: <13>Oct 13 16:54:44 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:44 google-dataproc-startup[837]: <13>Oct 13 16:54:44 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 7.'
<13>Oct 13 16:54:44 google-dataproc-startup[837]: <13>Oct 13 16:54:44 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 7.'
<13>Oct 13 16:54:44 google-dataproc-startup[837]: <13>Oct 13 16:54:44 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 7.
<13>Oct 13 16:54:44 google-dataproc-startup[837]: <13>Oct 13 16:54:44 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:45 google-dataproc-startup[837]: <13>Oct 13 16:54:45 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:45 google-dataproc-startup[837]: <13>Oct 13 16:54:45 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:45 google-dataproc-startup[837]: <13>Oct 13 16:54:45 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:45 google-dataproc-startup[837]: <13>Oct 13 16:54:45 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 8.'
<13>Oct 13 16:54:45 google-dataproc-startup[837]: <13>Oct 13 16:54:45 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 8.'
<13>Oct 13 16:54:45 google-dataproc-startup[837]: <13>Oct 13 16:54:45 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 8.
<13>Oct 13 16:54:45 google-dataproc-startup[837]: <13>Oct 13 16:54:45 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:46 google-dataproc-startup[837]: <13>Oct 13 16:54:46 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:46 google-dataproc-startup[837]: <13>Oct 13 16:54:46 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:46 google-dataproc-startup[837]: <13>Oct 13 16:54:46 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:46 google-dataproc-startup[837]: <13>Oct 13 16:54:46 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 9.'
<13>Oct 13 16:54:46 google-dataproc-startup[837]: <13>Oct 13 16:54:46 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 9.'
<13>Oct 13 16:54:46 google-dataproc-startup[837]: <13>Oct 13 16:54:46 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 9.
<13>Oct 13 16:54:46 google-dataproc-startup[837]: <13>Oct 13 16:54:46 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:47 google-dataproc-startup[837]: <13>Oct 13 16:54:47 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:47 google-dataproc-startup[837]: <13>Oct 13 16:54:47 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:47 google-dataproc-startup[837]: <13>Oct 13 16:54:47 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:47 google-dataproc-startup[837]: <13>Oct 13 16:54:47 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 10.'
<13>Oct 13 16:54:47 google-dataproc-startup[837]: <13>Oct 13 16:54:47 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 10.'
<13>Oct 13 16:54:47 google-dataproc-startup[837]: <13>Oct 13 16:54:47 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 10.
<13>Oct 13 16:54:47 google-dataproc-startup[837]: <13>Oct 13 16:54:47 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:48 google-dataproc-startup[837]: <13>Oct 13 16:54:48 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:48 google-dataproc-startup[837]: <13>Oct 13 16:54:48 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:48 google-dataproc-startup[837]: <13>Oct 13 16:54:48 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:48 google-dataproc-startup[837]: <13>Oct 13 16:54:48 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 11.'
<13>Oct 13 16:54:48 google-dataproc-startup[837]: <13>Oct 13 16:54:48 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 11.'
<13>Oct 13 16:54:48 google-dataproc-startup[837]: <13>Oct 13 16:54:48 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 11.
<13>Oct 13 16:54:48 google-dataproc-startup[837]: <13>Oct 13 16:54:48 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:49 google-dataproc-startup[837]: <13>Oct 13 16:54:49 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:49 google-dataproc-startup[837]: <13>Oct 13 16:54:49 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:49 google-dataproc-startup[837]: <13>Oct 13 16:54:49 setup-hive-metastore[1495]: nc: connect to cluster-ea0a-m port 10000 (tcp) failed: Connection refused
<13>Oct 13 16:54:49 google-dataproc-startup[837]: <13>Oct 13 16:54:49 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 12.'
<13>Oct 13 16:54:49 google-dataproc-startup[837]: <13>Oct 13 16:54:49 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 12.'
<13>Oct 13 16:54:49 google-dataproc-startup[837]: <13>Oct 13 16:54:49 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 failed. Retry attempt: 12.
<13>Oct 13 16:54:49 google-dataproc-startup[837]: <13>Oct 13 16:54:49 setup-hive-metastore[1495]: + sleep 1
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + for i in $(seq 1 ${max_retry})
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + nc -v -z -w 0 cluster-ea0a-m 10000
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: Connection to cluster-ea0a-m 10000 port [tcp/webmin] succeeded!
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + update_succeeded=1
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + loginfo 'nc -v -z -w 0 cluster-ea0a-m 10000 succeeded.'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + echo 'nc -v -z -w 0 cluster-ea0a-m 10000 succeeded.'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: nc -v -z -w 0 cluster-ea0a-m 10000 succeeded.
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + break
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + ((  1  ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + loginfo 'Service up on host=cluster-ea0a-m port=10000.'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + echo 'Service up on host=cluster-ea0a-m port=10000.'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: Service up on host=cluster-ea0a-m port=10000.
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: ++ get_dataproc_property am.primary_only
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: ++ local property_name=am.primary_only
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: +++ local property_name=am.primary_only
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: ++++ cut -d = -f 2-
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: ++++ tail -n 1
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: +++ local property_value=false
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: +++ echo false
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + pid=1494
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + cmd='setup_service hadoop-yarn-resourcemanager'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1494 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + echo 'Waiting on pid=1494 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: Waiting on pid=1494 cmd=[setup_service hadoop-yarn-resourcemanager]
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + wait 1494
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + pid=1493
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + cmd='setup_service hadoop-hdfs-namenode'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1493 cmd=[setup_service hadoop-hdfs-namenode]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + echo 'Waiting on pid=1493 cmd=[setup_service hadoop-hdfs-namenode]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: Waiting on pid=1493 cmd=[setup_service hadoop-hdfs-namenode]
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + wait 1493
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + pid=1438
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + cmd='uninstall_component proxy-agent'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1438 cmd=[uninstall_component proxy-agent]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + echo 'Waiting on pid=1438 cmd=[uninstall_component proxy-agent]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: Waiting on pid=1438 cmd=[uninstall_component proxy-agent]
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + wait 1438
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + pid=1437
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + cmd='uninstall_component presto'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1437 cmd=[uninstall_component presto]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + echo 'Waiting on pid=1437 cmd=[uninstall_component presto]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: Waiting on pid=1437 cmd=[uninstall_component presto]
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + wait 1437
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + pid=1435
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + cmd='uninstall_component kerberos'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1435 cmd=[uninstall_component kerberos]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + echo 'Waiting on pid=1435 cmd=[uninstall_component kerberos]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: Waiting on pid=1435 cmd=[uninstall_component kerberos]
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + wait 1435
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + pid=1433
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + cmd='uninstall_component jupyter'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1433 cmd=[uninstall_component jupyter]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + echo 'Waiting on pid=1433 cmd=[uninstall_component jupyter]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: Waiting on pid=1433 cmd=[uninstall_component jupyter]
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + wait 1433
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + pid=1432
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + cmd='uninstall_component anaconda'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1432 cmd=[uninstall_component anaconda]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + echo 'Waiting on pid=1432 cmd=[uninstall_component anaconda]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: Waiting on pid=1432 cmd=[uninstall_component anaconda]
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + wait 1432
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + pid=1431
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + cmd='bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + loginfo 'Waiting on pid=1431 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + echo 'Waiting on pid=1431 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: Waiting on pid=1431 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + status=0
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + wait 1431
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( status != 0 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( ++i  ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + (( i < 16 ))
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + BACKGROUND_PROCESSES=()
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + BACKGROUND_COMMANDS=()
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + systemctl daemon-reload
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: ++ local property_value=false
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: ++ echo false
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 13 16:54:50 google-dataproc-startup[837]: <13>Oct 13 16:54:50 setup-hive-metastore[1495]: + [[ hive-metastore == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + loginfo 'All done'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: + echo 'All done'
<13>Oct 13 16:54:50 google-dataproc-startup[837]: All done
