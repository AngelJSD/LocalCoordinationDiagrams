+ run_with_logger --tag google-dataproc-startup
+ local tag=
+ local pid=803
+ [[ --tag == \-\-\t\a\g ]]
+ tag=google-dataproc-startup
+ shift 2
+ exec
++ logger -s -t 'google-dataproc-startup[803]'
<13>Oct 28 05:33:14 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=()
<13>Oct 28 05:33:14 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=()
<13>Oct 28 05:33:14 google-dataproc-startup[803]: + cd /tmp
<13>Oct 28 05:33:14 google-dataproc-startup[803]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ INSTALL_GCS_CONNECTOR=1
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ ENABLE_HDFS=1
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ ENABLE_NFS_GCS_FILE_CACHE=1
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ GCS_ADMIN=gcsadmin
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ GCS_CACHE_CLEANER_LOG_DIRECTORY=/var/log/hadoop-hdfs
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,DRFA
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,RFA
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ NUM_WORKERS=10
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ WORKERS=()
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ CORES_PER_MAP_TASK=1.0
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ CORES_PER_APP_MASTER=2.0
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ HDFS_DATA_DIRS_PERM=700
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ COMMON_JAVA_OPTS=("-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance")
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ DATAPROC_VERSION_KEY=1_3
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ declare -A DATAPROC_TO_GCS_CONNECTOR_VERSION
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ DATAPROC_TO_GCS_CONNECTOR_VERSION=(["1_0"]="1.6.10-hadoop2" ["1_1"]="1.6.10-hadoop2" ["1_2"]="1.6.10-hadoop2" ["1_3"]="hadoop2-1.9.17" ["1_4"]="hadoop2-1.9.17" ["1_5"]="hadoop2-1.9.17" ["2_0"]="hadoop3-1.9.17")
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ GCS_CONNECTOR_VERSION=hadoop2-1.9.17
<13>Oct 28 05:33:14 google-dataproc-startup[803]: ++ GCS_CONNECTOR_JAR=https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-1.9.17.jar
<13>Oct 28 05:33:14 google-dataproc-startup[803]: +++ /usr/share/google/get_metadata_value attributes/dataproc_gcs_connector_url
<13>Oct 28 05:33:14 google-dataproc-startup[803]: +++ true
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ GCS_CONNECTOR_JAR_OVERRIDE=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ [[ ! -z '' ]]
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ declare -A DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ DATAPROC_TO_BQ_CONNECTOR_VERSION=(["1_0"]="0.10.11-hadoop2" ["1_1"]="0.10.11-hadoop2" ["1_2"]="0.10.11-hadoop2")
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ BIGQUERY_CONNECTOR_VERSION=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ [[ -n '' ]]
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ /usr/share/google/get_metadata_value attributes/dataproc_bq_connector_url
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ true
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ BIGQUERY_CONNECTOR_JAR_OVERRIDE=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ [[ ! -z '' ]]
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ unset DATAPROC_VERSION_KEY GCS_CONNECTOR_VERSION BIGQUERY_CONNECTOR_VERSION DATAPROC_TO_GCS_CONNECTOR_VERSION DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ dpkg -s hive
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ HIVE_VERSION=2.3.5
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ dpkg -s spark-core
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ SPARK_VERSION=2.3.3
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ readonly APT_SENTINEL=apt.lastupdate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ APT_SENTINEL=apt.lastupdate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + source /usr/local/share/google/dataproc/bdutil/cluster_properties.sh
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + source /usr/local/share/google/dataproc/bdutil/components/startup-script-components.sh
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ readonly COMPONENTS_DIR=/var/lib/google/dataproc/components
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ COMPONENTS_DIR=/var/lib/google/dataproc/components
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + trap logstacktrace ERR
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + loginfo 'Starting Dataproc startup script'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + echo 'Starting Dataproc startup script'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: Starting Dataproc startup script
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ hostname -s
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + MY_HOSTNAME=cluster-4def-m
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ hostname -f
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ dnsdomainname
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + DOMAIN=us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ sed -r 's/-([mw](-[0-9]*)?)$//'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ echo cluster-4def-m
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + PREFIX=cluster-4def
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + merge_java_properties /tmp/cluster/properties/dataproc.properties /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + local dest=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/dataproc.properties ]]
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + cat /tmp/cluster/properties/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: Merged /tmp/cluster/properties/dataproc.properties.
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ local property_name=dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_name=dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_value=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ echo ''
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ local property_value=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ echo ''
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + OPTIONAL_COMPONENTS_VALUE=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + is_version_at_least 1.3 1.4
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + local ver1=1.3.0.0.0.0
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + local ver2=1.4
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + local log=/tmp/tmp.bHvHV06G8a
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + err_code=1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.bHvHV06G8a
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + rm -f /tmp/tmp.bHvHV06G8a
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + DATAPROC_MASTER=cluster-4def-m
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master-additional
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + DATAPROC_MASTER_ADDITIONAL=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + MASTER_COUNT=1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + ((  1 > 1  ))
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + is_component_selected kafka-server
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + local component=kafka-server
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ local property_name=dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_name=dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_value=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ echo ''
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ local property_value=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ echo ''
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + local activated_components=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + [[ '' == *kafka-server* ]]
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + COMPONENTS_TO_ACTIVATE=(${OPTIONAL_COMPONENTS_VALUE})
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + KERBEROS_ENABLED=false
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + is_component_selected kerberos
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + local component=kerberos
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ local property_name=dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_name=dataproc.components.activate
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_value=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ echo ''
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ local property_value=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ echo ''
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + local activated_components=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + [[ '' == *kerberos* ]]
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + [[ false == \t\r\u\e ]]
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ /usr/share/google/get_metadata_value ../project/project-id
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + PROJECT=lustrous-drake-255300
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ /usr/share/google/get_metadata_value attributes/dataproc-bucket
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + CONFIGBUCKET=dataproc-ed3c3d29-fb10-47bb-aca7-dcc358c68973-us-central1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ /usr/share/google/get_metadata_value attributes/dataproc-role
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + ROLE=Master
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-name
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + CLUSTER_NAME=cluster-4def
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-uuid
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + CLUSTER_UUID=066a710f-d5d9-4d64-9809-e3c34d37d020
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ /usr/share/google/get_metadata_value attributes/dataproc-worker-count
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + WORKER_COUNT=7
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + HBASE_CONF_DIR=/etc/hbase/conf
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + PIG_CONF_DIR=/etc/pig/conf
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + SPARK_CONF_DIR=/etc/spark/conf
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + ZOOKEEPER_CONF_DIR=/etc/zookeeper/conf
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + HADOOP_2_PORTS=(50010 50020 50070 50090)
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + ((  1 > 1  ))
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + HDFS_ROOT_URI=hdfs://cluster-4def-m
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + hostname=cluster-4def-m
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + [[ false == \t\r\u\e ]]
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + for i in "${!MASTER_HOSTNAMES[@]}"
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + [[ cluster-4def-m == \c\l\u\s\t\e\r\-\4\d\e\f\-\m ]]
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + MASTER_INDEX=0
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + break
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + ((  7 == 0  ))
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + ((  1 > 1  ))
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + PACKAGES_TO_UNINSTALL=(${DATAPROC_MASTER_HA_SERVICES} ${DATAPROC_WORKER_SERVICES})
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + SERVICES=(${DATAPROC_MASTER_SERVICES} ${DATAPROC_MASTER_EXCLUSIVE_SERVICES} ${DATAPROC_MASTER_STANDALONE_SERVICES})
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + loginfo 'Generating helper scripts'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + echo 'Generating helper scripts'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: Generating helper scripts
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + sudo -u hdfs hdfs namenode -genclusterid
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ cat /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ (( i=0 ))
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ (( i<1 ))
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ echo MASTER_HOSTNAME_0=cluster-4def-m
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ (( i++  ))
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ (( i<1 ))
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ cat /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ cat /usr/local/share/google/dataproc/bdutil/setup_master_nfs.sh /usr/local/share/google/dataproc/bdutil/setup_client_nfs.sh
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ cat /usr/local/share/google/dataproc/bdutil/configure_hadoop.sh /usr/local/share/google/dataproc/bdutil/configure_mysql.sh /usr/local/share/google/dataproc/bdutil/configure_hive.sh /usr/local/share/google/dataproc/bdutil/configure_hdfs.sh /usr/local/share/google/dataproc/bdutil/configure_connectors.sh /usr/local/share/google/dataproc/bdutil/configure_spark.sh /usr/local/share/google/dataproc/bdutil/configure_tez.sh /usr/local/share/google/dataproc/bdutil/configure_zookeeper.sh
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + cp -r /usr/local/share/google/dataproc/bdutil/conf/bq-mapred-template.xml /usr/local/share/google/dataproc/bdutil/conf/capacity-scheduler-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-template.xml /usr/local/share/google/dataproc/bdutil/conf/distcp-template.xml /usr/local/share/google/dataproc/bdutil/conf/gcs-core-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-template.xml /usr/local/share/google/dataproc/bdutil/conf/hive-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hive-template.xml /usr/local/share/google/dataproc/bdutil/conf/knox /usr/local/share/google/dataproc/bdutil/conf/mapred-template.xml /usr/local/share/google/dataproc/b
<13>Oct 28 05:33:15 google-dataproc-startup[803]: dutil/conf/yarn-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-template.xml /tmp
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + cp /usr/local/share/google/dataproc/bdutil/configure_mrv2_mem.py /tmp
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + chmod +x configure_mrv2_mem.py
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + loginfo 'Running helper scripts'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + echo 'Running helper scripts'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: Running helper scripts
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ get_dataproc_property dataproc.localssd.mount.enable
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ local property_name=dataproc.localssd.mount.enable
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.localssd.mount.enable
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_name=dataproc.localssd.mount.enable
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++++ grep '^dataproc.localssd.mount.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ local property_value=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: +++ echo ''
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ local property_value=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: ++ echo ''
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + MOUNT_DISKS_ENABLED=
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + [[ '' == \f\a\l\s\e ]]
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + DATAPROC_MOUNT_SERVICE_FILE=/usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + chmod +x /usr/local/share/google/dataproc/bdutil/mount_disks.sh
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + chmod 644 /usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + systemctl enable google-dataproc-disk-mount
<13>Oct 28 05:33:15 google-dataproc-startup[803]: Created symlink /etc/systemd/system/multi-user.target.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 28 05:33:15 google-dataproc-startup[803]: Created symlink /etc/systemd/system/hadoop-hdfs-namenode.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 28 05:33:15 google-dataproc-startup[803]: Created symlink /etc/systemd/system/hadoop-hdfs-datanode.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 28 05:33:15 google-dataproc-startup[803]: Created symlink /etc/systemd/system/hadoop-yarn-resourcemanager.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 28 05:33:15 google-dataproc-startup[803]: Created symlink /etc/systemd/system/hadoop-yarn-nodemanager.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 28 05:33:15 google-dataproc-startup[803]: + systemctl start google-dataproc-disk-mount
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + in_array nfs-kernel-server DATAPROC_MASTER_EXCLUSIVE_SERVICES
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + local value=nfs-kernel-server
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + local -n values=DATAPROC_MASTER_EXCLUSIVE_SERVICES
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + [[ !  hadoop-mapreduce-historyserver spark-history-server hive-webhcat-server jupyter knox proxy-agent zeppelin hadoop-yarn-timelineserver mariadb-server  =~  nfs-kernel-server  ]]
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + bash configuration_script.sh
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ INSTALL_GCS_CONNECTOR=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ ENABLE_HDFS=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ ENABLE_NFS_GCS_FILE_CACHE=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ GCS_ADMIN=gcsadmin
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ GCS_CACHE_CLEANER_LOG_DIRECTORY=/var/log/hadoop-hdfs
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,DRFA
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,RFA
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ NUM_WORKERS=10
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ WORKERS=()
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ CORES_PER_MAP_TASK=1.0
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ CORES_PER_APP_MASTER=2.0
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ HDFS_DATA_DIRS_PERM=700
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ COMMON_JAVA_OPTS=("-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance")
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ DATAPROC_VERSION_KEY=1_3
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ declare -A DATAPROC_TO_GCS_CONNECTOR_VERSION
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ DATAPROC_TO_GCS_CONNECTOR_VERSION=(["1_0"]="1.6.10-hadoop2" ["1_1"]="1.6.10-hadoop2" ["1_2"]="1.6.10-hadoop2" ["1_3"]="hadoop2-1.9.17" ["1_4"]="hadoop2-1.9.17" ["1_5"]="hadoop2-1.9.17" ["2_0"]="hadoop3-1.9.17")
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ GCS_CONNECTOR_VERSION=hadoop2-1.9.17
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ GCS_CONNECTOR_JAR=https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-1.9.17.jar
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ /usr/share/google/get_metadata_value attributes/dataproc_gcs_connector_url
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ true
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ GCS_CONNECTOR_JAR_OVERRIDE=
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ [[ ! -z '' ]]
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ declare -A DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ DATAPROC_TO_BQ_CONNECTOR_VERSION=(["1_0"]="0.10.11-hadoop2" ["1_1"]="0.10.11-hadoop2" ["1_2"]="0.10.11-hadoop2")
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ BIGQUERY_CONNECTOR_VERSION=
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ [[ -n '' ]]
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ /usr/share/google/get_metadata_value attributes/dataproc_bq_connector_url
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ true
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ BIGQUERY_CONNECTOR_JAR_OVERRIDE=
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ [[ ! -z '' ]]
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ unset DATAPROC_VERSION_KEY GCS_CONNECTOR_VERSION BIGQUERY_CONNECTOR_VERSION DATAPROC_TO_GCS_CONNECTOR_VERSION DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ dpkg -s hive
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ HIVE_VERSION=2.3.5
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ dpkg -s spark-core
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ SPARK_VERSION=2.3.3
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ CLUSTER_NAME=cluster-4def
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ CLUSTER_UUID=066a710f-d5d9-4d64-9809-e3c34d37d020
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ CONFIGBUCKET=dataproc-ed3c3d29-fb10-47bb-aca7-dcc358c68973-us-central1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ HDFS_ROOT_URI=hdfs://cluster-4def-m
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ MASTER_HOSTNAME_0=cluster-4def-m
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ MASTER_HOSTNAMES=(cluster-4def-m)
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ NUM_MASTERS=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ NUM_WORKERS=7
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ PREFIX=cluster-4def
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ PROJECT=lustrous-drake-255300
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ ROLE=Master
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ set +a
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ readonly APT_SENTINEL=apt.lastupdate
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ APT_SENTINEL=apt.lastupdate
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + set -e
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + loginfo 'Running configure_hadoop.sh'
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + echo 'Running configure_hadoop.sh'
<13>Oct 28 05:33:16 google-dataproc-startup[803]: Running configure_hadoop.sh
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + export HADOOP_TMP_DIR=/hadoop/tmp
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + HADOOP_TMP_DIR=/hadoop/tmp
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + mkdir -p /hadoop/tmp
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + export DEFAULT_NUM_MAPS=70
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + DEFAULT_NUM_MAPS=70
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + export DEFAULT_NUM_REDUCES=28
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + DEFAULT_NUM_REDUCES=28
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ grep -c processor /proc/cpuinfo
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + export NUM_CORES=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + NUM_CORES=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ python -c 'print int(1 //     1.0)'
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + export MAP_SLOTS=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + MAP_SLOTS=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ python -c 'print int(1 //     2.0)'
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + export REDUCE_SLOTS=0
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + REDUCE_SLOTS=0
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ awk '/^Mem:/{print $2}'
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ free -m
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + TOTAL_MEM=3704
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ python -c 'print int(3704 *     0.4)'
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + HADOOP_MR_MASTER_MEM_MB=1481
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + [[ -x configure_mrv2_mem.py ]]
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ mktemp /tmp/mrv2_XXX_tmp_env.sh
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + TEMP_ENV_FILE=/tmp/mrv2_q4x_tmp_env.sh
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + ./configure_mrv2_mem.py --output_file /tmp/mrv2_q4x_tmp_env.sh --total_memory 3704 --available_memory_ratio 0.8 --total_cores 1 --cores_per_map 1.0 --cores_per_reduce 2.0 --cores_per_app_master 2.0
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + source /tmp/mrv2_q4x_tmp_env.sh
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export YARN_MIN_MEM_MB=256
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ YARN_MIN_MEM_MB=256
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export YARN_MAX_MEM_MB=2816
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ YARN_MAX_MEM_MB=2816
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export NODEMANAGER_MEM_MB=2816
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ NODEMANAGER_MEM_MB=2816
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export APP_MASTER_MEM_MB=2816
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ APP_MASTER_MEM_MB=2816
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export CORES_PER_APP_MASTER_ROUNDED=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ CORES_PER_APP_MASTER_ROUNDED=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export APP_MASTER_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ APP_MASTER_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export MAP_MEM_MB=2816
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ MAP_MEM_MB=2816
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ CORES_PER_MAP_ROUNDED=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export MAP_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ MAP_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export REDUCE_MEM_MB=2816
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ REDUCE_MEM_MB=2816
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export CORES_PER_REDUCE_ROUNDED=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ CORES_PER_REDUCE_ROUNDED=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ export REDUCE_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ REDUCE_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ python -c 'print int(3704 / 4)'
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + HADOOP_CLIENT_MEM_MB=926
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + is_version_at_least 1.3 1.4
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + local ver1=1.3.0.0.0.0
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + local ver2=1.4
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + local log=/tmp/tmp.E1XQnXRsjR
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + err_code=1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.E1XQnXRsjR
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + rm -f /tmp/tmp.E1XQnXRsjR
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + DATA_DIRS=($(get_data_dirs))
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ get_data_dirs
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ mount_points=($(find /mnt/[0-9]*/ -maxdepth 0 || true))
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Oct 28 05:33:16 google-dataproc-startup[803]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ true
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ local mount_points
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ ((  0  ))
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ echo /
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ return
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + mkdir -p /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + chgrp hadoop -L -R /hadoop /hadoop/tmp /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + chmod g+rwx -R /hadoop /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + chmod 777 -R /hadoop/tmp
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + export MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + export NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + YARN_ENV_FILE=/etc/hadoop/conf/yarn-env.sh
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + [[ -f /etc/hadoop/conf/yarn-env.sh ]]
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:16 google-dataproc-startup[803]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ get_dataproc_property simplified.scaling.enable
<13>Oct 28 05:33:16 google-dataproc-startup[803]: ++ local property_name=simplified.scaling.enable
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:16 google-dataproc-startup[803]: +++ local property_name=simplified.scaling.enable
<13>Oct 28 05:33:17 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:17 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:17 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:17 google-dataproc-startup[803]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:17 google-dataproc-startup[803]: +++ local property_value=
<13>Oct 28 05:33:17 google-dataproc-startup[803]: +++ echo ''
<13>Oct 28 05:33:17 google-dataproc-startup[803]: ++ local property_value=
<13>Oct 28 05:33:17 google-dataproc-startup[803]: ++ echo ''
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + [[ '' == \t\r\u\e ]]
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + touch /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + chown root:hadoop /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + [[ 1 -gt 1 ]]
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + CORE_TEMPLATE=core-template.xml
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + YARN_TEMPLATE=yarn-template.xml
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file core-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file mapred-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file yarn-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file capacity-scheduler-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file distcp-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + is_version_at_least 1.3 1.4
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + local ver1=1.3.0.0.0.0
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + local ver2=1.4
<13>Oct 28 05:33:17 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + local log=/tmp/tmp.OSTmfgOaiS
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + err_code=1
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.OSTmfgOaiS
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + rm -f /tmp/tmp.OSTmfgOaiS
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value false --clobber
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + ZK_QUORUM=cluster-4def-m:2181,:2181,:2181
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + [[ 1 -gt 1 ]]
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + is_version_at_least 1.3 1.2
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + local ver1=1.3.0.0.0.0
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + local ver2=1.2
<13>Oct 28 05:33:17 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + local log=/tmp/tmp.0wMgfS64kp
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.2
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + err_code=0
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.0wMgfS64kp
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + rm -f /tmp/tmp.0wMgfS64kp
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + return 0
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.file.close.num-committed-allowed --value 1 --clobber
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + is_version_at_least 1.3 1.3
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + local ver1=1.3.0.0.0.0
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + local ver2=1.3
<13>Oct 28 05:33:17 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + local log=/tmp/tmp.V3NA4cBxat
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + err_code=0
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.V3NA4cBxat
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + rm -f /tmp/tmp.V3NA4cBxat
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + return 0
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/core-site.xml --name hadoop.http.filter.initializers --value org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter --clobber
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.webapp.cross-origin.enabled --value true --clobber
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.http-cross-origin.enabled --value true --clobber
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.enabled --value true --clobber
<13>Oct 28 05:33:17 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.hostname --value cluster-4def-m --clobber
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.bind-host --value 0.0.0.0 --clobber
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.system-metrics-publisher.enabled --value true --clobber
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.generic-application-history.enabled --value true --clobber
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ get_dataproc_property am.primary_only
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ local property_name=am.primary_only
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ local property_name=am.primary_only
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ local property_value=false
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ echo false
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ local property_value=false
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ echo false
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ /usr/share/google/get_metadata_value attributes/dataproc-datanode-enabled
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + DATAPROC_DATANODE_ENABLED=true
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ false == \t\r\u\e ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ '' == \t\r\u\e ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + set -e -x
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + xargs -n1 sed -i 's/^\(bind-address\)\s*=.*/\1 = 0.0.0.0/'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + grep -lr bind-address /etc/mysql
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + set -e -x
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + is_version_at_least 1.3 1.3
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver1=1.3.0.0.0.0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver2=1.3
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local log=/tmp/tmp.SV8f1SnAXw
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + err_code=0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.SV8f1SnAXw
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + rm -f /tmp/tmp.SV8f1SnAXw
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + return 0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + is_version_at_least 1.3 1.4
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver1=1.3.0.0.0.0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver2=1.4
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local log=/tmp/tmp.jUDDE1fyvB
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + err_code=1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.jUDDE1fyvB
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + rm -f /tmp/tmp.jUDDE1fyvB
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ 1 -gt 1 ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + METASTORE_URIS=thrift://cluster-4def-m:9083
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris --value thrift://cluster-4def-m:9083 --clobber
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + METADATA_STORE=jdbc:mysql://cluster-4def-m/metastore
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name javax.jdo.option.ConnectionURL --value jdbc:mysql://cluster-4def-m/metastore --clobber
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ 1 -gt 1 ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + set -e
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + loginfo 'Running configure_hdfs.sh'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + echo 'Running configure_hdfs.sh'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: Running configure_hdfs.sh
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + HDFS_ADMIN=hdfs
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + export HDFS_NAME_DIR=/hadoop/dfs/name
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + HDFS_NAME_DIR=/hadoop/dfs/name
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + export HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + DATA_DIRS=($(get_data_dirs))
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ get_data_dirs
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ mount_points=($(find /mnt/[0-9]*/ -maxdepth 0 || true))
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ true
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ local mount_points
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ ((  0  ))
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ echo /
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ return
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + mkdir -p /hadoop/dfs /hadoop/dfs/data
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + chown hdfs:hadoop -L -R /hadoop/dfs /hadoop/dfs/data
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + chmod 700 /hadoop/dfs/data
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ awk '/^Mem:/{print $2}'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ free -m
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + TOTAL_MEM=3704
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ python -c 'print int(3704 *     0.4 / 2)'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + NAMENODE_MEM_MB=740
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SECONDARYNAMENODE_MEM_MB=740
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + export HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ 1 -gt 1 ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + TEMPLATE=hdfs-template.xml
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file hdfs-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + ((  7 == 0  ))
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ get_java_property /tmp/cluster/properties/dataproc.properties simplified.scaling.enable
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ local property_file=/tmp/cluster/properties/dataproc.properties
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ local property_name=simplified.scaling.enable
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ cut -d = -f 2-
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ tail -n 1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ grep '^simplified.scaling.enable=' /tmp/cluster/properties/dataproc.properties
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ local property_value=
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ echo ''
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ '' == \t\r\u\e ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + set -e
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + loginfo 'Running configure_connectors.sh'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + echo 'Running configure_connectors.sh'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: Running configure_connectors.sh
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + ((  1  ))
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + export GCS_METADATA_CACHE_TYPE=FILESYSTEM_BACKED
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + GCS_METADATA_CACHE_TYPE=FILESYSTEM_BACKED
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ get_nfs_mount_point
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ echo /hadoop_gcs_connector_metadata_cache
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + export GCS_FILE_CACHE_DIRECTORY=/hadoop_gcs_connector_metadata_cache
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + GCS_FILE_CACHE_DIRECTORY=/hadoop_gcs_connector_metadata_cache
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file gcs-core-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ hostname -s
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ cluster-4def-m == \c\l\u\s\t\e\r\-\4\d\e\f\-\m ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ 1 -ne 0 ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + setup_cache_cleaner
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + mkdir -p /usr/lib/hadoop/google
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local cleaner=/usr/lib/hadoop/google/clean-caches.sh
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + make_cache_cleaner_script
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local gc_cleaner=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemCacheCleaner
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local etab=/var/lib/nfs/etab
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + chmod 755 /usr/lib/hadoop/google/clean-caches.sh
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + make_cleaner_crontab /usr/lib/hadoop/google/clean-caches.sh
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local cleaner=/usr/lib/hadoop/google/clean-caches.sh
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file bq-mapred-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + set -e
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + set -o nounset
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + loginfo 'Running configure_spark.sh'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + echo 'Running configure_spark.sh'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: Running configure_spark.sh
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_EVENTLOG_DIR=hdfs://cluster-4def-m/user/spark/eventlog
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_TMPDIR=/hadoop/spark/tmp
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_WORKDIR=/hadoop/spark/work
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_LOG_DIR=/var/log/spark
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + mkdir -p /hadoop/spark/tmp /hadoop/spark/work /var/log/spark
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + chown spark:spark -R /hadoop/spark /var/log/spark
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + chmod 1777 -R /hadoop/spark /var/log/spark
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + is_version_at_least 2.3.3 2
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver1=2.3.3.0.0.0.0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver2=2
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local log=/tmp/tmp.VIDwdci8Vv
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + dpkg --compare-versions 2.3.3.0.0.0.0 '>=' 2
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + err_code=0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.VIDwdci8Vv
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + rm -f /tmp/tmp.VIDwdci8Vv
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + return 0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + RPC_SIZE_KEY=spark.rpc.message.maxSize
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + NUM_INITIAL_EXECUTORS_KEY=spark.executor.instances
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_YARN_DIR=/usr/lib/spark/yarn
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARKR_LIB_DIR=/usr/lib/spark/R/lib
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ -f /usr/lib/spark/R/lib/sparkr.zip ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ awk '/^Mem:/{print $2}'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ free -m
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + TOTAL_MEM=3704
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ python -c 'print int(3704 * 0.15)'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_DAEMON_MEMORY=555
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ python -c 'print int(3704 / 4)'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_DRIVER_MEM_MB=926
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ python -c 'print int(926 / 2)'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_DRIVER_MAX_RESULT_MB=463
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ head -1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ find /tmp/mrv2_q4x_tmp_env.sh
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + YARN_MEMORY_ENV=/tmp/mrv2_q4x_tmp_env.sh
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + source /tmp/mrv2_q4x_tmp_env.sh
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export YARN_MIN_MEM_MB=256
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ YARN_MIN_MEM_MB=256
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export YARN_MAX_MEM_MB=2816
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ YARN_MAX_MEM_MB=2816
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export NODEMANAGER_MEM_MB=2816
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ NODEMANAGER_MEM_MB=2816
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export APP_MASTER_MEM_MB=2816
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ APP_MASTER_MEM_MB=2816
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export CORES_PER_APP_MASTER_ROUNDED=1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ CORES_PER_APP_MASTER_ROUNDED=1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export APP_MASTER_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ APP_MASTER_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export MAP_MEM_MB=2816
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ MAP_MEM_MB=2816
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ CORES_PER_MAP_ROUNDED=1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export MAP_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ MAP_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export REDUCE_MEM_MB=2816
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ REDUCE_MEM_MB=2816
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export CORES_PER_REDUCE_ROUNDED=1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ CORES_PER_REDUCE_ROUNDED=1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ export REDUCE_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ REDUCE_JAVA_OPTS=-Xmx2252m
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ python
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ cat
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_EXECUTOR_MEMORY=1280
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ python -c 'print max(1,     1 / 2)'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_EXECUTOR_CORES=1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ python -c 'print int(max(     1280 / 11, 384))'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_YARN_EXECUTOR_MEMORY_OVERHEAD=384
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + SPARK_EXECUTOR_MEMORY=896
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + is_version_at_least 1.3 1.4
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver1=1.3.0.0.0.0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver2=1.4
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local log=/tmp/tmp.gcphNSCdq8
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + err_code=1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.gcphNSCdq8
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + rm -f /tmp/tmp.gcphNSCdq8
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + is_version_at_least 1.3 1.3
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver1=1.3.0.0.0.0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver2=1.3
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local log=/tmp/tmp.zITlX5kzJ7
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + err_code=0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.zITlX5kzJ7
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + rm -f /tmp/tmp.zITlX5kzJ7
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + return 0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + set -e
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + loginfo 'Running configure_tez.sh'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + echo 'Running configure_tez.sh'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: Running configure_tez.sh
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + readonly CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + is_version_at_least 1.3 1.3
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver1=1.3.0.0.0.0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local ver2=1.3
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local log=/tmp/tmp.xNbOD5fKre
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + err_code=0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.xNbOD5fKre
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + rm -f /tmp/tmp.xNbOD5fKre
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + return 0
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ ls /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + configure_war /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local -r tez_war=/usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ mktemp -d
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local -r tmp_dir=/tmp/tmp.iya6DbPWHT
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + unzip -q /usr/lib/tez/tez-ui-0.9.2.war -d /tmp/tmp.iya6DbPWHT
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local -r tez_configs=/tmp/tmp.iya6DbPWHT/config/configs.env
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ cut -d ' ' -f 1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ md5sum /tmp/tmp.iya6DbPWHT/config/configs.env
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ 23fbfca8f7b8e142395c6bb4676427ae != \2\3\f\b\f\c\a\8\f\7\b\8\e\1\4\2\3\9\5\c\6\b\b\4\6\7\6\4\2\7\a\e ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ -f /tmp/tmp.iya6DbPWHT/config/configs.env ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + sed -i 's#\(.*\)//timeline: "http://localhost:8188"\(.*\)#\1timeline: "http://cluster-4def-m:8188"\2#' /tmp/tmp.iya6DbPWHT/config/configs.env
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + sed -i 's#\(.*\)//rm: "http://localhost:8088"\(.*\)#\1rm: "http://cluster-4def-m:8088"\2#' /tmp/tmp.iya6DbPWHT/config/configs.env
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ local property_name=dataproc.components.activate
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ local property_name=dataproc.components.activate
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ local property_value=
<13>Oct 28 05:33:18 google-dataproc-startup[803]: +++ echo ''
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ local property_value=
<13>Oct 28 05:33:18 google-dataproc-startup[803]: ++ echo ''
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + local -r optional_components_value=
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + [[ '' == *\k\n\o\x* ]]
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + cd /tmp/tmp.iya6DbPWHT
<13>Oct 28 05:33:18 google-dataproc-startup[803]: + zip -q /usr/lib/tez/tez-ui-0.9.2.war -r ./assets ./config ./fonts ./index.html ./META-INF ./WEB-INF
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + cd ..
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + rm -rf /tmp/tmp.iya6DbPWHT
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++ stat /usr/lib/tez/tez-common-0.9.2.jar --format=%Y
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + touch -d @1568819629 /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-names --value tez --clobber
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-on-disk-path.tez --value /usr/lib/tez/tez-ui-0.9.2.war --clobber
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-web-path.tez --value /tez-ui --clobber
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.history.logging.service.class --value org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService --clobber
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.tez-ui.history-url.base --value http://cluster-4def-m:8188/tez-ui/ --clobber
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.am.node-blacklisting.enabled --value false --clobber
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + set -e
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + declare -r ZOOKEEPER_CONFIG=/etc/zookeeper/conf/zoo.cfg
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + declare -r ZOOKEEPER_DATA_DIR=/var/lib/zookeeper/
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i=0 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i<1 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo server.0=cluster-4def-m:2888:3888
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i++  ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i<1 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo autopurge.purgeInterval=168
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++ sed -e 's/.*-m-//'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++ uname -n
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + declare -r MY_ID=cluster-4def-m
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo cluster-4def-m
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + [[ false == \t\r\u\e ]]
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + loginfo 'Populating initial cluster member list'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo 'Populating initial cluster member list'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: Populating initial cluster member list
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++ get_dataproc_property dataproc.worker.custom.init.actions.mode
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++ local property_name=dataproc.worker.custom.init.actions.mode
<13>Oct 28 05:33:19 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.worker.custom.init.actions.mode
<13>Oct 28 05:33:19 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:19 google-dataproc-startup[803]: +++ local property_name=dataproc.worker.custom.init.actions.mode
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++++ grep '^dataproc.worker.custom.init.actions.mode=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:19 google-dataproc-startup[803]: +++ local property_value=
<13>Oct 28 05:33:19 google-dataproc-startup[803]: +++ echo ''
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++ local property_value=
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++ echo ''
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + WORKER_CUSTOM_INIT_ACTIONS_MODE=
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + WORKER_COUNT=7
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++ get_dataproc_property simplified.scaling.enable
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++ local property_name=simplified.scaling.enable
<13>Oct 28 05:33:19 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Oct 28 05:33:19 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:19 google-dataproc-startup[803]: +++ local property_name=simplified.scaling.enable
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:19 google-dataproc-startup[803]: +++ local property_value=
<13>Oct 28 05:33:19 google-dataproc-startup[803]: +++ echo ''
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++ local property_value=
<13>Oct 28 05:33:19 google-dataproc-startup[803]: ++ echo ''
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + [[ '' != \t\r\u\e ]]
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + ((  7 == 0  ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + [[ '' == \R\U\N\_\B\E\F\O\R\E\_\S\E\R\V\I\C\E\S ]]
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + MEMBERSHIP_FILE=/etc/hadoop/conf/nodes_include
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i=0 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i<7 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo cluster-4def-w-0.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i++  ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i<7 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo cluster-4def-w-1.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i++  ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i<7 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo cluster-4def-w-2.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i++  ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i<7 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo cluster-4def-w-3.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i++  ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i<7 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo cluster-4def-w-4.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i++  ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i<7 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo cluster-4def-w-5.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i++  ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i<7 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo cluster-4def-w-6.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i++  ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + (( i<7 ))
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + loginfo 'Merging user-specified cluster properties'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo 'Merging user-specified cluster properties'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: Merging user-specified cluster properties
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + merge_xml_properties /tmp/cluster/properties/capacity-scheduler.xml /etc/hadoop/conf/capacity-scheduler.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/capacity-scheduler.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + local dest=/etc/hadoop/conf/capacity-scheduler.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/capacity-scheduler.xml ]]
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file /tmp/cluster/properties/capacity-scheduler.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: Merged /tmp/cluster/properties/capacity-scheduler.xml.
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + merge_xml_properties /tmp/cluster/properties/core.xml /etc/hadoop/conf/core-site.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/core.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + local dest=/etc/hadoop/conf/core-site.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/core.xml ]]
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file /tmp/cluster/properties/core.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/core.xml.'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/core.xml.'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: Merged /tmp/cluster/properties/core.xml.
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + merge_xml_properties /tmp/cluster/properties/distcp.xml /etc/hadoop/conf/distcp-default.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/distcp.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + local dest=/etc/hadoop/conf/distcp-default.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/distcp.xml ]]
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file /tmp/cluster/properties/distcp.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: Merged /tmp/cluster/properties/distcp.xml.
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + merge_xml_properties /tmp/cluster/properties/hdfs.xml /etc/hadoop/conf/hdfs-site.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/hdfs.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + local dest=/etc/hadoop/conf/hdfs-site.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/hdfs.xml ]]
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /tmp/cluster/properties/hdfs.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Oct 28 05:33:19 google-dataproc-startup[803]: Merged /tmp/cluster/properties/hdfs.xml.
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + merge_xml_properties /tmp/cluster/properties/mapred.xml /etc/hadoop/conf/mapred-site.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/mapred.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + local dest=/etc/hadoop/conf/mapred-site.xml
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/mapred.xml ]]
<13>Oct 28 05:33:19 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file /tmp/cluster/properties/mapred.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Merged /tmp/cluster/properties/mapred.xml.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + merge_xml_properties /tmp/cluster/properties/yarn.xml /etc/hadoop/conf/yarn-site.xml
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/yarn.xml
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local dest=/etc/hadoop/conf/yarn-site.xml
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/yarn.xml ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /tmp/cluster/properties/yarn.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Merged /tmp/cluster/properties/yarn.xml.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + merge_xml_properties /tmp/cluster/properties/hive.xml /etc/hive/conf/hive-site.xml
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/hive.xml
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local dest=/etc/hive/conf/hive-site.xml
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/hive.xml ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/hive/conf/hive-site.xml --source_configuration_file /tmp/cluster/properties/hive.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Merged /tmp/cluster/properties/hive.xml.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + merge_java_properties /tmp/cluster/properties/pig.properties /etc/pig/conf/pig.properties
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/pig.properties
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local dest=/etc/pig/conf/pig.properties
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/pig.properties ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + cat /tmp/cluster/properties/pig.properties
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Merged /tmp/cluster/properties/pig.properties.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + merge_java_properties /tmp/cluster/properties/spark.properties /etc/spark/conf/spark-defaults.conf
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/spark.properties
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local dest=/etc/spark/conf/spark-defaults.conf
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/spark.properties ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + cat /tmp/cluster/properties/spark.properties
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Merged /tmp/cluster/properties/spark.properties.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + merge_java_properties /tmp/cluster/properties/zookeeper.properties /etc/zookeeper/conf/zoo.cfg
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/zookeeper.properties
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local dest=/etc/zookeeper/conf/zoo.cfg
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/zookeeper.properties ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + cat /tmp/cluster/properties/zookeeper.properties
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Merged /tmp/cluster/properties/zookeeper.properties.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + merge_sh_env_vars /tmp/cluster/properties/hadoop-env.sh /etc/hadoop/conf/hadoop-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/hadoop-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local dest=/etc/hadoop/conf/hadoop-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/hadoop-env.sh ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + cat /tmp/cluster/properties/hadoop-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Merged /tmp/cluster/properties/hadoop-env.sh.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + merge_sh_env_vars /tmp/cluster/properties/mapred-env.sh /etc/hadoop/conf/mapred-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/mapred-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local dest=/etc/hadoop/conf/mapred-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/mapred-env.sh ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + cat /tmp/cluster/properties/mapred-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Merged /tmp/cluster/properties/mapred-env.sh.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + merge_sh_env_vars /tmp/cluster/properties/spark-env.sh /etc/spark/conf/spark-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/spark-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local dest=/etc/spark/conf/spark-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/spark-env.sh ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + cat /tmp/cluster/properties/spark-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Merged /tmp/cluster/properties/spark-env.sh.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + merge_sh_env_vars /tmp/cluster/properties/yarn-env.sh /etc/hadoop/conf/yarn-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/yarn-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local dest=/etc/hadoop/conf/yarn-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/yarn-env.sh ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + cat /tmp/cluster/properties/yarn-env.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Merged /tmp/cluster/properties/yarn-env.sh.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ -f /etc/hbase/conf/hbase-site.xml ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ -f /etc/tez/conf/tez-site.xml ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + merge_xml_properties /tmp/cluster/properties/tez.xml /etc/tez/conf/tez-site.xml
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local src=/tmp/cluster/properties/tez.xml
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local dest=/etc/tez/conf/tez-site.xml
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ ! -f /tmp/cluster/properties/tez.xml ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + bdconfig merge_configurations --configuration_file /etc/tez/conf/tez-site.xml --source_configuration_file /tmp/cluster/properties/tez.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Merged /tmp/cluster/properties/tez.xml.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ false == \t\r\u\e ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + ACTIVATABLE_COMPONENTS=(${DATAPROC_OPTIONAL_COMPONENTS})
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + DATAPROC_NON_DEBIAN_COMPONENTS=(${DATAPROC_NON_DEBIAN_COMPONENTS})
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PACKAGES_TO_KEEP=("${SERVICES[@]}" ${DATAPROC_COMMON_PACKAGES})
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + DATAPROC_START_AFTER_HDFS_SERVICES=(${DATAPROC_START_AFTER_HDFS_SERVICES})
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + COMPONENTS_TO_ACTIVATE=($(intersection COMPONENTS_TO_ACTIVATE PACKAGES_TO_KEEP))
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ intersection COMPONENTS_TO_ACTIVATE PACKAGES_TO_KEEP
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local -n values=COMPONENTS_TO_ACTIVATE
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local -n filter=PACKAGES_TO_KEEP
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ sort -u
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ sort -u
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ printf '%s\n' hadoop-hdfs-namenode hadoop-yarn-resourcemanager hive-metastore hive-server2 zookeeper-server solr-server hadoop-mapreduce-historyserver spark-history-server hive-webhcat-server jupyter knox proxy-agent zeppelin hadoop-yarn-timelineserver mariadb-server hadoop-hdfs-secondarynamenode openjdk-8-jdk openjdk-8-dbg libjansi-java python-numpy libmysql-java hadoop-client hive pig spark-core spark-python spark-r autofs libhdfs0 libsnappy-dev libzstd-dev libatlas3-base libopenblas-base libapr1 vim git bash-completion spark-yarn-shuffle spark-datanucleus spark-extras hadoop-lzo python-setuptools anaconda druid kafka-server kerberos presto openssl tez hive-hcatalog
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ printf '%s\n'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + NON_ACTIVATED_COMPONENTS=($(difference ACTIVATABLE_COMPONENTS COMPONENTS_TO_ACTIVATE))
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ difference ACTIVATABLE_COMPONENTS COMPONENTS_TO_ACTIVATE
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local -n filter=COMPONENTS_TO_ACTIVATE
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ sort -u
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ sort -u
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ printf '%s\n'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ printf '%s\n' zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PACKAGES_TO_UNINSTALL+=($(difference NON_ACTIVATED_COMPONENTS PACKAGES_TO_UNINSTALL))
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ difference NON_ACTIVATED_COMPONENTS PACKAGES_TO_UNINSTALL
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local -n values=NON_ACTIVATED_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local -n filter=PACKAGES_TO_UNINSTALL
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ sort -u
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ sort -u
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ printf '%s\n' anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + NON_DEBIAN_COMPONENTS_TO_UNINSTALL=($(intersection PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS))
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ intersection PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local -n values=PACKAGES_TO_UNINSTALL
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local -n filter=DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ sort -u
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ sort -u
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ printf '%s\n' anaconda jupyter kerberos presto proxy-agent
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + DEBIAN_COMPONENTS_TO_UNINSTALL=($(difference PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS))
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ difference PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local -n values=PACKAGES_TO_UNINSTALL
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local -n filter=DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ sort -u
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ sort -u
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ printf '%s\n' anaconda jupyter kerberos presto proxy-agent
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ false != \t\r\u\e ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + DEBIAN_COMPONENTS_TO_UNINSTALL+=('krb5-kpropd' 'krb5-kdc' 'krb5-admin-server' 'krb5-user' 'krb5-config' 'xinetd')
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + uninstall_packages
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_retries set_selections
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local retry_backoff
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + cmd=("$@")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -a cmd
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'About to run '\''set_selections'\'' with retries...'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'About to run '\''set_selections'\'' with retries...'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: About to run 'set_selections' with retries...
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local update_succeeded=0
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + (( i = 0 ))
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + (( i < 12 ))
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + set_selections
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + debconf-set-selections
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + cat
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + update_succeeded=1
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + break
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + ((  1  ))
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Uninstalling un-needed daemons'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Uninstalling un-needed daemons'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Uninstalling un-needed daemons
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag uninstall bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1400
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true] as pid 1400'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true] as pid 1400
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag uninstall-component-anaconda uninstall_component anaconda
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1401
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [uninstall_component anaconda] as pid 1401'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [uninstall_component anaconda] as pid 1401
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag uninstall-component-jupyter uninstall_component jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1402
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [uninstall_component jupyter] as pid 1402'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [uninstall_component jupyter] as pid 1402
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag uninstall-component-kerberos uninstall_component kerberos
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1403
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [uninstall_component kerberos] as pid 1403'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [uninstall_component kerberos] as pid 1403
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag uninstall-component-presto uninstall_component presto
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1404
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [uninstall_component presto] as pid 1404'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [uninstall_component presto] as pid 1404
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag uninstall-component-proxy-agent uninstall_component proxy-agent
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1405
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [uninstall_component proxy-agent] as pid 1405'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [uninstall_component proxy-agent] as pid 1405
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + is_version_at_least 1.3 1.3
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local ver1=1.3.0.0.0.0
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local ver2=1.3
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag uninstall-component-proxy-agent uninstall_component proxy-agent
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1405
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=uninstall-component-proxy-agent
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ mktemp
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag uninstall-component-presto uninstall_component presto
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1404
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=uninstall-component-presto
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag uninstall-component-kerberos uninstall_component kerberos
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1403
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=uninstall-component-kerberos
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag uninstall-component-jupyter uninstall_component jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1402
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=uninstall-component-jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'uninstall-component-proxy-agent[1405]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag uninstall-component-anaconda uninstall_component anaconda
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1401
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=uninstall-component-anaconda
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local log=/tmp/tmp.A0VaQ0ABJe
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'uninstall-component-presto[1404]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'uninstall-component-kerberos[1403]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag uninstall bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1400
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=uninstall
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'uninstall-component-jupyter[1402]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: + uninstall_component proxy-agent
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'uninstall-component-anaconda[1401]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + err_code=0
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + grep -C 10 -i warning /tmp/tmp.A0VaQ0ABJe
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + uninstall_component presto
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + local component=presto
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-kerberos[1403]: + uninstall_component kerberos
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-kerberos[1403]: + local component=kerberos
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-kerberos[1403]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-kerberos[1403]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-kerberos[1403]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-kerberos[1403]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-kerberos[1403]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'uninstall[1400]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: + uninstall_component jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: + uninstall_component anaconda
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + rm -f /tmp/tmp.A0VaQ0ABJe
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: + local component=proxy-agent
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: + set -exo pipefail
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + set -exo pipefail
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/presto.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-kerberos[1403]: + set -euxo pipefail
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: + local component=jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: + set -euxo pipefail
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: + local component=anaconda
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: + set -exo pipefail
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ readonly HTTP_PORT=8060
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ HTTP_PORT=8060
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ readonly PRESTO_BIN_PATH=/usr/bin/presto
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ PRESTO_BIN_PATH=/usr/bin/presto
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ readonly PRESTO_CLI_PATH=/opt/presto-cli
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ PRESTO_CLI_PATH=/opt/presto-cli
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ readonly PRESTO_DATA_DIRECTORY=/var/presto/data
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ PRESTO_DATA_DIRECTORY=/var/presto/data
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ readonly PRESTO_INSTALL_DIRECTORY=/opt/presto-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ PRESTO_INSTALL_DIRECTORY=/opt/presto-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ readonly INIT_SCRIPT=/usr/lib/systemd/system/presto.service
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ INIT_SCRIPT=/usr/lib/systemd/system/presto.service
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ readonly PRESTO_VERSION=0.215
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: ++ PRESTO_VERSION=0.215
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + rm -Rf /opt/presto-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + return 0
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/jupyter.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/anaconda.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ export WHEEL_DIR=/opt/dataproc/jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ WHEEL_DIR=/opt/dataproc/jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ export JGSCM_WHEEL=/opt/dataproc/jupyter/jgscm
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ JGSCM_WHEEL=/opt/dataproc/jupyter/jgscm
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ export JGSCM_REQUIREMENTS=/opt/dataproc/jupyter/jgscm.requirements
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ JGSCM_REQUIREMENTS=/opt/dataproc/jupyter/jgscm.requirements
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ export JUPYTER_ETC_DIR=/etc/jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ JUPYTER_ETC_DIR=/etc/jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ export JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ export JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ export PYSPARK_KERNELSPEC=/opt/conda/anaconda/share/jupyter/kernels/pyspark/kernel.json
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ PYSPARK_KERNELSPEC=/opt/conda/anaconda/share/jupyter/kernels/pyspark/kernel.json
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ export JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ export JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: ++ JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: + rm -Rf /etc/jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: ++ export ANACONDA_INSTALL_PATH=/opt/conda/anaconda
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: ++ ANACONDA_INSTALL_PATH=/opt/conda/anaconda
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: ++ export ANACONDA_BIN_DIR=/opt/conda/anaconda/bin
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: ++ ANACONDA_BIN_DIR=/opt/conda/anaconda/bin
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: ++ export PATH=/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: ++ PATH=/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-anaconda[1401]: + rm -Rf /opt/conda/anaconda
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall[1400]: + bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/proxy-agent.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ get_dataproc_property dataproc.monitoring.stackdriver.enable
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local property_name=dataproc.monitoring.stackdriver.enable
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: ++ readonly PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: ++ PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: ++ readonly PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: ++ PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: ++ readonly PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: ++ PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: + rm -f /usr/bin/proxy-forwarding-agent
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-jupyter[1402]: + rm -Rf /opt/dataproc/jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.monitoring.stackdriver.enable
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ local property_name=dataproc.monitoring.stackdriver.enable
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-proxy-agent[1405]: + rm -f /usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++++ grep '^dataproc.monitoring.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ local property_value=false
<13>Oct 28 05:33:20 google-dataproc-startup[803]: +++ echo false
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ local property_value=false
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ echo false
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + STACKDRIVER_MONITORING_ENABLED=false
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ false == \t\r\u\e ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Stackdriver monitoring disabled.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Stackdriver monitoring disabled.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Stackdriver monitoring disabled.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Replace dataproc plugin instance_name label with gce instance name.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Replace dataproc plugin instance_name label with gce instance name.'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Replace dataproc plugin instance_name label with gce instance name.
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for PLUGIN_FILE in /opt/stackdriver/collectd/etc/collectd.d/dataproc*
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ hostname
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + sed -i 's/"label:instance_name".*$/"label:instance_name" "cluster-4def-m"/g' /opt/stackdriver/collectd/etc/collectd.d/dataproc_collectd_default-20170324-133642.conf
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + chmod +x /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Running verify_setup.sh'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Running verify_setup.sh'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + rm -Rf /var/presto/data
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Running verify_setup.sh
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.warehouse.dir
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + rm -f /usr/bin/presto
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + rm -f /opt/presto-cli
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 uninstall-component-presto[1404]: + rm -f /usr/lib/systemd/system/presto.service
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + hive_warehouse_dir=None
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ None == \g\s\:\/\/* ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + loginfo 'Starting services'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Starting services'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Starting services
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array hadoop-hdfs-namenode ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=hadoop-hdfs-namenode
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-hdfs-namenode  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + case "${SERVICE}" in
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag setup-hadoop-hdfs-namenode setup_service hadoop-hdfs-namenode
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1463
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [setup_service hadoop-hdfs-namenode] as pid 1463'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [setup_service hadoop-hdfs-namenode] as pid 1463
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array hadoop-yarn-resourcemanager ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=hadoop-yarn-resourcemanager
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-yarn-resourcemanager  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + case "${SERVICE}" in
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1464
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [setup_service hadoop-yarn-resourcemanager] as pid 1464'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [setup_service hadoop-yarn-resourcemanager] as pid 1464
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array hive-metastore ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=hive-metastore
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-metastore  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + case "${SERVICE}" in
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag setup-hive-metastore setup_service hive-metastore
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1465
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [setup_service hive-metastore] as pid 1465'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [setup_service hive-metastore] as pid 1465
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array hive-server2 ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=hive-server2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-server2  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + case "${SERVICE}" in
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag setup-hive-server2 setup_service hive-server2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1466
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [setup_service hive-server2] as pid 1466'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [setup_service hive-server2] as pid 1466
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array zookeeper-server ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=zookeeper-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  zookeeper-server  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + continue
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array solr-server ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=solr-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  solr-server  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + continue
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array hadoop-mapreduce-historyserver ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=hadoop-mapreduce-historyserver
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-mapreduce-historyserver  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + case "${SERVICE}" in
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1467
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [setup_service hadoop-mapreduce-historyserver] as pid 1467'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [setup_service hadoop-mapreduce-historyserver] as pid 1467
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array spark-history-server ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=spark-history-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  spark-history-server  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + case "${SERVICE}" in
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag setup-spark-history-server setup_service spark-history-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1468
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [setup_service spark-history-server] as pid 1468'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [setup_service spark-history-server] as pid 1468
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array hive-webhcat-server ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=hive-webhcat-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-webhcat-server  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + continue
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array jupyter ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=jupyter
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  jupyter  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + continue
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array knox ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=knox
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  knox  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + continue
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array proxy-agent ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=proxy-agent
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  proxy-agent  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + continue
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array zeppelin ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=zeppelin
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  zeppelin  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + continue
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array hadoop-yarn-timelineserver ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=hadoop-yarn-timelineserver
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-yarn-timelineserver  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + case "${SERVICE}" in
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1469
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1464
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=setup-hadoop-yarn-resourcemanager
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag setup-hive-metastore setup_service hive-metastore
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1465
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=setup-hive-metastore
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag setup-hive-server2 setup_service hive-server2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1466
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=setup-hive-server2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1467
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=setup-hadoop-mapreduce-historyserver
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag setup-spark-history-server setup_service spark-history-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1468
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=setup-spark-history-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag setup-hadoop-hdfs-namenode setup_service hadoop-hdfs-namenode
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1463
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=setup-hadoop-hdfs-namenode
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'setup-hadoop-yarn-resourcemanager[1464]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'setup-hive-metastore[1465]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'setup-hive-server2[1466]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'setup-hadoop-mapreduce-historyserver[1467]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1469
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=setup-hadoop-yarn-timelineserver
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'setup-spark-history-server[1468]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [setup_service hadoop-yarn-timelineserver] as pid 1469'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [setup_service hadoop-yarn-timelineserver] as pid 1469
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array mariadb-server ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=mariadb-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  mariadb-server  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + case "${SERVICE}" in
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag setup-mariadb setup_service mariadb
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1487
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + echo 'Started background process [setup_service mariadb] as pid 1487'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: Started background process [setup_service mariadb] as pid 1487
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + in_array hadoop-hdfs-secondarynamenode ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local value=hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'setup-hadoop-hdfs-namenode[1463]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + setup_service hadoop-yarn-resourcemanager
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + setup_service hive-metastore
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + setup_service hive-server2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + setup_service hadoop-mapreduce-historyserver
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + export KERBEROS_ENABLED=false
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + KERBEROS_ENABLED=false
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + export -f login_through_keytab_if_necessary
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + export MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + local service=hadoop-mapreduce-historyserver
<13>Oct 28 05:33:20 google-dataproc-startup[803]: ++ logger -s -t 'setup-hadoop-yarn-timelineserver[1469]'
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + setup_service spark-history-server
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + export KERBEROS_ENABLED=false
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + KERBEROS_ENABLED=false
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_with_logger --tag setup-mariadb setup_service mariadb
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local pid=1487
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + tag=setup-mariadb
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-hdfs-secondarynamenode  ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + return 1
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + case "${SERVICE}" in
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + run_in_background --tag setup-hadoop-hdfs-secondarynamenode setup_service hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:20 google-dataproc-startup[803]: + PID=1496
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + setup_service hadoop-hdfs-namenode
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + export KERBEROS_ENABLED=false
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + KERBEROS_ENABLED=false
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + export -f login_through_keytab_if_necessary
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + export MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + local service=hadoop-hdfs-namenode
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:20 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + case "${MASTER_INDEX?}" in
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + loginfo 'Formatting NameNode'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + echo 'Formatting NameNode'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: Formatting NameNode
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + run_with_retries su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + local retry_backoff
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + cmd=("$@")
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + local -a cmd
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + loginfo 'About to run '\''su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + echo 'About to run '\''su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: About to run 'su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive' with retries...
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + local update_succeeded=0
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + (( i = 0 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + (( i < 12 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-hdfs-namenode[1463]: + su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + export KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + export -f login_through_keytab_if_necessary
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + export MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + local service=hive-server2
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + [[ hive-server2 == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + enable_service hive-server2
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + local service=hive-server2
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + local unit=hive-server2.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + run_with_retries systemctl enable hive-server2.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + local retry_backoff
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + cmd=("$@")
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + local -a cmd
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + loginfo 'About to run '\''systemctl enable hive-server2.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + echo 'About to run '\''systemctl enable hive-server2.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: About to run 'systemctl enable hive-server2.service' with retries...
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + local update_succeeded=0
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + (( i = 0 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + (( i < 12 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: + systemctl enable hive-server2.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: hive-server2.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-server2[1466]: Executing: /lib/systemd/systemd-sysv-install enable hive-server2
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + setup_service hadoop-yarn-timelineserver
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + export KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + export -f login_through_keytab_if_necessary
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + export MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + local service=hadoop-yarn-timelineserver
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + enable_service hadoop-yarn-timelineserver
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + local service=hadoop-yarn-timelineserver
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + local unit=hadoop-yarn-timelineserver.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + run_with_retries systemctl enable hadoop-yarn-timelineserver.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + local retry_backoff
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + cmd=("$@")
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + local -a cmd
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + loginfo 'About to run '\''systemctl enable hadoop-yarn-timelineserver.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + echo 'About to run '\''systemctl enable hadoop-yarn-timelineserver.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: About to run 'systemctl enable hadoop-yarn-timelineserver.service' with retries...
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + local update_succeeded=0
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + (( i = 0 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + (( i < 12 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: + systemctl enable hadoop-yarn-timelineserver.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: hadoop-yarn-timelineserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-timelineserver[1469]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-timelineserver
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + export KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + export -f login_through_keytab_if_necessary
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + export MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + local service=hive-metastore
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + [[ hive-metastore == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + wait_for_port cluster-4def-m 3306
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + local -r host=cluster-4def-m
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + local -r port=3306
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + loginfo 'Waiting for service to come up on host=cluster-4def-m port=3306.'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + echo 'Waiting for service to come up on host=cluster-4def-m port=3306.'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: Waiting for service to come up on host=cluster-4def-m port=3306.
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + retry_with_constant_backoff nc -v -z -w 0 cluster-4def-m 3306
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + local max_retry=300
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + cmd=("$@")
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + local -a cmd
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + local update_succeeded=0
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: ++ seq 1 300
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + export KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + export -f login_through_keytab_if_necessary
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + export MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + local service=hadoop-yarn-resourcemanager
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + enable_service hadoop-yarn-resourcemanager
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + local service=hadoop-yarn-resourcemanager
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + local unit=hadoop-yarn-resourcemanager.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + run_with_retries systemctl enable hadoop-yarn-resourcemanager.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + local retry_backoff
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + cmd=("$@")
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + local -a cmd
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + loginfo 'About to run '\''systemctl enable hadoop-yarn-resourcemanager.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + echo 'About to run '\''systemctl enable hadoop-yarn-resourcemanager.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: About to run 'systemctl enable hadoop-yarn-resourcemanager.service' with retries...
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + local update_succeeded=0
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + (( i = 0 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + (( i < 12 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: + systemctl enable hadoop-yarn-resourcemanager.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: hadoop-yarn-resourcemanager.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-yarn-resourcemanager[1464]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-resourcemanager
<13>Oct 28 05:33:21 google-dataproc-startup[803]: ++ logger -s -t 'setup-mariadb[1487]'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + [[ hadoop-mapreduce-historyserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + enable_service hadoop-mapreduce-historyserver
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + local service=hadoop-mapreduce-historyserver
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + local unit=hadoop-mapreduce-historyserver.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + run_with_retries systemctl enable hadoop-mapreduce-historyserver.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + local retry_backoff
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + cmd=("$@")
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + local -a cmd
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + loginfo 'About to run '\''systemctl enable hadoop-mapreduce-historyserver.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + echo 'About to run '\''systemctl enable hadoop-mapreduce-historyserver.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: About to run 'systemctl enable hadoop-mapreduce-historyserver.service' with retries...
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + local update_succeeded=0
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + (( i = 0 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + (( i < 12 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: + systemctl enable hadoop-mapreduce-historyserver.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: hadoop-mapreduce-historyserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hadoop-mapreduce-historyserver[1467]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-mapreduce-historyserver
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + export -f login_through_keytab_if_necessary
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + export MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + local service=spark-history-server
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + [[ spark-history-server == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + enable_service spark-history-server
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + local service=spark-history-server
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + local unit=spark-history-server.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + run_with_retries systemctl enable spark-history-server.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + local retry_backoff
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + cmd=("$@")
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + local -a cmd
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + loginfo 'About to run '\''systemctl enable spark-history-server.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + echo 'About to run '\''systemctl enable spark-history-server.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: About to run 'systemctl enable spark-history-server.service' with retries...
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + local update_succeeded=0
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + (( i = 0 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + (( i < 12 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: + systemctl enable spark-history-server.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: spark-history-server.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-spark-history-server[1468]: Executing: /lib/systemd/systemd-sysv-install enable spark-history-server
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + run_with_logger --tag setup-hadoop-hdfs-secondarynamenode setup_service hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + local pid=1496
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + tag=setup-hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 3306
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + echo 'Started background process [setup_service hadoop-hdfs-secondarynamenode] as pid 1496'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: Started background process [setup_service hadoop-hdfs-secondarynamenode] as pid 1496
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + loginfo 'Configuring optional components'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: + echo 'Configuring optional components'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: Configuring optional components
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + setup_service mariadb
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + export KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + export -f login_through_keytab_if_necessary
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + export MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + local service=mariadb
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + [[ mariadb == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + enable_service mariadb
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + local service=mariadb
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + local unit=mariadb.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + run_with_retries systemctl enable mariadb.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + local retry_backoff
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + cmd=("$@")
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + local -a cmd
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + loginfo 'About to run '\''systemctl enable mariadb.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + echo 'About to run '\''systemctl enable mariadb.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: About to run 'systemctl enable mariadb.service' with retries...
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + local update_succeeded=0
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + (( i = 0 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + (( i < 12 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-mariadb[1487]: + systemctl enable mariadb.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: ++ logger -s -t 'setup-hadoop-hdfs-secondarynamenode[1496]'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: ++ get_dataproc_property dataproc.conscrypt.provider.enable
<13>Oct 28 05:33:21 google-dataproc-startup[803]: ++ local property_name=dataproc.conscrypt.provider.enable
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 1.'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 1.'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 1.
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:20 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + setup_service hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + export KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + KERBEROS_ENABLED=false
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + export -f login_through_keytab_if_necessary
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + export MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + local service=hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + enable_service hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + local service=hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + local unit=hadoop-hdfs-secondarynamenode.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + run_with_retries systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + local retry_backoff
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + cmd=("$@")
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + local -a cmd
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + loginfo 'About to run '\''systemctl enable hadoop-hdfs-secondarynamenode.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + echo 'About to run '\''systemctl enable hadoop-hdfs-secondarynamenode.service'\'' with retries...'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: About to run 'systemctl enable hadoop-hdfs-secondarynamenode.service' with retries...
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + local update_succeeded=0
<13>Oct 28 05:33:21 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.conscrypt.provider.enable
<13>Oct 28 05:33:21 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:21 google-dataproc-startup[803]: +++ local property_name=dataproc.conscrypt.provider.enable
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + (( i = 0 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:21 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:21 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:21 google-dataproc-startup[803]: ++++ grep '^dataproc.conscrypt.provider.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-mariadb[1487]: Created symlink /etc/systemd/system/mysql.service → /lib/systemd/system/mariadb.service.
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + (( i < 12 ))
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-mariadb[1487]: Created symlink /etc/systemd/system/mysqld.service → /lib/systemd/system/mariadb.service.
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: + systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Oct 28 05:33:21 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-mariadb[1487]: Created symlink /etc/systemd/system/multi-user.target.wants/mariadb.service → /lib/systemd/system/mariadb.service.
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: hadoop-hdfs-secondarynamenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ local property_value=true
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ echo true
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ local property_value=true
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ echo true
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + CONSCRYPT_ENABLED=true
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + [[ true == \t\r\u\e ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt.jar /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/libconscrypt.jar
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-hadoop-hdfs-secondarynamenode[1496]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni.so /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/libconscrypt_openjdk_jni.so
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + cp /usr/local/share/google/dataproc/java.security.conscrypt /etc/java-8-openjdk/security/java.security
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ get_dataproc_property dataproc.logging.stackdriver.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ local property_name=dataproc.logging.stackdriver.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ local property_name=dataproc.logging.stackdriver.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ tail -n 1
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ grep '^dataproc.logging.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ local property_value=
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ echo ''
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ local property_value=
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ echo ''
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + STACKDRIVER_LOGGING_ENABLED=
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + [[ '' == \f\a\l\s\e ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + loginfo 'Stackdriver enabled; enabling google-fluentd.'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + echo 'Stackdriver enabled; enabling google-fluentd.'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: Stackdriver enabled; enabling google-fluentd.
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + source /usr/local/share/google/dataproc/bdutil/configure_fluentd.sh
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ set -e
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ set -u
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ loginfo 'Running configure_fluentd.sh'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ echo 'Running configure_fluentd.sh'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: Running configure_fluentd.sh
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ FLUENTD_BASE_DIR=/etc/google-fluentd
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ FLUENTD_CONF_DIR=/etc/google-fluentd/config.d
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ FLUENTD_PLUGIN_DIR=/etc/google-fluentd/plugin
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ cp /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/monitoring.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/out_google_cloud.rb /etc/google-fluentd/plugin
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ get_dataproc_property dataproc.logging.stackdriver.job.driver.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ local property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.driver.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ local property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++++ cut -d = -f 2-
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++++ tail -n 1
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++++ grep '^dataproc.logging.stackdriver.job.driver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ local property_value=
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ echo ''
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ local property_value=
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ echo ''
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ JOB_DRIVER_LOGGING_ENABLED=
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ [[ '' == \t\r\u\e ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-mariadb[1487]: + update_succeeded=1
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-mariadb[1487]: + break
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-mariadb[1487]: + ((  1  ))
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ get_dataproc_property dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ local property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-mariadb[1487]: ++ systemctl show mariadb.service -p Restart,RemainAfterExit
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ local property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++++ cut -d = -f 2-
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++++ tail -n 1
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++++ grep '^dataproc.logging.stackdriver.job.yarn.container.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ local property_value=
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++++ echo ''
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ local property_value=
<13>Oct 28 05:33:22 google-dataproc-startup[803]: +++ echo ''
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ CONTAINER_LOGGING_ENABLED=
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ [[ '' == \t\r\u\e ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + run_in_background --tag setup-google-fluentd setup_service google-fluentd
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + PID=1571
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + echo 'Started background process [setup_service google-fluentd] as pid 1571'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: Started background process [setup_service google-fluentd] as pid 1571
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + wait_on_async_processes
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + loginfo 'Waiting on async proccesses'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + echo 'Waiting on async proccesses'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: Waiting on async proccesses
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + (( i = 0 ))
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + pid=1571
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + cmd='setup_service google-fluentd'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1571 cmd=[setup_service google-fluentd]'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + echo 'Waiting on pid=1571 cmd=[setup_service google-fluentd]'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: Waiting on pid=1571 cmd=[setup_service google-fluentd]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + wait 1571
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + run_with_logger --tag setup-google-fluentd setup_service google-fluentd
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + local tag=
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + local pid=1571
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + tag=setup-google-fluentd
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + shift 2
<13>Oct 28 05:33:22 google-dataproc-startup[803]: + exec
<13>Oct 28 05:33:22 google-dataproc-startup[803]: ++ logger -s -t 'setup-google-fluentd[1571]'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + setup_service google-fluentd
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + export KERBEROS_ENABLED=false
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + KERBEROS_ENABLED=false
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + export -f login_through_keytab_if_necessary
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + export MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + MY_FULL_HOSTNAME=cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + local service=google-fluentd
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + [[ google-fluentd == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + enable_service google-fluentd
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + local service=google-fluentd
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + local unit=google-fluentd.service
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + run_with_retries systemctl enable google-fluentd.service
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + local retry_backoff
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + cmd=("$@")
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + local -a cmd
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + loginfo 'About to run '\''systemctl enable google-fluentd.service'\'' with retries...'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + echo 'About to run '\''systemctl enable google-fluentd.service'\'' with retries...'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: About to run 'systemctl enable google-fluentd.service' with retries...
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + local update_succeeded=0
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + (( i = 0 ))
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + (( i < 12 ))
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: + systemctl enable google-fluentd.service
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: google-fluentd.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:21 setup-google-fluentd[1571]: Executing: /lib/systemd/systemd-sysv-install enable google-fluentd
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 3306
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 2.'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 2.'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 2.
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + local 'props=Restart=on-abort
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: RemainAfterExit=no'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + [[ Restart=on-abort
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + in_array mariadb DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + local value=mariadb
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  mariadb  ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + return 1
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + [[ mariadb == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + run_with_retries systemctl start mariadb
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + local retry_backoff
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + cmd=("$@")
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + local -a cmd
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + loginfo 'About to run '\''systemctl start mariadb'\'' with retries...'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + echo 'About to run '\''systemctl start mariadb'\'' with retries...'
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: About to run 'systemctl start mariadb' with retries...
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + local update_succeeded=0
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + (( i = 0 ))
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + (( i < 12 ))
<13>Oct 28 05:33:22 google-dataproc-startup[803]: <13>Oct 28 05:33:22 setup-mariadb[1487]: + systemctl start mariadb
<13>Oct 28 05:33:23 google-dataproc-startup[803]: <13>Oct 28 05:33:23 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:23 google-dataproc-startup[803]: <13>Oct 28 05:33:23 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 3306
<13>Oct 28 05:33:23 google-dataproc-startup[803]: <13>Oct 28 05:33:23 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 05:33:23 google-dataproc-startup[803]: <13>Oct 28 05:33:23 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 3.'
<13>Oct 28 05:33:23 google-dataproc-startup[803]: <13>Oct 28 05:33:23 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 3.'
<13>Oct 28 05:33:23 google-dataproc-startup[803]: <13>Oct 28 05:33:23 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 3.
<13>Oct 28 05:33:23 google-dataproc-startup[803]: <13>Oct 28 05:33:23 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 3306
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 4.'
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 4.'
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 4.
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-yarn-resourcemanager[1464]: + update_succeeded=1
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-yarn-resourcemanager[1464]: + break
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-yarn-resourcemanager[1464]: + ((  1  ))
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-yarn-resourcemanager[1464]: ++ systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-mapreduce-historyserver[1467]: + update_succeeded=1
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-mapreduce-historyserver[1467]: + break
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-mapreduce-historyserver[1467]: + ((  1  ))
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-mapreduce-historyserver[1467]: ++ systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-yarn-timelineserver[1469]: + update_succeeded=1
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-yarn-timelineserver[1469]: + break
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-yarn-timelineserver[1469]: + ((  1  ))
<13>Oct 28 05:33:24 google-dataproc-startup[803]: <13>Oct 28 05:33:24 setup-hadoop-yarn-timelineserver[1469]: ++ systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + update_succeeded=1
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + break
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + ((  1  ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 3306
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: ++ systemctl show hive-server2.service -p Restart,RemainAfterExit
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 5.'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 5.'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 5.
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 uninstall[1400]: Reading package lists...
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + update_succeeded=1
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + break
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + ((  1  ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: ++ systemctl show spark-history-server.service -p Restart,RemainAfterExit
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + update_succeeded=1
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + break
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + ((  1  ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: ++ systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + update_succeeded=1
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + break
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + ((  1  ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: ++ systemctl show google-fluentd.service -p Restart,RemainAfterExit
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + local 'props=Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: RemainAfterExit=no'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + local drop_in_dir=/etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + mkdir /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: + local 'props=Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: RemainAfterExit=no'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: + local drop_in_dir=/etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: + mkdir /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + in_array hadoop-yarn-resourcemanager DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + local value=hadoop-yarn-resourcemanager
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-yarn-resourcemanager  ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + return 1
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + run_with_retries systemctl start hadoop-yarn-resourcemanager
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + local retry_backoff
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + cmd=("$@")
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + local -a cmd
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + loginfo 'About to run '\''systemctl start hadoop-yarn-resourcemanager'\'' with retries...'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + echo 'About to run '\''systemctl start hadoop-yarn-resourcemanager'\'' with retries...'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: About to run 'systemctl start hadoop-yarn-resourcemanager' with retries...
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + local update_succeeded=0
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + (( i = 0 ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + (( i < 12 ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: + systemctl start hadoop-yarn-resourcemanager
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: + in_array hadoop-mapreduce-historyserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: + local value=hadoop-mapreduce-historyserver
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-mapreduce-historyserver  ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-mapreduce-historyserver[1467]: + return
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + local 'props=Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: RemainAfterExit=no'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + local drop_in_dir=/etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + mkdir /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + in_array hadoop-yarn-timelineserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + local value=hadoop-yarn-timelineserver
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-yarn-timelineserver  ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + return 1
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + run_with_retries systemctl start hadoop-yarn-timelineserver
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + local retry_backoff
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + cmd=("$@")
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + local -a cmd
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + loginfo 'About to run '\''systemctl start hadoop-yarn-timelineserver'\'' with retries...'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + echo 'About to run '\''systemctl start hadoop-yarn-timelineserver'\'' with retries...'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: About to run 'systemctl start hadoop-yarn-timelineserver' with retries...
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + local update_succeeded=0
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + (( i = 0 ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + (( i < 12 ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: + systemctl start hadoop-yarn-timelineserver
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + local 'props=Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: RemainAfterExit=no'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + local drop_in_dir=/etc/systemd/system/hive-server2.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + mkdir /etc/systemd/system/hive-server2.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-server2.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + local 'props=Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: RemainAfterExit=no'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + local drop_in_dir=/etc/systemd/system/spark-history-server.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + mkdir /etc/systemd/system/spark-history-server.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + local 'props=Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: RemainAfterExit=no'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + local drop_in_dir=/etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + mkdir /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + in_array hive-server2 DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + local value=hive-server2
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hive-server2  ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + return 1
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + [[ hive-server2 == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hive-server2[1466]: + return
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/spark-history-server.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + local 'props=Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: RemainAfterExit=yes'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + [[ Restart=no
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + in_array google-fluentd DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + local value=google-fluentd
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  google-fluentd  ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + return 1
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + [[ google-fluentd == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + run_with_retries systemctl start google-fluentd
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + local retry_backoff
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + cmd=("$@")
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + local -a cmd
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + loginfo 'About to run '\''systemctl start google-fluentd'\'' with retries...'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + echo 'About to run '\''systemctl start google-fluentd'\'' with retries...'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: About to run 'systemctl start google-fluentd' with retries...
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + local update_succeeded=0
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + (( i = 0 ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + (( i < 12 ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-google-fluentd[1571]: + systemctl start google-fluentd
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + in_array spark-history-server DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + local value=spark-history-server
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  spark-history-server  ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-spark-history-server[1468]: + return
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + in_array hadoop-hdfs-secondarynamenode DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + local value=hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-hdfs-secondarynamenode  ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + return 1
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + run_with_retries systemctl start hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + local retry_backoff
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + cmd=("$@")
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + local -a cmd
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + loginfo 'About to run '\''systemctl start hadoop-hdfs-secondarynamenode'\'' with retries...'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + echo 'About to run '\''systemctl start hadoop-hdfs-secondarynamenode'\'' with retries...'
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: About to run 'systemctl start hadoop-hdfs-secondarynamenode' with retries...
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + local update_succeeded=0
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + (( i = 0 ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + (( i < 12 ))
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: + systemctl start hadoop-hdfs-secondarynamenode
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-resourcemanager[1464]: Warning: hadoop-yarn-resourcemanager.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-yarn-timelineserver[1469]: Warning: hadoop-yarn-timelineserver.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 28 05:33:25 google-dataproc-startup[803]: <13>Oct 28 05:33:25 setup-hadoop-hdfs-secondarynamenode[1496]: Warning: hadoop-hdfs-secondarynamenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 28 05:33:26 google-dataproc-startup[803]: <13>Oct 28 05:33:26 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:26 google-dataproc-startup[803]: <13>Oct 28 05:33:26 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 3306
<13>Oct 28 05:33:26 google-dataproc-startup[803]: <13>Oct 28 05:33:26 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 05:33:26 google-dataproc-startup[803]: <13>Oct 28 05:33:26 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 6.'
<13>Oct 28 05:33:26 google-dataproc-startup[803]: <13>Oct 28 05:33:26 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 6.'
<13>Oct 28 05:33:26 google-dataproc-startup[803]: <13>Oct 28 05:33:26 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 6.
<13>Oct 28 05:33:26 google-dataproc-startup[803]: <13>Oct 28 05:33:26 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:26 google-dataproc-startup[803]: <13>Oct 28 05:33:26 uninstall[1400]: Building dependency tree...
<13>Oct 28 05:33:26 google-dataproc-startup[803]: <13>Oct 28 05:33:26 uninstall[1400]: Reading state information...
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]: The following packages will be REMOVED:
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 3306
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 7.'
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 7.'
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 7.
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   bind9-host* druid* fonts-font-awesome* fonts-mathjax* geoip-database*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   hadoop-hdfs-datanode* hadoop-hdfs-journalnode* hadoop-hdfs-zkfc*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   hadoop-yarn-nodemanager* hive-webhcat* hive-webhcat-server*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   javascript-common* kafka* kafka-server* knox* krb5-admin-server*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   krb5-config* krb5-kdc* krb5-kpropd* krb5-user* libbind9-140* libc-ares2*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   libdns162* libev4* libfile-copy-recursive-perl* libgeoip1* libgssrpc4*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   libhttp-parser2.8* libisc160* libisccc140* libisccfg140* libjs-bootstrap*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   libjs-d3* libjs-es5-shim* libjs-highlight.js* libjs-jquery*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   libjs-jquery-datatables* libjs-jquery-metadata* libjs-jquery-selectize.js*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   libjs-jquery-tablesorter* libjs-jquery-ui* libjs-json* libjs-mathjax*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   libjs-microplugin.js* libjs-modernizr* libjs-prettify* libjs-sifter.js*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   libjs-twitter-bootstrap* libjs-twitter-bootstrap-datepicker*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   libkadm5clnt-mit11* libkadm5srv-mit11* libkdb5-8* liblua5.1-0*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   libluajit-5.1-2* libluajit-5.1-common* liblwres141* libuv1* libverto-libev1*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   libverto1* libyaml-0-2* littler* node-highlight.js* node-normalize.css*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   nodejs* nodejs-doc* pandoc* pandoc-data* r-cran-assertthat*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-base64enc* r-cran-bindr* r-cran-bindrcpp* r-cran-bit* r-cran-bit64*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-cli* r-cran-colorspace* r-cran-crayon* r-cran-data.table* r-cran-dbi*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-digest* r-cran-dplyr* r-cran-evaluate* r-cran-fansi* r-cran-filehash*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-ggplot2* r-cran-glue* r-cran-googlevis* r-cran-gtable* r-cran-hexbin*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-highr* r-cran-hms* r-cran-htmltools* r-cran-htmlwidgets*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-httpuv* r-cran-jsonlite* r-cran-knitr* r-cran-labeling* r-cran-later*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-lazyeval* r-cran-littler* r-cran-magrittr* r-cran-mapproj*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-maps* r-cran-markdown* r-cran-memoise* r-cran-mime* r-cran-munsell*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-pillar* r-cran-pkgconfig* r-cran-pkgkitten* r-cran-plyr* r-cran-png*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-praise* r-cran-promises* r-cran-purrr* r-cran-r6*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-rcolorbrewer* r-cran-rcpp* r-cran-reshape2* r-cran-rlang*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-rmarkdown* r-cran-rsqlite* r-cran-scales* r-cran-shiny*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-sourcetools* r-cran-sp* r-cran-stringi* r-cran-stringr*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-testit* r-cran-testthat* r-cran-tibble* r-cran-tidyselect*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-tikzdevice* r-cran-tinytex* r-cran-utf8* r-cran-viridislite*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   r-cran-withr* r-cran-xfun* r-cran-xml2* r-cran-xtable* r-cran-yaml* solr*
<13>Oct 28 05:33:27 google-dataproc-startup[803]: <13>Oct 28 05:33:27 uninstall[1400]:   solr-server* update-inetd* xinetd* zeppelin* zookeeper-server*
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 3306
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 8.'
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 8.'
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 3306 failed. Retry attempt: 8.
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: + update_succeeded=1
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: + break
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: + ((  1  ))
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: + [[ mariadb == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: ++ get_dataproc_property am.primary_only
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: ++ local property_name=am.primary_only
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: +++ local property_name=am.primary_only
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: ++++ tail -n 1
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: +++ local property_value=false
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: +++ echo false
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: ++ local property_value=false
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: ++ echo false
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 05:33:28 google-dataproc-startup[803]: <13>Oct 28 05:33:28 setup-mariadb[1487]: + [[ mariadb == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 3306
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: Connection to cluster-4def-m 3306 port [tcp/mysql] succeeded!
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + update_succeeded=1
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 3306 succeeded.'
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 3306 succeeded.'
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 3306 succeeded.
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + break
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + ((  1  ))
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + loginfo 'Service up on host=cluster-4def-m port=3306.'
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + echo 'Service up on host=cluster-4def-m port=3306.'
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: Service up on host=cluster-4def-m port=3306.
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + enable_service hive-metastore
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + local service=hive-metastore
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + local unit=hive-metastore.service
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + run_with_retries systemctl enable hive-metastore.service
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + local retry_backoff
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + cmd=("$@")
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + local -a cmd
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + loginfo 'About to run '\''systemctl enable hive-metastore.service'\'' with retries...'
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + echo 'About to run '\''systemctl enable hive-metastore.service'\'' with retries...'
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: About to run 'systemctl enable hive-metastore.service' with retries...
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + local update_succeeded=0
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + (( i = 0 ))
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + (( i < 12 ))
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: + systemctl enable hive-metastore.service
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: hive-metastore.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 05:33:29 google-dataproc-startup[803]: <13>Oct 28 05:33:29 setup-hive-metastore[1465]: Executing: /lib/systemd/systemd-sysv-install enable hive-metastore
<13>Oct 28 05:33:30 google-dataproc-startup[803]: <13>Oct 28 05:33:30 uninstall[1400]: 0 upgraded, 0 newly installed, 146 to remove and 1 not upgraded.
<13>Oct 28 05:33:30 google-dataproc-startup[803]: <13>Oct 28 05:33:30 uninstall[1400]: After this operation, 2,006 MB disk space will be freed.
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + update_succeeded=1
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + break
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + ((  1  ))
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: ++ systemctl show hive-metastore.service -p Restart,RemainAfterExit
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + local 'props=Restart=no
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: RemainAfterExit=no'
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + [[ Restart=no
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + [[ Restart=no
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + local drop_in_dir=/etc/systemd/system/hive-metastore.service.d
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + mkdir /etc/systemd/system/hive-metastore.service.d
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-metastore.service.d
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + in_array hive-metastore DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + local value=hive-metastore
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hive-metastore  ]]
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + return 1
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + [[ hive-metastore == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + run_with_retries systemctl start hive-metastore
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + local retry_backoff
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + cmd=("$@")
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + local -a cmd
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + loginfo 'About to run '\''systemctl start hive-metastore'\'' with retries...'
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + echo 'About to run '\''systemctl start hive-metastore'\'' with retries...'
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: About to run 'systemctl start hive-metastore' with retries...
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + local update_succeeded=0
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + (( i = 0 ))
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + (( i < 12 ))
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: + systemctl start hive-metastore
<13>Oct 28 05:33:31 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:31.582+0000: 4.027: [GC (Allocation Failure) 2019-10-28T05:33:31.582+0000: 4.027: [ParNew: 15936K->1920K(17856K), 0.0960388 secs] 15936K->3244K(57472K), 0.0961077 secs] [Times: user=0.02 sys=0.00, real=0.09 secs] 
<13>Oct 28 05:33:32 google-dataproc-startup[803]: <13>Oct 28 05:33:31 setup-hive-metastore[1465]: Warning: hive-metastore.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 28 05:33:32 google-dataproc-startup[803]: <13>Oct 28 05:33:32 uninstall[1400]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 119619 files and directories currently installed.)
<13>Oct 28 05:33:32 google-dataproc-startup[803]: <13>Oct 28 05:33:32 uninstall[1400]: Removing krb5-admin-server (1.15-1+deb9u1) ...
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:33 INFO namenode.NameNode: STARTUP_MSG: 
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: /************************************************************
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: STARTUP_MSG: Starting NameNode
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: STARTUP_MSG:   host = cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal/10.128.0.34
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: STARTUP_MSG:   args = [-format, -nonInteractive]
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: STARTUP_MSG:   version = 2.9.2
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/hadoop-lzo-0.4.20.jar:/usr/lib/hadoop/lib/httpclient-4.5.2.jar:/usr/lib/hadoop/lib/gcs-connector-hadoop2-1.9.17.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/gcs-connector.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.1
<13>Oct 28 05:33:33 google-dataproc-startup[803]: 3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jetty-6.1.26.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0
<13>Oct 28 05:33:33 google-dataproc-startup[803]: .4.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/u
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: sr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/httpcore-4.4.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/li
<13>Oct 28 05:33:33 google-dataproc-startup[803]: b/jetty-sslengine-6.1.26.jar:/usr/lib/hadoop/lib/m
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: ockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.9.2.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-common-2.9.2-tests.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-2.9.2.jar:/usr/lib/hadoop/.//hadoop-nfs-2.9.2.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth-2.9.2.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib
<13>Oct 28 05:33:33 google-dataproc-startup[803]: /hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoo
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: p-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2
<13>Oct 28 05:33:33 google-dataproc-startup[803]: .7.8.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: .jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.9.2-tests.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/l
<13>Oct 28 05:33:33 google-dataproc-startup[803]: ib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: /hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/api-as
<13>Oct 28 05:33:33 google-dataproc-startup[803]: n1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/comm
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: ons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hado
<13>Oct 28 05:33:33 google-dataproc-startup[803]: op-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/ha
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: doop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/ht
<13>Oct 28 05:33:33 google-dataproc-startup[803]: tpcore-4.4.4.jar:/usr/lib/hadoop-yarn/lib/jetty-ut
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: il-6.1.26.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.9.2.jar:/usr
<13>Oct 28 05:33:33 google-dataproc-startup[803]: /lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/li
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: b/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-y
<13>Oct 28 05:33:33 google-dataproc-startup[803]: arn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-ya
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: rn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.9.2.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce
<13>Oct 28 05:33:33 google-dataproc-startup[803]: /lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapredu
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: ce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//l
<13>Oct 28 05:33:33 google-dataproc-startup[803]: eveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/./
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: /jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ar
<13>Oct 28 05:33:33 google-dataproc-startup[803]: chive-logs.jar:/usr/lib/hadoop-mapreduce/.//log4j-
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: 1.2.17.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//h
<13>Oct 28 05:33:33 google-dataproc-startup[803]: adoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hado
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: op-archives-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.199.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2
<13>Oct 28 05:33:33 google-dataproc-startup[803]: .0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//mssql-jd
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: bc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client
<13>Oct 28 05:33:33 google-dataproc-startup[803]: -hs-plugins-2.9.2.jar:/usr/lib/hadoop-mapreduce/./
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: /hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.9.2-tests.jar:/usr
<13>Oct 28 05:33:33 google-dataproc-startup[803]: /lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: /lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.9.2.jar:/usr/lib/hadoop-ma
<13>Oct 28 05:33:33 google-dataproc-startup[803]: preduce/.//json-20170516.jar:/usr/lib/hadoop-mapre
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: duce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.5.j
<13>Oct 28 05:33:33 google-dataproc-startup[803]: ar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: .jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sd
<13>Oct 28 05:33:33 google-dataproc-startup[803]: k-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ar
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: chive-logs-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.13.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: STARTUP_MSG:   build = https://bigdataoss-internal.googlesource.com/third_party/apache/hadoop -r 849ee9eda72c7e8b1eb9fc5a830432c887914111; compiled by 'bigtop' on 2019-09-18T10:58Z
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: STARTUP_MSG:   java = 1.8.0_222
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: ************************************************************/
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: + update_succeeded=1
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: + break
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: + ((  1  ))
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:33 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: ++ get_dataproc_property am.primary_only
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: ++ local property_name=am.primary_only
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: + update_succeeded=1
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: + break
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: + ((  1  ))
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: +++ local property_name=am.primary_only
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: ++ get_dataproc_property am.primary_only
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: ++ local property_name=am.primary_only
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: +++ local property_name=am.primary_only
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: ++++ tail -n 1
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: +++ local property_value=false
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: +++ echo false
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: ++ local property_value=false
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: ++ echo false
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: + [[ 0 -eq 0 ]]
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-resourcemanager[1464]: + [[ false == \t\r\u\e ]]
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: ++++ tail -n 1
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: +++ local property_value=false
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: +++ echo false
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: ++ local property_value=false
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: ++ echo false
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 05:33:33 google-dataproc-startup[803]: <13>Oct 28 05:33:33 setup-hadoop-yarn-timelineserver[1469]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 05:33:34 google-dataproc-startup[803]: <13>Oct 28 05:33:34 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:34 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]
<13>Oct 28 05:33:35 google-dataproc-startup[803]: <13>Oct 28 05:33:35 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:35.597+0000: 8.041: [GC (Allocation Failure) 2019-10-28T05:33:35.597+0000: 8.041: [ParNew: 17856K->1920K(17856K), 0.0413514 secs] 19180K->5700K(57472K), 0.0414204 secs] [Times: user=0.02 sys=0.00, real=0.04 secs] 
<13>Oct 28 05:33:35 google-dataproc-startup[803]: <13>Oct 28 05:33:35 uninstall[1400]: Removing krb5-kpropd (1.15-1+deb9u1) ...
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + update_succeeded=1
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + break
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + ((  1  ))
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + [[ hive-metastore == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + wait_for_port cluster-4def-m 9083
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + local -r host=cluster-4def-m
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + local -r port=9083
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + loginfo 'Waiting for service to come up on host=cluster-4def-m port=9083.'
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + echo 'Waiting for service to come up on host=cluster-4def-m port=9083.'
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: Waiting for service to come up on host=cluster-4def-m port=9083.
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + retry_with_constant_backoff nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + local max_retry=300
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + cmd=("$@")
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + local -a cmd
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + local update_succeeded=0
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: ++ seq 1 300
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 1.'
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 1.'
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 1.
<13>Oct 28 05:33:36 google-dataproc-startup[803]: <13>Oct 28 05:33:36 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: + update_succeeded=1
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: + break
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: + ((  1  ))
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: ++ get_dataproc_property am.primary_only
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: ++ local property_name=am.primary_only
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: +++ local property_name=am.primary_only
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: ++++ tail -n 1
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: +++ local property_value=false
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: +++ echo false
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: ++ local property_value=false
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: ++ echo false
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-secondarynamenode[1496]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 2.'
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 2.'
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 2.
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:37.480+0000: 9.925: [GC (Allocation Failure) 2019-10-28T05:33:37.480+0000: 9.925: [ParNew: 17856K->1766K(17856K), 0.0514487 secs] 21636K->6323K(57472K), 0.0515200 secs] [Times: user=0.01 sys=0.00, real=0.06 secs] 
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:37 WARN common.Util: Path /hadoop/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
<13>Oct 28 05:33:37 google-dataproc-startup[803]: <13>Oct 28 05:33:37 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:37 WARN common.Util: Path /hadoop/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hadoop-hdfs-namenode[1463]: Formatting using clusterid: CID-89b22a75-f072-46ee-9761-501de1ff0bba
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 3.'
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 3.'
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 3.
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:38 INFO namenode.FSEditLog: Edit logging is async:true
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:38 INFO namenode.FSNamesystem: KeyProvider: null
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:38 INFO namenode.FSNamesystem: fsLock is fair: true
<13>Oct 28 05:33:38 google-dataproc-startup[803]: <13>Oct 28 05:33:38 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:38 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:39 INFO namenode.FSNamesystem: fsOwner             = hdfs (auth:SIMPLE)
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:39 INFO namenode.FSNamesystem: supergroup          = hadoop
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:39 INFO namenode.FSNamesystem: isPermissionEnabled = false
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:39 INFO namenode.FSNamesystem: HA Enabled: false
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 4.'
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 4.'
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 4.
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:39 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
<13>Oct 28 05:33:39 google-dataproc-startup[803]: <13>Oct 28 05:33:39 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:39.708+0000: 12.152: [GC (Allocation Failure) 2019-10-28T05:33:39.708+0000: 12.152: [ParNew: 17702K->1818K(17856K), 0.0342787 secs] 22259K->6982K(57472K), 0.0343434 secs] [Times: user=0.01 sys=0.00, real=0.04 secs] 
<13>Oct 28 05:33:40 google-dataproc-startup[803]: <13>Oct 28 05:33:40 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:40 google-dataproc-startup[803]: <13>Oct 28 05:33:40 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:40 google-dataproc-startup[803]: <13>Oct 28 05:33:40 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:40 google-dataproc-startup[803]: <13>Oct 28 05:33:40 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 5.'
<13>Oct 28 05:33:40 google-dataproc-startup[803]: <13>Oct 28 05:33:40 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 5.'
<13>Oct 28 05:33:40 google-dataproc-startup[803]: <13>Oct 28 05:33:40 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 5.
<13>Oct 28 05:33:40 google-dataproc-startup[803]: <13>Oct 28 05:33:40 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:41 google-dataproc-startup[803]: <13>Oct 28 05:33:41 uninstall[1400]: Removing krb5-kdc (1.15-1+deb9u1) ...
<13>Oct 28 05:33:41 google-dataproc-startup[803]: <13>Oct 28 05:33:41 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:41 google-dataproc-startup[803]: <13>Oct 28 05:33:41 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:41 google-dataproc-startup[803]: <13>Oct 28 05:33:41 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:41 google-dataproc-startup[803]: <13>Oct 28 05:33:41 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 6.'
<13>Oct 28 05:33:41 google-dataproc-startup[803]: <13>Oct 28 05:33:41 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 6.'
<13>Oct 28 05:33:41 google-dataproc-startup[803]: <13>Oct 28 05:33:41 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 6.
<13>Oct 28 05:33:41 google-dataproc-startup[803]: <13>Oct 28 05:33:41 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:41 google-dataproc-startup[803]: <13>Oct 28 05:33:41 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:41.811+0000: 14.255: [GC (Allocation Failure) 2019-10-28T05:33:41.811+0000: 14.255: [ParNew: 17754K->1919K(17856K), 0.0468683 secs] 22918K->8253K(57472K), 0.0469353 secs] [Times: user=0.01 sys=0.00, real=0.04 secs] 
<13>Oct 28 05:33:42 google-dataproc-startup[803]: <13>Oct 28 05:33:42 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:42 google-dataproc-startup[803]: <13>Oct 28 05:33:42 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:42 google-dataproc-startup[803]: <13>Oct 28 05:33:42 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:42 google-dataproc-startup[803]: <13>Oct 28 05:33:42 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 7.'
<13>Oct 28 05:33:42 google-dataproc-startup[803]: <13>Oct 28 05:33:42 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 7.'
<13>Oct 28 05:33:42 google-dataproc-startup[803]: <13>Oct 28 05:33:42 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 7.
<13>Oct 28 05:33:42 google-dataproc-startup[803]: <13>Oct 28 05:33:42 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO util.HostsFileReader: Adding a node "cluster-4def-w-0.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO util.HostsFileReader: Adding a node "cluster-4def-w-1.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO util.HostsFileReader: Adding a node "cluster-4def-w-2.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO util.HostsFileReader: Adding a node "cluster-4def-w-3.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO util.HostsFileReader: Adding a node "cluster-4def-w-4.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO util.HostsFileReader: Adding a node "cluster-4def-w-5.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO util.HostsFileReader: Adding a node "cluster-4def-w-6.us-central1-a.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 8.'
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 8.'
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 8.
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.retry-hostname-dns-lookup=true
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Oct 28 05:33:43
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO util.GSet: Computing capacity for map BlocksMap
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO util.GSet: VM type       = 64-bit
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO util.GSet: 2.0% max memory 731.7 MB = 14.6 MB
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO util.GSet: capacity      = 2^21 = 2097152 entries
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:43.672+0000: 16.116: [GC (Allocation Failure) 2019-10-28T05:33:43.672+0000: 16.116: [ParNew: 17342K->1920K(17856K), 0.0057295 secs] 23676K->8535K(57472K), 0.0057920 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManager: defaultReplication         = 2
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManager: maxReplication             = 512
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManager: minReplication             = 1
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
<13>Oct 28 05:33:43 google-dataproc-startup[803]: <13>Oct 28 05:33:43 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:43 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO namenode.FSNamesystem: Append Enabled: true
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 9.'
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 9.'
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 9.
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO util.GSet: Computing capacity for map INodeMap
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO util.GSet: VM type       = 64-bit
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO util.GSet: 1.0% max memory 731.7 MB = 7.3 MB
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO util.GSet: capacity      = 2^20 = 1048576 entries
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:44.634+0000: 17.079: [GC (Allocation Failure) 2019-10-28T05:33:44.634+0000: 17.079: [ParNew: 17856K->732K(17856K), 0.1638400 secs] 24471K->15840K(57472K), 0.1638934 secs] [Times: user=0.02 sys=0.01, real=0.16 secs] 
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO namenode.FSDirectory: ACLs enabled? false
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO namenode.FSDirectory: XAttrs enabled? true
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO namenode.NameNode: Caching file names occurring more than 10 times
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO util.GSet: Computing capacity for map cachedBlocks
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO util.GSet: VM type       = 64-bit
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO util.GSet: 0.25% max memory 731.7 MB = 1.8 MB
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO util.GSet: capacity      = 2^18 = 262144 entries
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
<13>Oct 28 05:33:44 google-dataproc-startup[803]: <13>Oct 28 05:33:44 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:44 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:45 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:45 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:45 INFO util.GSet: Computing capacity for map NameNodeRetryCache
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:45 INFO util.GSet: VM type       = 64-bit
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:45 INFO util.GSet: 0.029999999329447746% max memory 731.7 MB = 224.8 KB
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:45 INFO util.GSet: capacity      = 2^15 = 32768 entries
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:45 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1318458561-10.128.0.34-1572240825224
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:45 INFO common.Storage: Storage directory /hadoop/dfs/name has been successfully formatted.
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:45 INFO namenode.FSImageFormatProtobuf: Saving image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 10.'
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 10.'
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 10.
<13>Oct 28 05:33:45 google-dataproc-startup[803]: <13>Oct 28 05:33:45 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:45.986+0000: 18.430: [GC (Allocation Failure) 2019-10-28T05:33:45.986+0000: 18.430: [ParNew: 16668K->1920K(17856K), 0.1194309 secs] 31776K->21819K(57472K), 0.1194918 secs] [Times: user=0.01 sys=0.00, real=0.12 secs] 
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:46.107+0000: 18.551: [GC (CMS Initial Mark) [1 CMS-initial-mark: 19899K(39616K)] 21954K(57472K), 0.0341931 secs] [Times: user=0.01 sys=0.00, real=0.03 secs] 
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:46.152+0000: 18.597: [CMS-concurrent-mark-start]
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 uninstall[1400]: Removing krb5-user (1.15-1+deb9u1) ...
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:46.295+0000: 18.739: [CMS-concurrent-mark: 0.143/0.143 secs] [Times: user=0.02 sys=0.01, real=0.14 secs] 
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:46.296+0000: 18.741: [CMS-concurrent-preclean-start]
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:46.302+0000: 18.746: [CMS-concurrent-preclean: 0.001/0.001 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] 
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 uninstall[1400]: Removing krb5-config (2.6) ...
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:46.328+0000: 18.772: [GC (CMS Final Remark) [YG occupancy: 3694 K (17856 K)]2019-10-28T05:33:46.328+0000: 18.772: [Rescan (parallel) , 0.0025350 secs]2019-10-28T05:33:46.330+0000: 18.774: [weak refs processing, 0.0000719 secs]2019-10-28T05:33:46.330+0000: 18.774: [class unloading, 0.0024327 secs]2019-10-28T05:33:46.344+0000: 18.788: [scrub symbol table, 0.0089476 secs]2019-10-28T05:33:46.382+0000: 18.826: [scrub string table, 0.0004679 secs][1 CMS-remark: 19899K(39616K)] 23593K(57472K), 0.0567222 secs] [Times: user=0.01 sys=0.00, real=0.06 secs] 
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:46.394+0000: 18.838: [CMS-concurrent-sweep-start]
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:46.396+0000: 18.840: [CMS-concurrent-sweep: 0.002/0.002 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:46.396+0000: 18.840: [CMS-concurrent-reset-start]
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 uninstall[1400]: Removing bind9-host (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 2019-10-28T05:33:46.534+0000: 19.004: [CMS-concurrent-reset: 0.137/0.137 secs] [Times: user=0.02 sys=0.01, real=0.16 secs] 
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:46 INFO namenode.FSImageFormatProtobuf: Image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 319 bytes saved in 1 seconds .
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 uninstall[1400]: Removing druid (0.13.0-incubating-1) ...
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 11.'
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 11.'
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 11.
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:46 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: 19/10/28 05:33:46 INFO namenode.NameNode: SHUTDOWN_MSG: 
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: /************************************************************
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: SHUTDOWN_MSG: Shutting down NameNode at cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal/10.128.0.34
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: ************************************************************/
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 uninstall[1400]: dpkg: warning: while removing druid, directory '/usr/lib/druid/extensions/mysql-metadata-storage' not empty so not removed
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 uninstall[1400]: dpkg: warning: while removing druid, directory '/usr/lib/druid/conf/druid/_common' not empty so not removed
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 uninstall[1400]: Removing r-cran-rmarkdown (1.11+dfsg-1~bpo9+1) ...
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]: Heap
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]:  par new generation   total 17856K, used 8964K [0x00000000d1c00000, 0x00000000d2f50000, 0x00000000d6f30000)
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]:   eden space 15936K,  44% used [0x00000000d1c00000, 0x00000000d22e10b0, 0x00000000d2b90000)
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]:   from space 1920K, 100% used [0x00000000d2b90000, 0x00000000d2d70000, 0x00000000d2d70000)
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]:   to   space 1920K,   0% used [0x00000000d2d70000, 0x00000000d2d70000, 0x00000000d2f50000)
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]:  concurrent mark-sweep generation total 39616K, used 16990K [0x00000000d6f30000, 0x00000000d95e0000, 0x0000000100000000)
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]:  Metaspace       used 19008K, capacity 19224K, committed 19456K, reserved 1067008K
<13>Oct 28 05:33:46 google-dataproc-startup[803]: <13>Oct 28 05:33:46 setup-hadoop-hdfs-namenode[1463]:   class space    used 2133K, capacity 2192K, committed 2304K, reserved 1048576K
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + update_succeeded=1
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + break
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + ((  1  ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + enable_service hadoop-hdfs-namenode
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + local service=hadoop-hdfs-namenode
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + local unit=hadoop-hdfs-namenode.service
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + run_with_retries systemctl enable hadoop-hdfs-namenode.service
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + local retry_backoff
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + cmd=("$@")
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + local -a cmd
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + loginfo 'About to run '\''systemctl enable hadoop-hdfs-namenode.service'\'' with retries...'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + echo 'About to run '\''systemctl enable hadoop-hdfs-namenode.service'\'' with retries...'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: About to run 'systemctl enable hadoop-hdfs-namenode.service' with retries...
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + local update_succeeded=0
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + (( i = 0 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + (( i < 12 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: + systemctl enable hadoop-hdfs-namenode.service
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: hadoop-hdfs-namenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hadoop-hdfs-namenode[1463]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-namenode
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 uninstall[1400]: Removing r-cran-shiny (1.2.0+dfsg-1~bpo9+1) ...
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 uninstall[1400]: Removing fonts-font-awesome (4.7.0~dfsg-1) ...
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 uninstall[1400]: Removing r-cran-knitr (1.21+dfsg-2~bpo9+1) ...
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: + update_succeeded=1
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: + break
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: + ((  1  ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: + [[ google-fluentd == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: ++ get_dataproc_property am.primary_only
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: ++ local property_name=am.primary_only
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 12.'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 12.'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 12.
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: +++ local property_name=am.primary_only
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: ++++ tail -n 1
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: ++++ cut -d = -f 2-
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: +++ local property_value=false
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: +++ echo false
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + pid=1496
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + cmd='setup_service hadoop-hdfs-secondarynamenode'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1496 cmd=[setup_service hadoop-hdfs-secondarynamenode]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + echo 'Waiting on pid=1496 cmd=[setup_service hadoop-hdfs-secondarynamenode]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: Waiting on pid=1496 cmd=[setup_service hadoop-hdfs-secondarynamenode]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + wait 1496
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + pid=1487
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + cmd='setup_service mariadb'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1487 cmd=[setup_service mariadb]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + echo 'Waiting on pid=1487 cmd=[setup_service mariadb]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: Waiting on pid=1487 cmd=[setup_service mariadb]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + wait 1487
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + pid=1469
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + cmd='setup_service hadoop-yarn-timelineserver'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1469 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + echo 'Waiting on pid=1469 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: Waiting on pid=1469 cmd=[setup_service hadoop-yarn-timelineserver]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + wait 1469
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + pid=1468
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + cmd='setup_service spark-history-server'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1468 cmd=[setup_service spark-history-server]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + echo 'Waiting on pid=1468 cmd=[setup_service spark-history-server]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: Waiting on pid=1468 cmd=[setup_service spark-history-server]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + wait 1468
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + pid=1467
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + cmd='setup_service hadoop-mapreduce-historyserver'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1467 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + echo 'Waiting on pid=1467 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: Waiting on pid=1467 cmd=[setup_service hadoop-mapreduce-historyserver]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + wait 1467
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + pid=1466
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + cmd='setup_service hive-server2'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1466 cmd=[setup_service hive-server2]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + echo 'Waiting on pid=1466 cmd=[setup_service hive-server2]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: Waiting on pid=1466 cmd=[setup_service hive-server2]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + wait 1466
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + pid=1465
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + cmd='setup_service hive-metastore'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1465 cmd=[setup_service hive-metastore]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + echo 'Waiting on pid=1465 cmd=[setup_service hive-metastore]'
<13>Oct 28 05:33:47 google-dataproc-startup[803]: Waiting on pid=1465 cmd=[setup_service hive-metastore]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:33:47 google-dataproc-startup[803]: + wait 1465
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: ++ local property_value=false
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: ++ echo false
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 setup-google-fluentd[1571]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 05:33:47 google-dataproc-startup[803]: <13>Oct 28 05:33:47 uninstall[1400]: Removing r-cran-markdown (0.9+dfsg-1~bpo9+1) ...
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 uninstall[1400]: Removing libjs-mathjax (2.7.0-2) ...
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + update_succeeded=1
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + break
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + ((  1  ))
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: ++ systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + local 'props=Restart=no
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: RemainAfterExit=no'
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + [[ Restart=no
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + [[ Restart=no
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + local drop_in_dir=/etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + mkdir /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + in_array hadoop-hdfs-namenode DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + local value=hadoop-hdfs-namenode
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-hdfs-namenode  ]]
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + return 1
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + run_with_retries systemctl start hadoop-hdfs-namenode
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + local retry_backoff
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + cmd=("$@")
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + local -a cmd
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + loginfo 'About to run '\''systemctl start hadoop-hdfs-namenode'\'' with retries...'
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + echo 'About to run '\''systemctl start hadoop-hdfs-namenode'\'' with retries...'
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: About to run 'systemctl start hadoop-hdfs-namenode' with retries...
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + local update_succeeded=0
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + (( i = 0 ))
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + (( i < 12 ))
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: + systemctl start hadoop-hdfs-namenode
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 13.'
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 13.'
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 13.
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:48 google-dataproc-startup[803]: <13>Oct 28 05:33:48 setup-hadoop-hdfs-namenode[1463]: Warning: hadoop-hdfs-namenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 28 05:33:49 google-dataproc-startup[803]: <13>Oct 28 05:33:49 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:49 google-dataproc-startup[803]: <13>Oct 28 05:33:49 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:49 google-dataproc-startup[803]: <13>Oct 28 05:33:49 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:49 google-dataproc-startup[803]: <13>Oct 28 05:33:49 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 14.'
<13>Oct 28 05:33:49 google-dataproc-startup[803]: <13>Oct 28 05:33:49 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 14.'
<13>Oct 28 05:33:49 google-dataproc-startup[803]: <13>Oct 28 05:33:49 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 14.
<13>Oct 28 05:33:49 google-dataproc-startup[803]: <13>Oct 28 05:33:49 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:49 google-dataproc-startup[803]: <13>Oct 28 05:33:49 uninstall[1400]: Removing fonts-mathjax (2.7.0-2) ...
<13>Oct 28 05:33:50 google-dataproc-startup[803]: <13>Oct 28 05:33:50 uninstall[1400]: Removing geoip-database (20170512-1) ...
<13>Oct 28 05:33:50 google-dataproc-startup[803]: <13>Oct 28 05:33:50 uninstall[1400]: Removing hadoop-hdfs-datanode (2.9.2-1) ...
<13>Oct 28 05:33:50 google-dataproc-startup[803]: <13>Oct 28 05:33:50 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:50 google-dataproc-startup[803]: <13>Oct 28 05:33:50 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:50 google-dataproc-startup[803]: <13>Oct 28 05:33:50 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:50 google-dataproc-startup[803]: <13>Oct 28 05:33:50 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 15.'
<13>Oct 28 05:33:50 google-dataproc-startup[803]: <13>Oct 28 05:33:50 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 15.'
<13>Oct 28 05:33:50 google-dataproc-startup[803]: <13>Oct 28 05:33:50 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 15.
<13>Oct 28 05:33:50 google-dataproc-startup[803]: <13>Oct 28 05:33:50 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:51 google-dataproc-startup[803]: <13>Oct 28 05:33:51 uninstall[1400]: Removing hadoop-hdfs-journalnode (2.9.2-1) ...
<13>Oct 28 05:33:51 google-dataproc-startup[803]: <13>Oct 28 05:33:51 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:51 google-dataproc-startup[803]: <13>Oct 28 05:33:51 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:51 google-dataproc-startup[803]: <13>Oct 28 05:33:51 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:51 google-dataproc-startup[803]: <13>Oct 28 05:33:51 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 16.'
<13>Oct 28 05:33:51 google-dataproc-startup[803]: <13>Oct 28 05:33:51 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 16.'
<13>Oct 28 05:33:51 google-dataproc-startup[803]: <13>Oct 28 05:33:51 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 16.
<13>Oct 28 05:33:51 google-dataproc-startup[803]: <13>Oct 28 05:33:51 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:52 google-dataproc-startup[803]: <13>Oct 28 05:33:52 uninstall[1400]: Removing hadoop-hdfs-zkfc (2.9.2-1) ...
<13>Oct 28 05:33:52 google-dataproc-startup[803]: <13>Oct 28 05:33:52 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:52 google-dataproc-startup[803]: <13>Oct 28 05:33:52 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:52 google-dataproc-startup[803]: <13>Oct 28 05:33:52 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:52 google-dataproc-startup[803]: <13>Oct 28 05:33:52 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 17.'
<13>Oct 28 05:33:52 google-dataproc-startup[803]: <13>Oct 28 05:33:52 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 17.'
<13>Oct 28 05:33:52 google-dataproc-startup[803]: <13>Oct 28 05:33:52 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 17.
<13>Oct 28 05:33:52 google-dataproc-startup[803]: <13>Oct 28 05:33:52 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:53 google-dataproc-startup[803]: <13>Oct 28 05:33:53 uninstall[1400]: Removing hadoop-yarn-nodemanager (2.9.2-1) ...
<13>Oct 28 05:33:53 google-dataproc-startup[803]: <13>Oct 28 05:33:53 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:53 google-dataproc-startup[803]: <13>Oct 28 05:33:53 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:54 google-dataproc-startup[803]: <13>Oct 28 05:33:54 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:54 google-dataproc-startup[803]: <13>Oct 28 05:33:54 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 18.'
<13>Oct 28 05:33:54 google-dataproc-startup[803]: <13>Oct 28 05:33:54 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 18.'
<13>Oct 28 05:33:54 google-dataproc-startup[803]: <13>Oct 28 05:33:54 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 18.
<13>Oct 28 05:33:54 google-dataproc-startup[803]: <13>Oct 28 05:33:54 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:55 google-dataproc-startup[803]: <13>Oct 28 05:33:55 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:55 google-dataproc-startup[803]: <13>Oct 28 05:33:55 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:55 google-dataproc-startup[803]: <13>Oct 28 05:33:55 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:55 google-dataproc-startup[803]: <13>Oct 28 05:33:55 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 19.'
<13>Oct 28 05:33:55 google-dataproc-startup[803]: <13>Oct 28 05:33:55 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 19.'
<13>Oct 28 05:33:55 google-dataproc-startup[803]: <13>Oct 28 05:33:55 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 19.
<13>Oct 28 05:33:55 google-dataproc-startup[803]: <13>Oct 28 05:33:55 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:55 google-dataproc-startup[803]: <13>Oct 28 05:33:55 uninstall[1400]: Removing hive-webhcat-server (2.3.5-1) ...
<13>Oct 28 05:33:56 google-dataproc-startup[803]: <13>Oct 28 05:33:56 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:56 google-dataproc-startup[803]: <13>Oct 28 05:33:56 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:56 google-dataproc-startup[803]: <13>Oct 28 05:33:56 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:56 google-dataproc-startup[803]: <13>Oct 28 05:33:56 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 20.'
<13>Oct 28 05:33:56 google-dataproc-startup[803]: <13>Oct 28 05:33:56 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 20.'
<13>Oct 28 05:33:56 google-dataproc-startup[803]: <13>Oct 28 05:33:56 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 20.
<13>Oct 28 05:33:56 google-dataproc-startup[803]: <13>Oct 28 05:33:56 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 uninstall[1400]: Removing hive-webhcat (2.3.5-1) ...
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 21.'
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 21.'
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 21.
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 uninstall[1400]: Removing javascript-common (11) ...
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 uninstall[1400]: Removing kafka-server (1.1.1-1) ...
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 uninstall[1400]: Removing kafka (1.1.1-1) ...
<13>Oct 28 05:33:57 google-dataproc-startup[803]: <13>Oct 28 05:33:57 uninstall[1400]: Removing knox (1.1.0-1) ...
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 22.'
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 22.'
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 22.
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 uninstall[1400]: Removing libbind9-140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + update_succeeded=1
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + break
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + ((  1  ))
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + [[ 0 -eq 0 ]]
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + loginfo 'Waiting for namenode to listen on rpc port'
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + echo 'Waiting for namenode to listen on rpc port'
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: Waiting for namenode to listen on rpc port
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + wait_for_port cluster-4def-m 8020
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + local -r host=cluster-4def-m
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + local -r port=8020
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + loginfo 'Waiting for service to come up on host=cluster-4def-m port=8020.'
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + echo 'Waiting for service to come up on host=cluster-4def-m port=8020.'
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: Waiting for service to come up on host=cluster-4def-m port=8020.
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + retry_with_constant_backoff nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + local max_retry=300
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + cmd=("$@")
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + local -a cmd
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + local update_succeeded=0
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: ++ seq 1 300
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 1.'
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 1.'
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 1.
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 uninstall[1400]: Removing node-highlight.js (8.2+ds-5) ...
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 uninstall[1400]: Removing nodejs (8.11.1~dfsg-2~bpo9+1) ...
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 uninstall[1400]: Removing libc-ares2:amd64 (1.14.0-1~bpo9+1) ...
<13>Oct 28 05:33:58 google-dataproc-startup[803]: <13>Oct 28 05:33:58 uninstall[1400]: Removing libisccfg140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 uninstall[1400]: Removing libdns162:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 23.'
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 23.'
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 23.
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 uninstall[1400]: Removing update-inetd (4.44) ...
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 uninstall[1400]: Removing libfile-copy-recursive-perl (0.38-1) ...
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 uninstall[1400]: Removing liblwres141:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 uninstall[1400]: Removing libisccc140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 2.'
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 2.'
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 2.
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 uninstall[1400]: Removing libkadm5srv-mit11:amd64 (1.15-1+deb9u1) ...
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 uninstall[1400]: Removing libkdb5-8:amd64 (1.15-1+deb9u1) ...
<13>Oct 28 05:33:59 google-dataproc-startup[803]: <13>Oct 28 05:33:59 uninstall[1400]: Removing libkadm5clnt-mit11:amd64 (1.15-1+deb9u1) ...
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 uninstall[1400]: Removing libgssrpc4:amd64 (1.15-1+deb9u1) ...
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 uninstall[1400]: Removing libhttp-parser2.8:amd64 (2.8.1-1~bpo9+1) ...
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 24.'
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 24.'
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 24.
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 uninstall[1400]: Removing libisc160:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 uninstall[1400]: Removing libjs-bootstrap (3.3.7+dfsg-2+deb9u2) ...
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 uninstall[1400]: Removing libjs-d3 (3.5.17-2) ...
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 uninstall[1400]: Removing libjs-es5-shim (4.5.9-1) ...
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 3.'
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 3.'
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 3.
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 uninstall[1400]: Removing r-cran-highr (0.6-1) ...
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 uninstall[1400]: Removing libjs-highlight.js (8.2+ds-5) ...
<13>Oct 28 05:34:00 google-dataproc-startup[803]: <13>Oct 28 05:34:00 uninstall[1400]: Removing libjs-jquery-selectize.js (0.12.4+dfsg-1~bpo9+1) ...
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 uninstall[1400]: Removing libjs-twitter-bootstrap-datepicker (1.3.1+dfsg1-1) ...
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 uninstall[1400]: Removing libjs-twitter-bootstrap (2.0.2+dfsg-10) ...
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 25.'
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 25.'
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 25.
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 uninstall[1400]: Removing libjs-jquery-tablesorter (1:2.31.1+dfsg1-1~bpo9+1) ...
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 uninstall[1400]: Removing libjs-jquery-datatables (1.10.13+dfsg-2) ...
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 uninstall[1400]: Removing libjs-jquery-metadata (11-3) ...
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 uninstall[1400]: Removing libjs-jquery-ui (1.12.1+dfsg-4) ...
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 4.'
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 4.'
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 4.
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 uninstall[1400]: Removing libjs-json (0~20160510-1) ...
<13>Oct 28 05:34:01 google-dataproc-startup[803]: <13>Oct 28 05:34:01 uninstall[1400]: Removing libjs-microplugin.js (0.0.3+dfsg-1) ...
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 uninstall[1400]: Removing libjs-modernizr (2.6.2+ds1-1) ...
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 uninstall[1400]: Removing libjs-prettify (2013.03.04+dfsg-4) ...
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 26.'
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 26.'
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 26.
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 uninstall[1400]: Removing libjs-sifter.js (0.5.1+dfsg-2) ...
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 uninstall[1400]: Removing pandoc (1.17.2~dfsg-3) ...
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 uninstall[1400]: Removing liblua5.1-0:amd64 (5.1.5-8.1+b2) ...
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 uninstall[1400]: Removing libluajit-5.1-2:amd64 (2.1.0~beta3+dfsg-5.1~bpo9+1) ...
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 5.'
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 5.'
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 5.
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 uninstall[1400]: Removing libluajit-5.1-common (2.1.0~beta3+dfsg-5.1~bpo9+1) ...
<13>Oct 28 05:34:02 google-dataproc-startup[803]: <13>Oct 28 05:34:02 uninstall[1400]: Removing r-cran-httpuv (1.4.5.1+dfsg-1~bpo9+1) ...
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 uninstall[1400]: Removing libuv1:amd64 (1.18.0-3~bpo9+1) ...
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 uninstall[1400]: Removing libyaml-0-2:amd64 (0.2.1-1~bpo9+1) ...
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 uninstall[1400]: Removing r-cran-dplyr (0.7.8-1~bpo9+1) ...
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 27.'
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 27.'
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 27.
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 uninstall[1400]: Removing r-cran-tidyselect (0.2.5-1~bpo9+1) ...
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 uninstall[1400]: Removing r-cran-ggplot2 (3.1.0-1~bpo9+1) ...
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 6.'
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 6.'
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 6.
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:03 google-dataproc-startup[803]: <13>Oct 28 05:34:03 uninstall[1400]: Removing r-cran-scales (1.0.0-1~bpo9+1) ...
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 uninstall[1400]: Removing littler (0.3.1-1) ...
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 uninstall[1400]: Removing node-normalize.css (8.0.0-3~bpo9+1) ...
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 uninstall[1400]: Removing nodejs-doc (8.11.1~dfsg-2~bpo9+1) ...
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 28.'
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 28.'
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 28.
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 uninstall[1400]: Removing pandoc-data (1.17.2~dfsg-3) ...
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 uninstall[1400]: Removing r-cran-tibble (2.0.1-1~bpo9+1) ...
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 uninstall[1400]: Removing r-cran-pillar (1.3.1-1~bpo9+1) ...
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 7.'
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 7.'
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 7.
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:04 google-dataproc-startup[803]: <13>Oct 28 05:34:04 uninstall[1400]: Removing r-cran-base64enc (0.1-3-1) ...
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 uninstall[1400]: Removing r-cran-bindrcpp (0.2.2-2~bpo9+1) ...
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 uninstall[1400]: Removing r-cran-bindr (0.1.1-2~bpo9+1) ...
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 uninstall[1400]: Removing r-cran-bit64 (0.9-7-2~bpo9+1) ...
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 29.'
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 29.'
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 29.
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 uninstall[1400]: Removing r-cran-bit (1.1-14-1~bpo9+1) ...
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 uninstall[1400]: Removing r-cran-munsell (0.5.0-1~bpo9+1) ...
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 uninstall[1400]: Removing r-cran-colorspace (1.3-2-1) ...
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 uninstall[1400]: Removing r-cran-testthat (2.0.1-1~bpo9+1) ...
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 8.'
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 8.'
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 8.
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:05 google-dataproc-startup[803]: <13>Oct 28 05:34:05 uninstall[1400]: Removing r-cran-data.table (1.10.0-1) ...
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 uninstall[1400]: Removing r-cran-rsqlite (1.1-2-1) ...
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 uninstall[1400]: Removing r-cran-dbi (1.0.0-1~bpo9+2) ...
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 uninstall[1400]: Removing r-cran-memoise (1.1.0-1~bpo9+1) ...
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 30.'
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 30.'
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 30.
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 uninstall[1400]: Removing r-cran-htmlwidgets (1.3+dfsg-1~bpo9+1) ...
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 uninstall[1400]: Removing r-cran-htmltools (0.3.6-2~bpo9+1) ...
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 uninstall[1400]: Removing r-cran-digest (0.6.11-1) ...
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 9.'
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 9.'
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 9.
<13>Oct 28 05:34:06 google-dataproc-startup[803]: <13>Oct 28 05:34:06 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 uninstall[1400]: Removing r-cran-evaluate (0.10-1) ...
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 uninstall[1400]: Removing r-cran-fansi (0.4.0-1~bpo9+1) ...
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 uninstall[1400]: Removing r-cran-tikzdevice (0.10-1-1) ...
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 uninstall[1400]: Removing r-cran-filehash (2.3-1) ...
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 31.'
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 31.'
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 31.
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 uninstall[1400]: Removing r-cran-reshape2 (1.4.2-1) ...
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 uninstall[1400]: Removing r-cran-stringr (1.4.0-1~bpo9+1) ...
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 uninstall[1400]: Removing r-cran-glue (1.3.0-1~bpo9+1) ...
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 uninstall[1400]: Removing r-cran-googlevis (0.6.2-1) ...
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 uninstall[1400]: Removing r-cran-gtable (0.2.0-1) ...
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 10.'
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 10.'
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 10.
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:07 google-dataproc-startup[803]: <13>Oct 28 05:34:07 uninstall[1400]: Removing r-cran-hexbin (1.27.1-1) ...
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 uninstall[1400]: Removing r-cran-hms (0.4.2-1~bpo9+1) ...
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 uninstall[1400]: Removing r-cran-jsonlite (1.6+dfsg-1~bpo9+1) ...
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 uninstall[1400]: Removing r-cran-labeling (0.3-1) ...
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 uninstall[1400]: Removing r-cran-promises (1.0.1-2~bpo9+1) ...
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 uninstall[1400]: Removing r-cran-later (0.7.5+dfsg-2~bpo9+1) ...
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 uninstall[1400]: Removing r-cran-lazyeval (0.2.0-1) ...
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 32.'
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 32.'
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 32.
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 uninstall[1400]: Removing r-cran-purrr (0.3.0-1~bpo9+1) ...
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 uninstall[1400]: Removing r-cran-magrittr (1.5-3) ...
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 uninstall[1400]: Removing r-cran-mapproj (1.2-4-1) ...
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 uninstall[1400]: Removing r-cran-maps (3.1.1-1) ...
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:08 google-dataproc-startup[803]: <13>Oct 28 05:34:08 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 11.'
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 11.'
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 11.
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 uninstall[1400]: Removing r-cran-mime (0.5-1) ...
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 uninstall[1400]: Removing r-cran-pkgconfig (2.0.2-1~bpo9+1) ...
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 uninstall[1400]: Removing r-cran-plyr (1.8.4-1) ...
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 uninstall[1400]: Removing r-cran-png (0.1-7-1) ...
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 uninstall[1400]: Removing r-cran-praise (1.0.0-1) ...
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 33.'
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 33.'
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 33.
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 uninstall[1400]: Removing r-cran-r6 (2.4.0-1~bpo9+1) ...
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 uninstall[1400]: Removing r-cran-rcolorbrewer (1.1-2-1) ...
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 uninstall[1400]: Removing r-cran-rlang (0.3.1-2~bpo9+1) ...
<13>Oct 28 05:34:09 google-dataproc-startup[803]: <13>Oct 28 05:34:09 uninstall[1400]: Removing r-cran-sourcetools (0.1.5-1) ...
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 12.'
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 12.'
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 12.
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 uninstall[1400]: Removing r-cran-sp (1:1.2-4-1) ...
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 uninstall[1400]: Removing r-cran-stringi (1.2.4-2~bpo9+1) ...
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 uninstall[1400]: Removing r-cran-testit (0.6-1) ...
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 uninstall[1400]: Removing r-cran-tinytex (0.10-1~bpo9+1) ...
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 uninstall[1400]: Removing r-cran-utf8 (1.1.4-1~bpo9+1) ...
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 34.'
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 34.'
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 34.
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 uninstall[1400]: Removing r-cran-viridislite (0.3.0-3~bpo9+1) ...
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 uninstall[1400]: Removing r-cran-withr (2.1.2-1~bpo9+1) ...
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 uninstall[1400]: Removing r-cran-xfun (0.4-1~bpo9+1) ...
<13>Oct 28 05:34:10 google-dataproc-startup[803]: <13>Oct 28 05:34:10 uninstall[1400]: Removing r-cran-xml2 (1.1.0-1) ...
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 13.'
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 13.'
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 13.
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 uninstall[1400]: Removing r-cran-xtable (1:1.8-2-1) ...
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 uninstall[1400]: Removing r-cran-yaml (2.2.0-1~bpo9+1) ...
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 uninstall[1400]: Removing solr-server (6.6.5-1) ...
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 uninstall[1400]: Removing solr (6.6.5-1) ...
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 35.'
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 35.'
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 35.
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:11 google-dataproc-startup[803]: <13>Oct 28 05:34:11 uninstall[1400]: Removing xinetd (1:2.3.15-7) ...
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 14.'
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 14.'
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 14.
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 36.'
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 36.'
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 36.
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:12 google-dataproc-startup[803]: <13>Oct 28 05:34:12 uninstall[1400]: Removing zeppelin (0.8.0-1) ...
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 15.'
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 15.'
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 15.
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 37.'
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 37.'
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 37.
<13>Oct 28 05:34:13 google-dataproc-startup[803]: <13>Oct 28 05:34:13 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hadoop-hdfs-namenode[1463]: nc: connect to cluster-4def-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 16.'
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 16.'
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 failed. Retry attempt: 16.
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hadoop-hdfs-namenode[1463]: + sleep 1
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 uninstall[1400]: Removing zookeeper-server (3.4.13-1) ...
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 38.'
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 38.'
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 38.
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:14 google-dataproc-startup[803]: <13>Oct 28 05:34:14 uninstall[1400]: Removing libgeoip1:amd64 (1.6.9-4) ...
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 uninstall[1400]: Removing libjs-jquery (3.1.1-2+deb9u1) ...
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 uninstall[1400]: Removing r-cran-rcpp (1.0.0-1~bpo9+1) ...
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + nc -v -z -w 0 cluster-4def-m 8020
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: Connection to cluster-4def-m 8020 port [tcp/*] succeeded!
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + update_succeeded=1
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + loginfo 'nc -v -z -w 0 cluster-4def-m 8020 succeeded.'
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + echo 'nc -v -z -w 0 cluster-4def-m 8020 succeeded.'
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: nc -v -z -w 0 cluster-4def-m 8020 succeeded.
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + break
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + ((  1  ))
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + loginfo 'Service up on host=cluster-4def-m port=8020.'
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + echo 'Service up on host=cluster-4def-m port=8020.'
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: Service up on host=cluster-4def-m port=8020.
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + loginfo 'Initializing HDFS directories'
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + echo 'Initializing HDFS directories'
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: Initializing HDFS directories
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + HADOOP_USERS=(hdfs mapred yarn spark pig hive hbase zookeeper)
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + REAL_USERS=($(getent passwd | awk -F: '1000 < $3 && $3 < 6000 { print $1}'))
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: ++ getent passwd
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: ++ awk -F: '1000 < $3 && $3 < 6000 { print $1}'
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + HDFS_USERS=("${HADOOP_USERS[@]}" "${REAL_USERS[@]}")
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + HDFS_USER_DIRS=("${HDFS_USERS[@]/#//user/}")
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.eventLog.dir
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: ++ local property_file=/etc/spark/conf/spark-defaults.conf
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: ++ local property_name=spark.eventLog.dir
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: +++ grep '^spark.eventLog.dir=' /etc/spark/conf/spark-defaults.conf
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: +++ tail -n 1
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: +++ cut -d = -f 2-
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: ++ local property_value=hdfs://cluster-4def-m/user/spark/eventlog
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: ++ echo hdfs://cluster-4def-m/user/spark/eventlog
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + SPARK_EVENTLOG_DIR=hdfs://cluster-4def-m/user/spark/eventlog
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hadoop-hdfs-namenode[1463]: + su -s /bin/bash hdfs -c 'login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-4def-m.us-central1-a.c.lustrous-drake-255300.internal &&              hadoop fs -mkdir -p              /tmp/hadoop-yarn/staging/history /user/hdfs /user/mapred /user/yarn /user/spark /user/pig /user/hive /user/hbase /user/zookeeper hdfs://cluster-4def-m/user/spark/eventlog'
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 uninstall[1400]: Removing r-cran-cli (1.0.1-1~bpo9+1) ...
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 uninstall[1400]: Removing r-cran-assertthat (0.2.0-1~bpo9+1) ...
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 uninstall[1400]: Removing r-cran-crayon (1.3.4-2~bpo9+1) ...
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 uninstall[1400]: Removing r-cran-littler (0.3.1-1) ...
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 uninstall[1400]: Removing r-cran-pkgkitten (0.1.4-1) ...
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 39.'
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 39.'
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 39.
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 uninstall[1400]: Removing libverto1:amd64 (0.2.4-2.1) ...
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 uninstall[1400]: Removing libverto-libev1:amd64 (0.2.4-2.1) ...
<13>Oct 28 05:34:15 google-dataproc-startup[803]: <13>Oct 28 05:34:15 uninstall[1400]: Removing libev4 (1:4.22-1+b1) ...
<13>Oct 28 05:34:16 google-dataproc-startup[803]: <13>Oct 28 05:34:16 uninstall[1400]: Processing triggers for libc-bin (2.24-11+deb9u4) ...
<13>Oct 28 05:34:16 google-dataproc-startup[803]: <13>Oct 28 05:34:16 uninstall[1400]: Processing triggers for man-db (2.7.6.1-2) ...
<13>Oct 28 05:34:16 google-dataproc-startup[803]: <13>Oct 28 05:34:16 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:16 google-dataproc-startup[803]: <13>Oct 28 05:34:16 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:16 google-dataproc-startup[803]: <13>Oct 28 05:34:16 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:16 google-dataproc-startup[803]: <13>Oct 28 05:34:16 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 40.'
<13>Oct 28 05:34:16 google-dataproc-startup[803]: <13>Oct 28 05:34:16 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 40.'
<13>Oct 28 05:34:16 google-dataproc-startup[803]: <13>Oct 28 05:34:16 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 40.
<13>Oct 28 05:34:16 google-dataproc-startup[803]: <13>Oct 28 05:34:16 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:17 google-dataproc-startup[803]: <13>Oct 28 05:34:17 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:17 google-dataproc-startup[803]: <13>Oct 28 05:34:17 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:17 google-dataproc-startup[803]: <13>Oct 28 05:34:17 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:17 google-dataproc-startup[803]: <13>Oct 28 05:34:17 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 41.'
<13>Oct 28 05:34:17 google-dataproc-startup[803]: <13>Oct 28 05:34:17 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 41.'
<13>Oct 28 05:34:17 google-dataproc-startup[803]: <13>Oct 28 05:34:17 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 41.
<13>Oct 28 05:34:17 google-dataproc-startup[803]: <13>Oct 28 05:34:17 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:18 google-dataproc-startup[803]: <13>Oct 28 05:34:18 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:18 google-dataproc-startup[803]: <13>Oct 28 05:34:18 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:18 google-dataproc-startup[803]: <13>Oct 28 05:34:18 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:18 google-dataproc-startup[803]: <13>Oct 28 05:34:18 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 42.'
<13>Oct 28 05:34:18 google-dataproc-startup[803]: <13>Oct 28 05:34:18 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 42.'
<13>Oct 28 05:34:18 google-dataproc-startup[803]: <13>Oct 28 05:34:18 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 42.
<13>Oct 28 05:34:18 google-dataproc-startup[803]: <13>Oct 28 05:34:18 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:19 google-dataproc-startup[803]: <13>Oct 28 05:34:19 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:19 google-dataproc-startup[803]: <13>Oct 28 05:34:19 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:19 google-dataproc-startup[803]: <13>Oct 28 05:34:19 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:19 google-dataproc-startup[803]: <13>Oct 28 05:34:19 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 43.'
<13>Oct 28 05:34:19 google-dataproc-startup[803]: <13>Oct 28 05:34:19 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 43.'
<13>Oct 28 05:34:19 google-dataproc-startup[803]: <13>Oct 28 05:34:19 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 43.
<13>Oct 28 05:34:19 google-dataproc-startup[803]: <13>Oct 28 05:34:19 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:20 google-dataproc-startup[803]: <13>Oct 28 05:34:20 uninstall[1400]: Processing triggers for fontconfig (2.11.0-6.7+b1) ...
<13>Oct 28 05:34:20 google-dataproc-startup[803]: <13>Oct 28 05:34:20 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:20 google-dataproc-startup[803]: <13>Oct 28 05:34:20 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:20 google-dataproc-startup[803]: <13>Oct 28 05:34:20 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:20 google-dataproc-startup[803]: <13>Oct 28 05:34:20 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 44.'
<13>Oct 28 05:34:20 google-dataproc-startup[803]: <13>Oct 28 05:34:20 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 44.'
<13>Oct 28 05:34:20 google-dataproc-startup[803]: <13>Oct 28 05:34:20 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 44.
<13>Oct 28 05:34:20 google-dataproc-startup[803]: <13>Oct 28 05:34:20 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:21 google-dataproc-startup[803]: <13>Oct 28 05:34:21 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:21 google-dataproc-startup[803]: <13>Oct 28 05:34:21 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:21 google-dataproc-startup[803]: <13>Oct 28 05:34:21 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:21 google-dataproc-startup[803]: <13>Oct 28 05:34:21 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 45.'
<13>Oct 28 05:34:21 google-dataproc-startup[803]: <13>Oct 28 05:34:21 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 45.'
<13>Oct 28 05:34:21 google-dataproc-startup[803]: <13>Oct 28 05:34:21 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 45.
<13>Oct 28 05:34:21 google-dataproc-startup[803]: <13>Oct 28 05:34:21 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:22 google-dataproc-startup[803]: <13>Oct 28 05:34:22 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:22 google-dataproc-startup[803]: <13>Oct 28 05:34:22 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:22 google-dataproc-startup[803]: <13>Oct 28 05:34:22 uninstall[1400]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 102578 files and directories currently installed.)
<13>Oct 28 05:34:22 google-dataproc-startup[803]: <13>Oct 28 05:34:22 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:22 google-dataproc-startup[803]: <13>Oct 28 05:34:22 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 46.'
<13>Oct 28 05:34:22 google-dataproc-startup[803]: <13>Oct 28 05:34:22 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 46.'
<13>Oct 28 05:34:22 google-dataproc-startup[803]: <13>Oct 28 05:34:22 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 46.
<13>Oct 28 05:34:22 google-dataproc-startup[803]: <13>Oct 28 05:34:22 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:23 google-dataproc-startup[803]: <13>Oct 28 05:34:23 uninstall[1400]: Purging configuration files for update-inetd (4.44) ...
<13>Oct 28 05:34:23 google-dataproc-startup[803]: <13>Oct 28 05:34:23 uninstall[1400]: Purging configuration files for hive-webhcat-server (2.3.5-1) ...
<13>Oct 28 05:34:23 google-dataproc-startup[803]: <13>Oct 28 05:34:23 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:23 google-dataproc-startup[803]: <13>Oct 28 05:34:23 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 47.'
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 47.'
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 47.
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 uninstall[1400]: Purging configuration files for xinetd (1:2.3.15-7) ...
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: + run_with_retries sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: + local retry_backoff
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: + cmd=("$@")
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: + local -a cmd
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: + loginfo 'About to run '\''sudo -u hdfs hadoop fs -chmod -R 1777 /'\'' with retries...'
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: + echo 'About to run '\''sudo -u hdfs hadoop fs -chmod -R 1777 /'\'' with retries...'
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: About to run 'sudo -u hdfs hadoop fs -chmod -R 1777 /' with retries...
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: + local update_succeeded=0
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: + (( i = 0 ))
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: + (( i < 12 ))
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 setup-hadoop-hdfs-namenode[1463]: + sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Oct 28 05:34:24 google-dataproc-startup[803]: <13>Oct 28 05:34:24 uninstall[1400]: Purging configuration files for zookeeper-server (3.4.13-1) ...
<13>Oct 28 05:34:25 google-dataproc-startup[803]: <13>Oct 28 05:34:25 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:25 google-dataproc-startup[803]: <13>Oct 28 05:34:25 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:25 google-dataproc-startup[803]: <13>Oct 28 05:34:25 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:25 google-dataproc-startup[803]: <13>Oct 28 05:34:25 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 48.'
<13>Oct 28 05:34:25 google-dataproc-startup[803]: <13>Oct 28 05:34:25 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 48.'
<13>Oct 28 05:34:25 google-dataproc-startup[803]: <13>Oct 28 05:34:25 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 48.
<13>Oct 28 05:34:25 google-dataproc-startup[803]: <13>Oct 28 05:34:25 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:25 google-dataproc-startup[803]: <13>Oct 28 05:34:25 uninstall[1400]: Purging configuration files for hadoop-hdfs-journalnode (2.9.2-1) ...
<13>Oct 28 05:34:26 google-dataproc-startup[803]: <13>Oct 28 05:34:26 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:26 google-dataproc-startup[803]: <13>Oct 28 05:34:26 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:26 google-dataproc-startup[803]: <13>Oct 28 05:34:26 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:26 google-dataproc-startup[803]: <13>Oct 28 05:34:26 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 49.'
<13>Oct 28 05:34:26 google-dataproc-startup[803]: <13>Oct 28 05:34:26 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 49.'
<13>Oct 28 05:34:26 google-dataproc-startup[803]: <13>Oct 28 05:34:26 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 49.
<13>Oct 28 05:34:26 google-dataproc-startup[803]: <13>Oct 28 05:34:26 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:26 google-dataproc-startup[803]: <13>Oct 28 05:34:26 uninstall[1400]: Purging configuration files for solr (6.6.5-1) ...
<13>Oct 28 05:34:26 google-dataproc-startup[803]: <13>Oct 28 05:34:26 uninstall[1400]: Purging configuration files for krb5-admin-server (1.15-1+deb9u1) ...
<13>Oct 28 05:34:27 google-dataproc-startup[803]: <13>Oct 28 05:34:27 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:27 google-dataproc-startup[803]: <13>Oct 28 05:34:27 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:27 google-dataproc-startup[803]: <13>Oct 28 05:34:27 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:27 google-dataproc-startup[803]: <13>Oct 28 05:34:27 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 50.'
<13>Oct 28 05:34:27 google-dataproc-startup[803]: <13>Oct 28 05:34:27 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 50.'
<13>Oct 28 05:34:27 google-dataproc-startup[803]: <13>Oct 28 05:34:27 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 50.
<13>Oct 28 05:34:27 google-dataproc-startup[803]: <13>Oct 28 05:34:27 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:28 google-dataproc-startup[803]: <13>Oct 28 05:34:28 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:28 google-dataproc-startup[803]: <13>Oct 28 05:34:28 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:28 google-dataproc-startup[803]: <13>Oct 28 05:34:28 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:28 google-dataproc-startup[803]: <13>Oct 28 05:34:28 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 51.'
<13>Oct 28 05:34:28 google-dataproc-startup[803]: <13>Oct 28 05:34:28 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 51.'
<13>Oct 28 05:34:28 google-dataproc-startup[803]: <13>Oct 28 05:34:28 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 51.
<13>Oct 28 05:34:28 google-dataproc-startup[803]: <13>Oct 28 05:34:28 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:29 google-dataproc-startup[803]: <13>Oct 28 05:34:29 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:29 google-dataproc-startup[803]: <13>Oct 28 05:34:29 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:29 google-dataproc-startup[803]: <13>Oct 28 05:34:29 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:29 google-dataproc-startup[803]: <13>Oct 28 05:34:29 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 52.'
<13>Oct 28 05:34:29 google-dataproc-startup[803]: <13>Oct 28 05:34:29 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 52.'
<13>Oct 28 05:34:29 google-dataproc-startup[803]: <13>Oct 28 05:34:29 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 52.
<13>Oct 28 05:34:29 google-dataproc-startup[803]: <13>Oct 28 05:34:29 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:29 google-dataproc-startup[803]: <13>Oct 28 05:34:29 uninstall[1400]: Purging configuration files for kafka (1.1.1-1) ...
<13>Oct 28 05:34:29 google-dataproc-startup[803]: <13>Oct 28 05:34:29 uninstall[1400]: Purging configuration files for hadoop-hdfs-zkfc (2.9.2-1) ...
<13>Oct 28 05:34:30 google-dataproc-startup[803]: <13>Oct 28 05:34:30 uninstall[1400]: Purging configuration files for krb5-kdc (1.15-1+deb9u1) ...
<13>Oct 28 05:34:30 google-dataproc-startup[803]: <13>Oct 28 05:34:30 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:30 google-dataproc-startup[803]: <13>Oct 28 05:34:30 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:30 google-dataproc-startup[803]: <13>Oct 28 05:34:30 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:30 google-dataproc-startup[803]: <13>Oct 28 05:34:30 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 53.'
<13>Oct 28 05:34:30 google-dataproc-startup[803]: <13>Oct 28 05:34:30 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 53.'
<13>Oct 28 05:34:30 google-dataproc-startup[803]: <13>Oct 28 05:34:30 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 53.
<13>Oct 28 05:34:30 google-dataproc-startup[803]: <13>Oct 28 05:34:30 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:31 google-dataproc-startup[803]: <13>Oct 28 05:34:31 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:31 google-dataproc-startup[803]: <13>Oct 28 05:34:31 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:31 google-dataproc-startup[803]: <13>Oct 28 05:34:31 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:31 google-dataproc-startup[803]: <13>Oct 28 05:34:31 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 54.'
<13>Oct 28 05:34:31 google-dataproc-startup[803]: <13>Oct 28 05:34:31 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 54.'
<13>Oct 28 05:34:31 google-dataproc-startup[803]: <13>Oct 28 05:34:31 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 54.
<13>Oct 28 05:34:31 google-dataproc-startup[803]: <13>Oct 28 05:34:31 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:32 google-dataproc-startup[803]: <13>Oct 28 05:34:32 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:32 google-dataproc-startup[803]: <13>Oct 28 05:34:32 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:32 google-dataproc-startup[803]: <13>Oct 28 05:34:32 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:32 google-dataproc-startup[803]: <13>Oct 28 05:34:32 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 55.'
<13>Oct 28 05:34:32 google-dataproc-startup[803]: <13>Oct 28 05:34:32 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 55.'
<13>Oct 28 05:34:32 google-dataproc-startup[803]: <13>Oct 28 05:34:32 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 55.
<13>Oct 28 05:34:32 google-dataproc-startup[803]: <13>Oct 28 05:34:32 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:32 google-dataproc-startup[803]: <13>Oct 28 05:34:32 uninstall[1400]: Purging configuration files for libjs-jquery-ui (1.12.1+dfsg-4) ...
<13>Oct 28 05:34:32 google-dataproc-startup[803]: <13>Oct 28 05:34:32 uninstall[1400]: Purging configuration files for krb5-kpropd (1.15-1+deb9u1) ...
<13>Oct 28 05:34:33 google-dataproc-startup[803]: <13>Oct 28 05:34:33 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:33 google-dataproc-startup[803]: <13>Oct 28 05:34:33 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:33 google-dataproc-startup[803]: <13>Oct 28 05:34:33 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:33 google-dataproc-startup[803]: <13>Oct 28 05:34:33 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 56.'
<13>Oct 28 05:34:33 google-dataproc-startup[803]: <13>Oct 28 05:34:33 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 56.'
<13>Oct 28 05:34:33 google-dataproc-startup[803]: <13>Oct 28 05:34:33 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 56.
<13>Oct 28 05:34:33 google-dataproc-startup[803]: <13>Oct 28 05:34:33 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:33 google-dataproc-startup[803]: <13>Oct 28 05:34:33 uninstall[1400]: Purging configuration files for hadoop-hdfs-datanode (2.9.2-1) ...
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 57.'
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 57.'
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 57.
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 uninstall[1400]: Purging configuration files for javascript-common (11) ...
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + update_succeeded=1
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + break
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + ((  1  ))
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + run_with_retries sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-4def-m/user/spark/eventlog
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + local retry_backoff
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + cmd=("$@")
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + local -a cmd
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + loginfo 'About to run '\''sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-4def-m/user/spark/eventlog'\'' with retries...'
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + echo 'About to run '\''sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-4def-m/user/spark/eventlog'\'' with retries...'
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: About to run 'sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-4def-m/user/spark/eventlog' with retries...
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + local update_succeeded=0
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + (( i = 0 ))
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + (( i < 12 ))
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 setup-hadoop-hdfs-namenode[1463]: + sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-4def-m/user/spark/eventlog
<13>Oct 28 05:34:34 google-dataproc-startup[803]: <13>Oct 28 05:34:34 uninstall[1400]: Purging configuration files for krb5-config (2.6) ...
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 uninstall[1400]: Purging configuration files for knox (1.1.0-1) ...
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 58.'
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 58.'
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 58.
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 uninstall[1400]: Purging configuration files for r-cran-rmarkdown (1.11+dfsg-1~bpo9+1) ...
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 uninstall[1400]: Purging configuration files for hive-webhcat (2.3.5-1) ...
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 uninstall[1400]: Purging configuration files for solr-server (6.6.5-1) ...
<13>Oct 28 05:34:35 google-dataproc-startup[803]: <13>Oct 28 05:34:35 uninstall[1400]: Purging configuration files for kafka-server (1.1.1-1) ...
<13>Oct 28 05:34:36 google-dataproc-startup[803]: <13>Oct 28 05:34:36 uninstall[1400]: Purging configuration files for zeppelin (0.8.0-1) ...
<13>Oct 28 05:34:36 google-dataproc-startup[803]: <13>Oct 28 05:34:36 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:36 google-dataproc-startup[803]: <13>Oct 28 05:34:36 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:36 google-dataproc-startup[803]: <13>Oct 28 05:34:36 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:36 google-dataproc-startup[803]: <13>Oct 28 05:34:36 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 59.'
<13>Oct 28 05:34:36 google-dataproc-startup[803]: <13>Oct 28 05:34:36 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 59.'
<13>Oct 28 05:34:36 google-dataproc-startup[803]: <13>Oct 28 05:34:36 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 59.
<13>Oct 28 05:34:36 google-dataproc-startup[803]: <13>Oct 28 05:34:36 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:36 google-dataproc-startup[803]: <13>Oct 28 05:34:36 uninstall[1400]: Purging configuration files for hadoop-yarn-nodemanager (2.9.2-1) ...
<13>Oct 28 05:34:37 google-dataproc-startup[803]: <13>Oct 28 05:34:37 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:37 google-dataproc-startup[803]: <13>Oct 28 05:34:37 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:37 google-dataproc-startup[803]: <13>Oct 28 05:34:37 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:37 google-dataproc-startup[803]: <13>Oct 28 05:34:37 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 60.'
<13>Oct 28 05:34:37 google-dataproc-startup[803]: <13>Oct 28 05:34:37 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 60.'
<13>Oct 28 05:34:37 google-dataproc-startup[803]: <13>Oct 28 05:34:37 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 60.
<13>Oct 28 05:34:37 google-dataproc-startup[803]: <13>Oct 28 05:34:37 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:37 google-dataproc-startup[803]: <13>Oct 28 05:34:37 uninstall[1400]: Processing triggers for systemd (232-25+deb9u12) ...
<13>Oct 28 05:34:38 google-dataproc-startup[803]: <13>Oct 28 05:34:38 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:38 google-dataproc-startup[803]: <13>Oct 28 05:34:38 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:38 google-dataproc-startup[803]: <13>Oct 28 05:34:38 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:38 google-dataproc-startup[803]: <13>Oct 28 05:34:38 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 61.'
<13>Oct 28 05:34:38 google-dataproc-startup[803]: <13>Oct 28 05:34:38 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 61.'
<13>Oct 28 05:34:38 google-dataproc-startup[803]: <13>Oct 28 05:34:38 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 61.
<13>Oct 28 05:34:38 google-dataproc-startup[803]: <13>Oct 28 05:34:38 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:39 google-dataproc-startup[803]: <13>Oct 28 05:34:39 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:39 google-dataproc-startup[803]: <13>Oct 28 05:34:39 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:39 google-dataproc-startup[803]: <13>Oct 28 05:34:39 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:39 google-dataproc-startup[803]: <13>Oct 28 05:34:39 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 62.'
<13>Oct 28 05:34:39 google-dataproc-startup[803]: <13>Oct 28 05:34:39 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 62.'
<13>Oct 28 05:34:39 google-dataproc-startup[803]: <13>Oct 28 05:34:39 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 62.
<13>Oct 28 05:34:39 google-dataproc-startup[803]: <13>Oct 28 05:34:39 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 63.'
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 63.'
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 63.
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + update_succeeded=1
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + break
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + ((  1  ))
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + for SERVICE in "${DATAPROC_START_AFTER_HDFS_SERVICES[@]}"
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + run_with_retries systemctl start hadoop-mapreduce-historyserver
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + local retry_backoff
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + cmd=("$@")
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + local -a cmd
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + loginfo 'About to run '\''systemctl start hadoop-mapreduce-historyserver'\'' with retries...'
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + echo 'About to run '\''systemctl start hadoop-mapreduce-historyserver'\'' with retries...'
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: About to run 'systemctl start hadoop-mapreduce-historyserver' with retries...
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + local update_succeeded=0
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + (( i = 0 ))
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + (( i < 12 ))
<13>Oct 28 05:34:40 google-dataproc-startup[803]: <13>Oct 28 05:34:40 setup-hadoop-hdfs-namenode[1463]: + systemctl start hadoop-mapreduce-historyserver
<13>Oct 28 05:34:41 google-dataproc-startup[803]: <13>Oct 28 05:34:41 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:41 google-dataproc-startup[803]: <13>Oct 28 05:34:41 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:41 google-dataproc-startup[803]: <13>Oct 28 05:34:41 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:41 google-dataproc-startup[803]: <13>Oct 28 05:34:41 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 64.'
<13>Oct 28 05:34:41 google-dataproc-startup[803]: <13>Oct 28 05:34:41 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 64.'
<13>Oct 28 05:34:41 google-dataproc-startup[803]: <13>Oct 28 05:34:41 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 64.
<13>Oct 28 05:34:41 google-dataproc-startup[803]: <13>Oct 28 05:34:41 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:42 google-dataproc-startup[803]: <13>Oct 28 05:34:42 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:42 google-dataproc-startup[803]: <13>Oct 28 05:34:42 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:42 google-dataproc-startup[803]: <13>Oct 28 05:34:42 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:42 google-dataproc-startup[803]: <13>Oct 28 05:34:42 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 65.'
<13>Oct 28 05:34:42 google-dataproc-startup[803]: <13>Oct 28 05:34:42 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 65.'
<13>Oct 28 05:34:42 google-dataproc-startup[803]: <13>Oct 28 05:34:42 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 65.
<13>Oct 28 05:34:42 google-dataproc-startup[803]: <13>Oct 28 05:34:42 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:43 google-dataproc-startup[803]: <13>Oct 28 05:34:43 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:43 google-dataproc-startup[803]: <13>Oct 28 05:34:43 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:43 google-dataproc-startup[803]: <13>Oct 28 05:34:43 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:43 google-dataproc-startup[803]: <13>Oct 28 05:34:43 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 66.'
<13>Oct 28 05:34:43 google-dataproc-startup[803]: <13>Oct 28 05:34:43 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 66.'
<13>Oct 28 05:34:43 google-dataproc-startup[803]: <13>Oct 28 05:34:43 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 66.
<13>Oct 28 05:34:43 google-dataproc-startup[803]: <13>Oct 28 05:34:43 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:44 google-dataproc-startup[803]: <13>Oct 28 05:34:44 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:44 google-dataproc-startup[803]: <13>Oct 28 05:34:44 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:44 google-dataproc-startup[803]: <13>Oct 28 05:34:44 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:44 google-dataproc-startup[803]: <13>Oct 28 05:34:44 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 67.'
<13>Oct 28 05:34:44 google-dataproc-startup[803]: <13>Oct 28 05:34:44 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 67.'
<13>Oct 28 05:34:44 google-dataproc-startup[803]: <13>Oct 28 05:34:44 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 67.
<13>Oct 28 05:34:44 google-dataproc-startup[803]: <13>Oct 28 05:34:44 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:45 google-dataproc-startup[803]: <13>Oct 28 05:34:45 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:45 google-dataproc-startup[803]: <13>Oct 28 05:34:45 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:45 google-dataproc-startup[803]: <13>Oct 28 05:34:45 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:45 google-dataproc-startup[803]: <13>Oct 28 05:34:45 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 68.'
<13>Oct 28 05:34:45 google-dataproc-startup[803]: <13>Oct 28 05:34:45 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 68.'
<13>Oct 28 05:34:45 google-dataproc-startup[803]: <13>Oct 28 05:34:45 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 68.
<13>Oct 28 05:34:45 google-dataproc-startup[803]: <13>Oct 28 05:34:45 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:46 google-dataproc-startup[803]: <13>Oct 28 05:34:46 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:46 google-dataproc-startup[803]: <13>Oct 28 05:34:46 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:46 google-dataproc-startup[803]: <13>Oct 28 05:34:46 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:46 google-dataproc-startup[803]: <13>Oct 28 05:34:46 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 69.'
<13>Oct 28 05:34:46 google-dataproc-startup[803]: <13>Oct 28 05:34:46 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 69.'
<13>Oct 28 05:34:46 google-dataproc-startup[803]: <13>Oct 28 05:34:46 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 69.
<13>Oct 28 05:34:46 google-dataproc-startup[803]: <13>Oct 28 05:34:46 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + update_succeeded=1
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + break
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + ((  1  ))
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + for SERVICE in "${DATAPROC_START_AFTER_HDFS_SERVICES[@]}"
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + run_with_retries systemctl start spark-history-server
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + local retry_backoff
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + cmd=("$@")
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + local -a cmd
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + loginfo 'About to run '\''systemctl start spark-history-server'\'' with retries...'
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + echo 'About to run '\''systemctl start spark-history-server'\'' with retries...'
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: About to run 'systemctl start spark-history-server' with retries...
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + local update_succeeded=0
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + (( i = 0 ))
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + (( i < 12 ))
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hadoop-hdfs-namenode[1463]: + systemctl start spark-history-server
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 70.'
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 70.'
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 70.
<13>Oct 28 05:34:47 google-dataproc-startup[803]: <13>Oct 28 05:34:47 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:48 google-dataproc-startup[803]: <13>Oct 28 05:34:48 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:48 google-dataproc-startup[803]: <13>Oct 28 05:34:48 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:48 google-dataproc-startup[803]: <13>Oct 28 05:34:48 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 05:34:48 google-dataproc-startup[803]: <13>Oct 28 05:34:48 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 71.'
<13>Oct 28 05:34:48 google-dataproc-startup[803]: <13>Oct 28 05:34:48 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 71.'
<13>Oct 28 05:34:48 google-dataproc-startup[803]: <13>Oct 28 05:34:48 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 failed. Retry attempt: 71.
<13>Oct 28 05:34:48 google-dataproc-startup[803]: <13>Oct 28 05:34:48 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 9083
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: Connection to cluster-4def-m 9083 port [tcp/*] succeeded!
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + update_succeeded=1
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 9083 succeeded.'
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 9083 succeeded.'
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 9083 succeeded.
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + break
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + ((  1  ))
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + loginfo 'Service up on host=cluster-4def-m port=9083.'
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + echo 'Service up on host=cluster-4def-m port=9083.'
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: Service up on host=cluster-4def-m port=9083.
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + run_with_retries systemctl start hive-server2
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + local retry_backoff
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + cmd=("$@")
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + local -a cmd
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + loginfo 'About to run '\''systemctl start hive-server2'\'' with retries...'
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + echo 'About to run '\''systemctl start hive-server2'\'' with retries...'
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: About to run 'systemctl start hive-server2' with retries...
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + local update_succeeded=0
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + (( i = 0 ))
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + (( i < 12 ))
<13>Oct 28 05:34:49 google-dataproc-startup[803]: <13>Oct 28 05:34:49 setup-hive-metastore[1465]: + systemctl start hive-server2
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: + update_succeeded=1
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: + break
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: + ((  1  ))
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: ++ get_dataproc_property am.primary_only
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: ++ local property_name=am.primary_only
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: +++ local property_name=am.primary_only
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: ++++ tail -n 1
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: ++++ cut -d = -f 2-
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: +++ local property_value=false
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: +++ echo false
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: ++ local property_value=false
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: ++ echo false
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 05:34:50 google-dataproc-startup[803]: <13>Oct 28 05:34:50 setup-hadoop-hdfs-namenode[1463]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 05:34:53 google-dataproc-startup[803]: <13>Oct 28 05:34:53 setup-hive-metastore[1465]: + update_succeeded=1
<13>Oct 28 05:34:53 google-dataproc-startup[803]: <13>Oct 28 05:34:53 setup-hive-metastore[1465]: + break
<13>Oct 28 05:34:53 google-dataproc-startup[803]: <13>Oct 28 05:34:53 setup-hive-metastore[1465]: + ((  1  ))
<13>Oct 28 05:34:53 google-dataproc-startup[803]: <13>Oct 28 05:34:53 setup-hive-metastore[1465]: ++ get_xml_property_or_default /etc/hive/conf/hive-site.xml hive.server2.thrift.port 10000
<13>Oct 28 05:34:53 google-dataproc-startup[803]: <13>Oct 28 05:34:53 setup-hive-metastore[1465]: ++ file=/etc/hive/conf/hive-site.xml
<13>Oct 28 05:34:53 google-dataproc-startup[803]: <13>Oct 28 05:34:53 setup-hive-metastore[1465]: ++ property=hive.server2.thrift.port
<13>Oct 28 05:34:53 google-dataproc-startup[803]: <13>Oct 28 05:34:53 setup-hive-metastore[1465]: ++ default_value=10000
<13>Oct 28 05:34:53 google-dataproc-startup[803]: <13>Oct 28 05:34:53 setup-hive-metastore[1465]: +++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.server2.thrift.port
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: ++ val=None
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: ++ [[ None = \N\o\n\e ]]
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: ++ val=10000
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: ++ echo 10000
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + thrift_port=10000
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + wait_for_port cluster-4def-m 10000
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + local -r host=cluster-4def-m
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + local -r port=10000
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + loginfo 'Waiting for service to come up on host=cluster-4def-m port=10000.'
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + echo 'Waiting for service to come up on host=cluster-4def-m port=10000.'
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: Waiting for service to come up on host=cluster-4def-m port=10000.
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + retry_with_constant_backoff nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + local max_retry=300
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + cmd=("$@")
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + local -a cmd
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + local update_succeeded=0
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: ++ seq 1 300
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 1.'
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 1.'
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 1.
<13>Oct 28 05:34:54 google-dataproc-startup[803]: <13>Oct 28 05:34:54 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:55 google-dataproc-startup[803]: <13>Oct 28 05:34:55 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:55 google-dataproc-startup[803]: <13>Oct 28 05:34:55 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:34:55 google-dataproc-startup[803]: <13>Oct 28 05:34:55 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:34:55 google-dataproc-startup[803]: <13>Oct 28 05:34:55 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 2.'
<13>Oct 28 05:34:55 google-dataproc-startup[803]: <13>Oct 28 05:34:55 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 2.'
<13>Oct 28 05:34:55 google-dataproc-startup[803]: <13>Oct 28 05:34:55 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 2.
<13>Oct 28 05:34:55 google-dataproc-startup[803]: <13>Oct 28 05:34:55 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:56 google-dataproc-startup[803]: <13>Oct 28 05:34:56 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:56 google-dataproc-startup[803]: <13>Oct 28 05:34:56 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:34:56 google-dataproc-startup[803]: <13>Oct 28 05:34:56 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:34:56 google-dataproc-startup[803]: <13>Oct 28 05:34:56 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 3.'
<13>Oct 28 05:34:56 google-dataproc-startup[803]: <13>Oct 28 05:34:56 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 3.'
<13>Oct 28 05:34:56 google-dataproc-startup[803]: <13>Oct 28 05:34:56 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 3.
<13>Oct 28 05:34:56 google-dataproc-startup[803]: <13>Oct 28 05:34:56 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:57 google-dataproc-startup[803]: <13>Oct 28 05:34:57 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:57 google-dataproc-startup[803]: <13>Oct 28 05:34:57 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:34:57 google-dataproc-startup[803]: <13>Oct 28 05:34:57 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:34:57 google-dataproc-startup[803]: <13>Oct 28 05:34:57 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 4.'
<13>Oct 28 05:34:57 google-dataproc-startup[803]: <13>Oct 28 05:34:57 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 4.'
<13>Oct 28 05:34:57 google-dataproc-startup[803]: <13>Oct 28 05:34:57 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 4.
<13>Oct 28 05:34:57 google-dataproc-startup[803]: <13>Oct 28 05:34:57 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:58 google-dataproc-startup[803]: <13>Oct 28 05:34:58 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:58 google-dataproc-startup[803]: <13>Oct 28 05:34:58 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:34:58 google-dataproc-startup[803]: <13>Oct 28 05:34:58 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:34:58 google-dataproc-startup[803]: <13>Oct 28 05:34:58 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 5.'
<13>Oct 28 05:34:58 google-dataproc-startup[803]: <13>Oct 28 05:34:58 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 5.'
<13>Oct 28 05:34:58 google-dataproc-startup[803]: <13>Oct 28 05:34:58 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 5.
<13>Oct 28 05:34:58 google-dataproc-startup[803]: <13>Oct 28 05:34:58 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:34:59 google-dataproc-startup[803]: <13>Oct 28 05:34:59 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:34:59 google-dataproc-startup[803]: <13>Oct 28 05:34:59 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:34:59 google-dataproc-startup[803]: <13>Oct 28 05:34:59 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:34:59 google-dataproc-startup[803]: <13>Oct 28 05:34:59 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 6.'
<13>Oct 28 05:34:59 google-dataproc-startup[803]: <13>Oct 28 05:34:59 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 6.'
<13>Oct 28 05:34:59 google-dataproc-startup[803]: <13>Oct 28 05:34:59 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 6.
<13>Oct 28 05:34:59 google-dataproc-startup[803]: <13>Oct 28 05:34:59 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:00 google-dataproc-startup[803]: <13>Oct 28 05:35:00 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:00 google-dataproc-startup[803]: <13>Oct 28 05:35:00 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:00 google-dataproc-startup[803]: <13>Oct 28 05:35:00 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:00 google-dataproc-startup[803]: <13>Oct 28 05:35:00 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 7.'
<13>Oct 28 05:35:00 google-dataproc-startup[803]: <13>Oct 28 05:35:00 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 7.'
<13>Oct 28 05:35:00 google-dataproc-startup[803]: <13>Oct 28 05:35:00 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 7.
<13>Oct 28 05:35:00 google-dataproc-startup[803]: <13>Oct 28 05:35:00 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:01 google-dataproc-startup[803]: <13>Oct 28 05:35:01 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:01 google-dataproc-startup[803]: <13>Oct 28 05:35:01 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:01 google-dataproc-startup[803]: <13>Oct 28 05:35:01 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:01 google-dataproc-startup[803]: <13>Oct 28 05:35:01 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 8.'
<13>Oct 28 05:35:01 google-dataproc-startup[803]: <13>Oct 28 05:35:01 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 8.'
<13>Oct 28 05:35:01 google-dataproc-startup[803]: <13>Oct 28 05:35:01 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 8.
<13>Oct 28 05:35:01 google-dataproc-startup[803]: <13>Oct 28 05:35:01 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:02 google-dataproc-startup[803]: <13>Oct 28 05:35:02 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:02 google-dataproc-startup[803]: <13>Oct 28 05:35:02 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:02 google-dataproc-startup[803]: <13>Oct 28 05:35:02 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:02 google-dataproc-startup[803]: <13>Oct 28 05:35:02 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 9.'
<13>Oct 28 05:35:02 google-dataproc-startup[803]: <13>Oct 28 05:35:02 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 9.'
<13>Oct 28 05:35:02 google-dataproc-startup[803]: <13>Oct 28 05:35:02 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 9.
<13>Oct 28 05:35:02 google-dataproc-startup[803]: <13>Oct 28 05:35:02 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:03 google-dataproc-startup[803]: <13>Oct 28 05:35:03 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:03 google-dataproc-startup[803]: <13>Oct 28 05:35:03 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:03 google-dataproc-startup[803]: <13>Oct 28 05:35:03 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:03 google-dataproc-startup[803]: <13>Oct 28 05:35:03 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 10.'
<13>Oct 28 05:35:03 google-dataproc-startup[803]: <13>Oct 28 05:35:03 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 10.'
<13>Oct 28 05:35:03 google-dataproc-startup[803]: <13>Oct 28 05:35:03 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 10.
<13>Oct 28 05:35:03 google-dataproc-startup[803]: <13>Oct 28 05:35:03 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:04 google-dataproc-startup[803]: <13>Oct 28 05:35:04 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:04 google-dataproc-startup[803]: <13>Oct 28 05:35:04 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:04 google-dataproc-startup[803]: <13>Oct 28 05:35:04 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:04 google-dataproc-startup[803]: <13>Oct 28 05:35:04 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 11.'
<13>Oct 28 05:35:04 google-dataproc-startup[803]: <13>Oct 28 05:35:04 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 11.'
<13>Oct 28 05:35:04 google-dataproc-startup[803]: <13>Oct 28 05:35:04 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 11.
<13>Oct 28 05:35:04 google-dataproc-startup[803]: <13>Oct 28 05:35:04 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:05 google-dataproc-startup[803]: <13>Oct 28 05:35:05 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:05 google-dataproc-startup[803]: <13>Oct 28 05:35:05 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:05 google-dataproc-startup[803]: <13>Oct 28 05:35:05 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:05 google-dataproc-startup[803]: <13>Oct 28 05:35:05 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 12.'
<13>Oct 28 05:35:05 google-dataproc-startup[803]: <13>Oct 28 05:35:05 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 12.'
<13>Oct 28 05:35:05 google-dataproc-startup[803]: <13>Oct 28 05:35:05 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 12.
<13>Oct 28 05:35:05 google-dataproc-startup[803]: <13>Oct 28 05:35:05 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:06 google-dataproc-startup[803]: <13>Oct 28 05:35:06 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:06 google-dataproc-startup[803]: <13>Oct 28 05:35:06 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:06 google-dataproc-startup[803]: <13>Oct 28 05:35:06 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:06 google-dataproc-startup[803]: <13>Oct 28 05:35:06 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 13.'
<13>Oct 28 05:35:06 google-dataproc-startup[803]: <13>Oct 28 05:35:06 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 13.'
<13>Oct 28 05:35:06 google-dataproc-startup[803]: <13>Oct 28 05:35:06 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 13.
<13>Oct 28 05:35:06 google-dataproc-startup[803]: <13>Oct 28 05:35:06 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:07 google-dataproc-startup[803]: <13>Oct 28 05:35:07 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:07 google-dataproc-startup[803]: <13>Oct 28 05:35:07 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:07 google-dataproc-startup[803]: <13>Oct 28 05:35:07 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:07 google-dataproc-startup[803]: <13>Oct 28 05:35:07 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 14.'
<13>Oct 28 05:35:07 google-dataproc-startup[803]: <13>Oct 28 05:35:07 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 14.'
<13>Oct 28 05:35:07 google-dataproc-startup[803]: <13>Oct 28 05:35:07 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 14.
<13>Oct 28 05:35:07 google-dataproc-startup[803]: <13>Oct 28 05:35:07 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:08 google-dataproc-startup[803]: <13>Oct 28 05:35:08 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:08 google-dataproc-startup[803]: <13>Oct 28 05:35:08 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:08 google-dataproc-startup[803]: <13>Oct 28 05:35:08 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:08 google-dataproc-startup[803]: <13>Oct 28 05:35:08 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 15.'
<13>Oct 28 05:35:08 google-dataproc-startup[803]: <13>Oct 28 05:35:08 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 15.'
<13>Oct 28 05:35:08 google-dataproc-startup[803]: <13>Oct 28 05:35:08 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 15.
<13>Oct 28 05:35:08 google-dataproc-startup[803]: <13>Oct 28 05:35:08 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:09 google-dataproc-startup[803]: <13>Oct 28 05:35:09 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:09 google-dataproc-startup[803]: <13>Oct 28 05:35:09 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:09 google-dataproc-startup[803]: <13>Oct 28 05:35:09 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:09 google-dataproc-startup[803]: <13>Oct 28 05:35:09 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 16.'
<13>Oct 28 05:35:09 google-dataproc-startup[803]: <13>Oct 28 05:35:09 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 16.'
<13>Oct 28 05:35:09 google-dataproc-startup[803]: <13>Oct 28 05:35:09 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 16.
<13>Oct 28 05:35:09 google-dataproc-startup[803]: <13>Oct 28 05:35:09 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:10 google-dataproc-startup[803]: <13>Oct 28 05:35:10 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:10 google-dataproc-startup[803]: <13>Oct 28 05:35:10 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:10 google-dataproc-startup[803]: <13>Oct 28 05:35:10 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:10 google-dataproc-startup[803]: <13>Oct 28 05:35:10 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 17.'
<13>Oct 28 05:35:10 google-dataproc-startup[803]: <13>Oct 28 05:35:10 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 17.'
<13>Oct 28 05:35:10 google-dataproc-startup[803]: <13>Oct 28 05:35:10 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 17.
<13>Oct 28 05:35:10 google-dataproc-startup[803]: <13>Oct 28 05:35:10 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:11 google-dataproc-startup[803]: <13>Oct 28 05:35:11 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:11 google-dataproc-startup[803]: <13>Oct 28 05:35:11 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:11 google-dataproc-startup[803]: <13>Oct 28 05:35:11 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:11 google-dataproc-startup[803]: <13>Oct 28 05:35:11 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 18.'
<13>Oct 28 05:35:11 google-dataproc-startup[803]: <13>Oct 28 05:35:11 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 18.'
<13>Oct 28 05:35:11 google-dataproc-startup[803]: <13>Oct 28 05:35:11 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 18.
<13>Oct 28 05:35:11 google-dataproc-startup[803]: <13>Oct 28 05:35:11 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:12 google-dataproc-startup[803]: <13>Oct 28 05:35:12 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:12 google-dataproc-startup[803]: <13>Oct 28 05:35:12 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:12 google-dataproc-startup[803]: <13>Oct 28 05:35:12 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:12 google-dataproc-startup[803]: <13>Oct 28 05:35:12 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 19.'
<13>Oct 28 05:35:12 google-dataproc-startup[803]: <13>Oct 28 05:35:12 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 19.'
<13>Oct 28 05:35:12 google-dataproc-startup[803]: <13>Oct 28 05:35:12 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 19.
<13>Oct 28 05:35:12 google-dataproc-startup[803]: <13>Oct 28 05:35:12 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:13 google-dataproc-startup[803]: <13>Oct 28 05:35:13 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:13 google-dataproc-startup[803]: <13>Oct 28 05:35:13 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:13 google-dataproc-startup[803]: <13>Oct 28 05:35:13 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:13 google-dataproc-startup[803]: <13>Oct 28 05:35:13 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 20.'
<13>Oct 28 05:35:13 google-dataproc-startup[803]: <13>Oct 28 05:35:13 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 20.'
<13>Oct 28 05:35:13 google-dataproc-startup[803]: <13>Oct 28 05:35:13 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 20.
<13>Oct 28 05:35:13 google-dataproc-startup[803]: <13>Oct 28 05:35:13 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:14 google-dataproc-startup[803]: <13>Oct 28 05:35:14 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:14 google-dataproc-startup[803]: <13>Oct 28 05:35:14 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:14 google-dataproc-startup[803]: <13>Oct 28 05:35:14 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:14 google-dataproc-startup[803]: <13>Oct 28 05:35:14 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 21.'
<13>Oct 28 05:35:14 google-dataproc-startup[803]: <13>Oct 28 05:35:14 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 21.'
<13>Oct 28 05:35:14 google-dataproc-startup[803]: <13>Oct 28 05:35:14 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 21.
<13>Oct 28 05:35:14 google-dataproc-startup[803]: <13>Oct 28 05:35:14 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:15 google-dataproc-startup[803]: <13>Oct 28 05:35:15 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:15 google-dataproc-startup[803]: <13>Oct 28 05:35:15 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:15 google-dataproc-startup[803]: <13>Oct 28 05:35:15 setup-hive-metastore[1465]: nc: connect to cluster-4def-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 05:35:15 google-dataproc-startup[803]: <13>Oct 28 05:35:15 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 22.'
<13>Oct 28 05:35:15 google-dataproc-startup[803]: <13>Oct 28 05:35:15 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 22.'
<13>Oct 28 05:35:15 google-dataproc-startup[803]: <13>Oct 28 05:35:15 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 failed. Retry attempt: 22.
<13>Oct 28 05:35:15 google-dataproc-startup[803]: <13>Oct 28 05:35:15 setup-hive-metastore[1465]: + sleep 1
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + nc -v -z -w 0 cluster-4def-m 10000
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: Connection to cluster-4def-m 10000 port [tcp/webmin] succeeded!
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + update_succeeded=1
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + loginfo 'nc -v -z -w 0 cluster-4def-m 10000 succeeded.'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + echo 'nc -v -z -w 0 cluster-4def-m 10000 succeeded.'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: nc -v -z -w 0 cluster-4def-m 10000 succeeded.
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + break
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + ((  1  ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + loginfo 'Service up on host=cluster-4def-m port=10000.'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + echo 'Service up on host=cluster-4def-m port=10000.'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: Service up on host=cluster-4def-m port=10000.
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: ++ get_dataproc_property am.primary_only
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: ++ local property_name=am.primary_only
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: +++ local property_name=am.primary_only
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: ++++ cut -d = -f 2-
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: ++++ tail -n 1
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: +++ local property_value=false
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: +++ echo false
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + pid=1464
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + cmd='setup_service hadoop-yarn-resourcemanager'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1464 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + echo 'Waiting on pid=1464 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: Waiting on pid=1464 cmd=[setup_service hadoop-yarn-resourcemanager]
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + wait 1464
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + pid=1463
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + cmd='setup_service hadoop-hdfs-namenode'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1463 cmd=[setup_service hadoop-hdfs-namenode]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + echo 'Waiting on pid=1463 cmd=[setup_service hadoop-hdfs-namenode]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: Waiting on pid=1463 cmd=[setup_service hadoop-hdfs-namenode]
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + wait 1463
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + pid=1405
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + cmd='uninstall_component proxy-agent'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1405 cmd=[uninstall_component proxy-agent]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + echo 'Waiting on pid=1405 cmd=[uninstall_component proxy-agent]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: Waiting on pid=1405 cmd=[uninstall_component proxy-agent]
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + wait 1405
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + pid=1404
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + cmd='uninstall_component presto'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1404 cmd=[uninstall_component presto]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + echo 'Waiting on pid=1404 cmd=[uninstall_component presto]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: Waiting on pid=1404 cmd=[uninstall_component presto]
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + wait 1404
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + pid=1403
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + cmd='uninstall_component kerberos'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1403 cmd=[uninstall_component kerberos]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + echo 'Waiting on pid=1403 cmd=[uninstall_component kerberos]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: Waiting on pid=1403 cmd=[uninstall_component kerberos]
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + wait 1403
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + pid=1402
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + cmd='uninstall_component jupyter'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1402 cmd=[uninstall_component jupyter]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + echo 'Waiting on pid=1402 cmd=[uninstall_component jupyter]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: Waiting on pid=1402 cmd=[uninstall_component jupyter]
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + wait 1402
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + pid=1401
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + cmd='uninstall_component anaconda'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1401 cmd=[uninstall_component anaconda]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + echo 'Waiting on pid=1401 cmd=[uninstall_component anaconda]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: Waiting on pid=1401 cmd=[uninstall_component anaconda]
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + wait 1401
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + pid=1400
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + cmd='bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + loginfo 'Waiting on pid=1400 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + echo 'Waiting on pid=1400 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: Waiting on pid=1400 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + status=0
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + wait 1400
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( status != 0 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( ++i  ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + (( i < 16 ))
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + BACKGROUND_PROCESSES=()
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + BACKGROUND_COMMANDS=()
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + systemctl daemon-reload
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: ++ local property_value=false
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: ++ echo false
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 05:35:16 google-dataproc-startup[803]: <13>Oct 28 05:35:16 setup-hive-metastore[1465]: + [[ hive-metastore == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + loginfo 'All done'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: + echo 'All done'
<13>Oct 28 05:35:16 google-dataproc-startup[803]: All done
