+ run_with_logger --tag google-dataproc-startup
+ local tag=
+ local pid=843
+ [[ --tag == \-\-\t\a\g ]]
+ tag=google-dataproc-startup
+ shift 2
+ exec
++ logger -s -t 'google-dataproc-startup[843]'
<13>Oct 28 02:17:52 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=()
<13>Oct 28 02:17:52 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=()
<13>Oct 28 02:17:52 google-dataproc-startup[843]: + cd /tmp
<13>Oct 28 02:17:52 google-dataproc-startup[843]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ INSTALL_GCS_CONNECTOR=1
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ ENABLE_HDFS=1
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ ENABLE_NFS_GCS_FILE_CACHE=1
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ GCS_ADMIN=gcsadmin
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ GCS_CACHE_CLEANER_LOG_DIRECTORY=/var/log/hadoop-hdfs
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,DRFA
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,RFA
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ NUM_WORKERS=10
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ WORKERS=()
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ CORES_PER_MAP_TASK=1.0
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ CORES_PER_APP_MASTER=2.0
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ HDFS_DATA_DIRS_PERM=700
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ COMMON_JAVA_OPTS=("-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance")
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ DATAPROC_VERSION_KEY=1_3
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ declare -A DATAPROC_TO_GCS_CONNECTOR_VERSION
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ DATAPROC_TO_GCS_CONNECTOR_VERSION=(["1_0"]="1.6.10-hadoop2" ["1_1"]="1.6.10-hadoop2" ["1_2"]="1.6.10-hadoop2" ["1_3"]="hadoop2-1.9.17" ["1_4"]="hadoop2-1.9.17" ["1_5"]="hadoop2-1.9.17" ["2_0"]="hadoop3-1.9.17")
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ GCS_CONNECTOR_VERSION=hadoop2-1.9.17
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ GCS_CONNECTOR_JAR=https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-1.9.17.jar
<13>Oct 28 02:17:52 google-dataproc-startup[843]: +++ /usr/share/google/get_metadata_value attributes/dataproc_gcs_connector_url
<13>Oct 28 02:17:52 google-dataproc-startup[843]: +++ true
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ GCS_CONNECTOR_JAR_OVERRIDE=
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ [[ ! -z '' ]]
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ declare -A DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ DATAPROC_TO_BQ_CONNECTOR_VERSION=(["1_0"]="0.10.11-hadoop2" ["1_1"]="0.10.11-hadoop2" ["1_2"]="0.10.11-hadoop2")
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ BIGQUERY_CONNECTOR_VERSION=
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ [[ -n '' ]]
<13>Oct 28 02:17:52 google-dataproc-startup[843]: +++ /usr/share/google/get_metadata_value attributes/dataproc_bq_connector_url
<13>Oct 28 02:17:52 google-dataproc-startup[843]: +++ true
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ BIGQUERY_CONNECTOR_JAR_OVERRIDE=
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ [[ ! -z '' ]]
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ unset DATAPROC_VERSION_KEY GCS_CONNECTOR_VERSION BIGQUERY_CONNECTOR_VERSION DATAPROC_TO_GCS_CONNECTOR_VERSION DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Oct 28 02:17:52 google-dataproc-startup[843]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Oct 28 02:17:52 google-dataproc-startup[843]: +++ dpkg -s hive
<13>Oct 28 02:17:52 google-dataproc-startup[843]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ HIVE_VERSION=2.3.5
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ dpkg -s spark-core
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ SPARK_VERSION=2.3.3
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ readonly APT_SENTINEL=apt.lastupdate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ APT_SENTINEL=apt.lastupdate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + source /usr/local/share/google/dataproc/bdutil/cluster_properties.sh
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + source /usr/local/share/google/dataproc/bdutil/components/startup-script-components.sh
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ readonly COMPONENTS_DIR=/var/lib/google/dataproc/components
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ COMPONENTS_DIR=/var/lib/google/dataproc/components
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + trap logstacktrace ERR
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + loginfo 'Starting Dataproc startup script'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + echo 'Starting Dataproc startup script'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: Starting Dataproc startup script
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ hostname -s
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + MY_HOSTNAME=cluster-89fa-m
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ hostname -f
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ dnsdomainname
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + DOMAIN=us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ echo cluster-89fa-m
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ sed -r 's/-([mw](-[0-9]*)?)$//'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + PREFIX=cluster-89fa
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + merge_java_properties /tmp/cluster/properties/dataproc.properties /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + local dest=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/dataproc.properties ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + cat /tmp/cluster/properties/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: Merged /tmp/cluster/properties/dataproc.properties.
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ local property_name=dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_name=dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_value=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ echo ''
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ local property_value=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ echo ''
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + OPTIONAL_COMPONENTS_VALUE=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + is_version_at_least 1.3 1.4
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + local ver1=1.3.0.0.0.0
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + local ver2=1.4
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + local log=/tmp/tmp.lk3dCLoq4m
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + err_code=1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.lk3dCLoq4m
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + rm -f /tmp/tmp.lk3dCLoq4m
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + DATAPROC_MASTER=cluster-89fa-m
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ /usr/share/google/get_metadata_value attributes/dataproc-master-additional
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + DATAPROC_MASTER_ADDITIONAL=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + MASTER_COUNT=1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + ((  1 > 1  ))
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + is_component_selected kafka-server
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + local component=kafka-server
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ local property_name=dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_name=dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_value=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ echo ''
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ local property_value=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ echo ''
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + local activated_components=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + [[ '' == *kafka-server* ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + COMPONENTS_TO_ACTIVATE=(${OPTIONAL_COMPONENTS_VALUE})
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + KERBEROS_ENABLED=false
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + is_component_selected kerberos
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + local component=kerberos
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ local property_name=dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_name=dataproc.components.activate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_value=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ echo ''
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ local property_value=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ echo ''
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + local activated_components=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + [[ '' == *kerberos* ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + [[ false == \t\r\u\e ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ /usr/share/google/get_metadata_value ../project/project-id
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + PROJECT=lustrous-drake-255300
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ /usr/share/google/get_metadata_value attributes/dataproc-bucket
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + CONFIGBUCKET=dataproc-ed3c3d29-fb10-47bb-aca7-dcc358c68973-us-central1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ /usr/share/google/get_metadata_value attributes/dataproc-role
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + ROLE=Master
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-name
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + CLUSTER_NAME=cluster-89fa
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-uuid
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + CLUSTER_UUID=33d4c4cf-b26f-46c9-a05c-50ab90fb323c
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ /usr/share/google/get_metadata_value attributes/dataproc-worker-count
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + WORKER_COUNT=2
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + HBASE_CONF_DIR=/etc/hbase/conf
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + PIG_CONF_DIR=/etc/pig/conf
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + SPARK_CONF_DIR=/etc/spark/conf
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + ZOOKEEPER_CONF_DIR=/etc/zookeeper/conf
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + HADOOP_2_PORTS=(50010 50020 50070 50090)
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + ((  1 > 1  ))
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + HDFS_ROOT_URI=hdfs://cluster-89fa-m
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + hostname=cluster-89fa-m
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + [[ false == \t\r\u\e ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + for i in "${!MASTER_HOSTNAMES[@]}"
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + [[ cluster-89fa-m == \c\l\u\s\t\e\r\-\8\9\f\a\-\m ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + MASTER_INDEX=0
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + break
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + ((  2 == 0  ))
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + ((  1 > 1  ))
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + PACKAGES_TO_UNINSTALL=(${DATAPROC_MASTER_HA_SERVICES} ${DATAPROC_WORKER_SERVICES})
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + SERVICES=(${DATAPROC_MASTER_SERVICES} ${DATAPROC_MASTER_EXCLUSIVE_SERVICES} ${DATAPROC_MASTER_STANDALONE_SERVICES})
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + loginfo 'Generating helper scripts'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + echo 'Generating helper scripts'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: Generating helper scripts
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ cat /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ (( i=0 ))
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ (( i<1 ))
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ echo MASTER_HOSTNAME_0=cluster-89fa-m
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ (( i++  ))
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ (( i<1 ))
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ cat /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + sudo -u hdfs hdfs namenode -genclusterid
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ cat /usr/local/share/google/dataproc/bdutil/setup_master_nfs.sh /usr/local/share/google/dataproc/bdutil/setup_client_nfs.sh
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ cat /usr/local/share/google/dataproc/bdutil/configure_hadoop.sh /usr/local/share/google/dataproc/bdutil/configure_mysql.sh /usr/local/share/google/dataproc/bdutil/configure_hive.sh /usr/local/share/google/dataproc/bdutil/configure_hdfs.sh /usr/local/share/google/dataproc/bdutil/configure_connectors.sh /usr/local/share/google/dataproc/bdutil/configure_spark.sh /usr/local/share/google/dataproc/bdutil/configure_tez.sh /usr/local/share/google/dataproc/bdutil/configure_zookeeper.sh
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + cp -r /usr/local/share/google/dataproc/bdutil/conf/bq-mapred-template.xml /usr/local/share/google/dataproc/bdutil/conf/capacity-scheduler-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-template.xml /usr/local/share/google/dataproc/bdutil/conf/distcp-template.xml /usr/local/share/google/dataproc/bdutil/conf/gcs-core-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-template.xml /usr/local/share/google/dataproc/bdutil/conf/hive-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hive-template.xml /usr/local/share/google/dataproc/bdutil/conf/knox /usr/local/share/google/dataproc/bdutil/conf/mapred-template.xml /usr/local/share/google/dataproc/b
<13>Oct 28 02:17:53 google-dataproc-startup[843]: dutil/conf/yarn-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-template.xml /tmp
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + cp /usr/local/share/google/dataproc/bdutil/configure_mrv2_mem.py /tmp
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + chmod +x configure_mrv2_mem.py
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + loginfo 'Running helper scripts'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + echo 'Running helper scripts'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: Running helper scripts
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ get_dataproc_property dataproc.localssd.mount.enable
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ local property_name=dataproc.localssd.mount.enable
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.localssd.mount.enable
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_name=dataproc.localssd.mount.enable
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ grep '^dataproc.localssd.mount.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ local property_value=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ echo ''
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ local property_value=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ echo ''
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + MOUNT_DISKS_ENABLED=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + [[ '' == \f\a\l\s\e ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + DATAPROC_MOUNT_SERVICE_FILE=/usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + chmod +x /usr/local/share/google/dataproc/bdutil/mount_disks.sh
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + chmod 644 /usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + systemctl enable google-dataproc-disk-mount
<13>Oct 28 02:17:53 google-dataproc-startup[843]: Created symlink /etc/systemd/system/multi-user.target.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 28 02:17:53 google-dataproc-startup[843]: Created symlink /etc/systemd/system/hadoop-hdfs-namenode.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 28 02:17:53 google-dataproc-startup[843]: Created symlink /etc/systemd/system/hadoop-hdfs-datanode.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 28 02:17:53 google-dataproc-startup[843]: Created symlink /etc/systemd/system/hadoop-yarn-resourcemanager.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 28 02:17:53 google-dataproc-startup[843]: Created symlink /etc/systemd/system/hadoop-yarn-nodemanager.service.wants/google-dataproc-disk-mount.service → /usr/lib/systemd/system/google-dataproc-disk-mount.service.
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + systemctl start google-dataproc-disk-mount
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + in_array nfs-kernel-server DATAPROC_MASTER_EXCLUSIVE_SERVICES
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + local value=nfs-kernel-server
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + local -n values=DATAPROC_MASTER_EXCLUSIVE_SERVICES
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + [[ !  hadoop-mapreduce-historyserver spark-history-server hive-webhcat-server jupyter knox proxy-agent zeppelin hadoop-yarn-timelineserver mariadb-server  =~  nfs-kernel-server  ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + bash configuration_script.sh
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ INSTALL_GCS_CONNECTOR=1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ ENABLE_HDFS=1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ ENABLE_NFS_GCS_FILE_CACHE=1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ GCS_ADMIN=gcsadmin
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ GCS_CACHE_CLEANER_LOG_DIRECTORY=/var/log/hadoop-hdfs
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,DRFA
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ GCS_CACHE_CLEANER_LOGGER=INFO,RFA
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ NUM_WORKERS=10
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ WORKERS=()
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ CORES_PER_MAP_TASK=1.0
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ CORES_PER_APP_MASTER=2.0
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ HDFS_DATA_DIRS_PERM=700
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ COMMON_JAVA_OPTS=("-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance")
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ DATAPROC_VERSION_KEY=1_3
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ declare -A DATAPROC_TO_GCS_CONNECTOR_VERSION
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ DATAPROC_TO_GCS_CONNECTOR_VERSION=(["1_0"]="1.6.10-hadoop2" ["1_1"]="1.6.10-hadoop2" ["1_2"]="1.6.10-hadoop2" ["1_3"]="hadoop2-1.9.17" ["1_4"]="hadoop2-1.9.17" ["1_5"]="hadoop2-1.9.17" ["2_0"]="hadoop3-1.9.17")
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ GCS_CONNECTOR_VERSION=hadoop2-1.9.17
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ GCS_CONNECTOR_JAR=https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-1.9.17.jar
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ /usr/share/google/get_metadata_value attributes/dataproc_gcs_connector_url
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ true
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ GCS_CONNECTOR_JAR_OVERRIDE=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ [[ ! -z '' ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ declare -A DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ DATAPROC_TO_BQ_CONNECTOR_VERSION=(["1_0"]="0.10.11-hadoop2" ["1_1"]="0.10.11-hadoop2" ["1_2"]="0.10.11-hadoop2")
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ BIGQUERY_CONNECTOR_VERSION=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ [[ -n '' ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ /usr/share/google/get_metadata_value attributes/dataproc_bq_connector_url
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ true
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ BIGQUERY_CONNECTOR_JAR_OVERRIDE=
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ [[ ! -z '' ]]
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ unset DATAPROC_VERSION_KEY GCS_CONNECTOR_VERSION BIGQUERY_CONNECTOR_VERSION DATAPROC_TO_GCS_CONNECTOR_VERSION DATAPROC_TO_BQ_CONNECTOR_VERSION
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ dpkg -s hive
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ HIVE_VERSION=2.3.5
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ sed -n 's/^Version: \([0-9\.]*\).*/\1/p'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: +++ dpkg -s spark-core
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ SPARK_VERSION=2.3.3
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ CLUSTER_NAME=cluster-89fa
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ CLUSTER_UUID=33d4c4cf-b26f-46c9-a05c-50ab90fb323c
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ CONFIGBUCKET=dataproc-ed3c3d29-fb10-47bb-aca7-dcc358c68973-us-central1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ HDFS_ROOT_URI=hdfs://cluster-89fa-m
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ MASTER_HOSTNAME_0=cluster-89fa-m
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ MASTER_HOSTNAMES=(cluster-89fa-m)
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ NUM_MASTERS=1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ NUM_WORKERS=2
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ PREFIX=cluster-89fa
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ PROJECT=lustrous-drake-255300
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ ROLE=Master
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ set +a
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ readonly APT_SENTINEL=apt.lastupdate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ APT_SENTINEL=apt.lastupdate
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + set -e
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + loginfo 'Running configure_hadoop.sh'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + echo 'Running configure_hadoop.sh'
<13>Oct 28 02:17:53 google-dataproc-startup[843]: Running configure_hadoop.sh
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + export HADOOP_TMP_DIR=/hadoop/tmp
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + HADOOP_TMP_DIR=/hadoop/tmp
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + mkdir -p /hadoop/tmp
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + export DEFAULT_NUM_MAPS=20
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + DEFAULT_NUM_MAPS=20
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + export DEFAULT_NUM_REDUCES=8
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + DEFAULT_NUM_REDUCES=8
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ grep -c processor /proc/cpuinfo
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + export NUM_CORES=2
<13>Oct 28 02:17:53 google-dataproc-startup[843]: + NUM_CORES=2
<13>Oct 28 02:17:53 google-dataproc-startup[843]: ++ python -c 'print int(2 //     1.0)'
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + export MAP_SLOTS=2
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + MAP_SLOTS=2
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ python -c 'print int(2 //     2.0)'
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + export REDUCE_SLOTS=1
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + REDUCE_SLOTS=1
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ awk '/^Mem:/{print $2}'
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ free -m
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + TOTAL_MEM=7483
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ python -c 'print int(7483 *     0.4)'
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + HADOOP_MR_MASTER_MEM_MB=2993
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + [[ -x configure_mrv2_mem.py ]]
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ mktemp /tmp/mrv2_XXX_tmp_env.sh
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + TEMP_ENV_FILE=/tmp/mrv2_Mj7_tmp_env.sh
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + ./configure_mrv2_mem.py --output_file /tmp/mrv2_Mj7_tmp_env.sh --total_memory 7483 --available_memory_ratio 0.8 --total_cores 2 --cores_per_map 1.0 --cores_per_reduce 2.0 --cores_per_app_master 2.0
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + source /tmp/mrv2_Mj7_tmp_env.sh
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export YARN_MIN_MEM_MB=512
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ YARN_MIN_MEM_MB=512
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export YARN_MAX_MEM_MB=5632
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ YARN_MAX_MEM_MB=5632
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export NODEMANAGER_MEM_MB=5632
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ NODEMANAGER_MEM_MB=5632
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export APP_MASTER_MEM_MB=5632
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ APP_MASTER_MEM_MB=5632
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export MAP_MEM_MB=2560
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ MAP_MEM_MB=2560
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ CORES_PER_MAP_ROUNDED=1
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export REDUCE_MEM_MB=5632
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ REDUCE_MEM_MB=5632
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export CORES_PER_REDUCE_ROUNDED=2
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ CORES_PER_REDUCE_ROUNDED=2
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ export REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ python -c 'print int(7483 / 4)'
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + HADOOP_CLIENT_MEM_MB=1870
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + is_version_at_least 1.3 1.4
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + local ver1=1.3.0.0.0.0
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + local ver2=1.4
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + local log=/tmp/tmp.0PMT4LrLwJ
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + err_code=1
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.0PMT4LrLwJ
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + rm -f /tmp/tmp.0PMT4LrLwJ
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + DATA_DIRS=($(get_data_dirs))
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ get_data_dirs
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ mount_points=($(find /mnt/[0-9]*/ -maxdepth 0 || true))
<13>Oct 28 02:17:54 google-dataproc-startup[843]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Oct 28 02:17:54 google-dataproc-startup[843]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Oct 28 02:17:54 google-dataproc-startup[843]: +++ true
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ local mount_points
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ ((  0  ))
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ echo /
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ return
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + mkdir -p /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + chgrp hadoop -L -R /hadoop /hadoop/tmp /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + chmod g+rwx -R /hadoop /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + chmod 777 -R /hadoop/tmp
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + export MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + export NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + YARN_ENV_FILE=/etc/hadoop/conf/yarn-env.sh
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + [[ -f /etc/hadoop/conf/yarn-env.sh ]]
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ get_dataproc_property simplified.scaling.enable
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ local property_name=simplified.scaling.enable
<13>Oct 28 02:17:54 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Oct 28 02:17:54 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:54 google-dataproc-startup[843]: +++ local property_name=simplified.scaling.enable
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:54 google-dataproc-startup[843]: +++ local property_value=
<13>Oct 28 02:17:54 google-dataproc-startup[843]: +++ echo ''
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ local property_value=
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ echo ''
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + [[ '' == \t\r\u\e ]]
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + touch /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + chown root:hadoop /etc/hadoop/conf/nodes_include /etc/hadoop/conf/nodes_exclude
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + [[ 1 -gt 1 ]]
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + CORE_TEMPLATE=core-template.xml
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + YARN_TEMPLATE=yarn-template.xml
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file core-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file mapred-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file yarn-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file capacity-scheduler-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file distcp-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + is_version_at_least 1.3 1.4
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + local ver1=1.3.0.0.0.0
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + local ver2=1.4
<13>Oct 28 02:17:54 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + local log=/tmp/tmp.6Ecmg08GsS
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + err_code=1
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.6Ecmg08GsS
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + rm -f /tmp/tmp.6Ecmg08GsS
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:54 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value false --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + ZK_QUORUM=cluster-89fa-m:2181,:2181,:2181
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + [[ 1 -gt 1 ]]
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + is_version_at_least 1.3 1.2
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local ver1=1.3.0.0.0.0
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local ver2=1.2
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local log=/tmp/tmp.AFFGcn1csp
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.2
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + err_code=0
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.AFFGcn1csp
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + rm -f /tmp/tmp.AFFGcn1csp
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + return 0
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.file.close.num-committed-allowed --value 1 --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + is_version_at_least 1.3 1.3
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local ver1=1.3.0.0.0.0
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local ver2=1.3
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local log=/tmp/tmp.UOJjkfq8Yr
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + err_code=0
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.UOJjkfq8Yr
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + rm -f /tmp/tmp.UOJjkfq8Yr
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + return 0
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/core-site.xml --name hadoop.http.filter.initializers --value org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.webapp.cross-origin.enabled --value true --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.http-cross-origin.enabled --value true --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.enabled --value true --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.hostname --value cluster-89fa-m --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.bind-host --value 0.0.0.0 --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.system-metrics-publisher.enabled --value true --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.generic-application-history.enabled --value true --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ get_dataproc_property am.primary_only
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ local property_name=am.primary_only
<13>Oct 28 02:17:55 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 02:17:55 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:55 google-dataproc-startup[843]: +++ local property_name=am.primary_only
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:55 google-dataproc-startup[843]: +++ local property_value=false
<13>Oct 28 02:17:55 google-dataproc-startup[843]: +++ echo false
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ local property_value=false
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ echo false
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ /usr/share/google/get_metadata_value attributes/dataproc-datanode-enabled
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + DATAPROC_DATANODE_ENABLED=true
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + [[ false == \t\r\u\e ]]
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + [[ '' == \t\r\u\e ]]
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + set -e -x
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + grep -lr bind-address /etc/mysql
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + xargs -n1 sed -i 's/^\(bind-address\)\s*=.*/\1 = 0.0.0.0/'
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + set -e -x
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + is_version_at_least 1.3 1.3
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local ver1=1.3.0.0.0.0
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local ver2=1.3
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local log=/tmp/tmp.pf5WJJTLHs
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + err_code=0
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.pf5WJJTLHs
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + rm -f /tmp/tmp.pf5WJJTLHs
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + return 0
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + is_version_at_least 1.3 1.4
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local ver1=1.3.0.0.0.0
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local ver2=1.4
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local log=/tmp/tmp.XtE6waijwo
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + err_code=1
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.XtE6waijwo
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + rm -f /tmp/tmp.XtE6waijwo
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + [[ 1 -gt 1 ]]
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + METASTORE_URIS=thrift://cluster-89fa-m:9083
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris --value thrift://cluster-89fa-m:9083 --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + METADATA_STORE=jdbc:mysql://cluster-89fa-m/metastore
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name javax.jdo.option.ConnectionURL --value jdbc:mysql://cluster-89fa-m/metastore --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + [[ 1 -gt 1 ]]
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + set -e
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + loginfo 'Running configure_hdfs.sh'
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + echo 'Running configure_hdfs.sh'
<13>Oct 28 02:17:55 google-dataproc-startup[843]: Running configure_hdfs.sh
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + HDFS_ADMIN=hdfs
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + export HDFS_NAME_DIR=/hadoop/dfs/name
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + HDFS_NAME_DIR=/hadoop/dfs/name
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + export HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + DATA_DIRS=($(get_data_dirs))
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ get_data_dirs
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ mount_points=($(find /mnt/[0-9]*/ -maxdepth 0 || true))
<13>Oct 28 02:17:55 google-dataproc-startup[843]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Oct 28 02:17:55 google-dataproc-startup[843]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Oct 28 02:17:55 google-dataproc-startup[843]: +++ true
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ local mount_points
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ ((  0  ))
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ echo /
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ return
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + mkdir -p /hadoop/dfs /hadoop/dfs/data
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + chown hdfs:hadoop -L -R /hadoop/dfs /hadoop/dfs/data
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + chmod 700 /hadoop/dfs/data
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ awk '/^Mem:/{print $2}'
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ free -m
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + TOTAL_MEM=7483
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ python -c 'print int(7483 *     0.4 / 2)'
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + NAMENODE_MEM_MB=1496
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + SECONDARYNAMENODE_MEM_MB=1496
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + export HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + [[ 1 -gt 1 ]]
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + TEMPLATE=hdfs-template.xml
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file hdfs-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + ((  2 == 0  ))
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ get_java_property /tmp/cluster/properties/dataproc.properties simplified.scaling.enable
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ local property_file=/tmp/cluster/properties/dataproc.properties
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ local property_name=simplified.scaling.enable
<13>Oct 28 02:17:55 google-dataproc-startup[843]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:55 google-dataproc-startup[843]: +++ tail -n 1
<13>Oct 28 02:17:55 google-dataproc-startup[843]: +++ grep '^simplified.scaling.enable=' /tmp/cluster/properties/dataproc.properties
<13>Oct 28 02:17:55 google-dataproc-startup[843]: +++ cut -d = -f 2-
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ local property_value=
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ echo ''
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + [[ '' == \t\r\u\e ]]
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + set -e
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + loginfo 'Running configure_connectors.sh'
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + echo 'Running configure_connectors.sh'
<13>Oct 28 02:17:55 google-dataproc-startup[843]: Running configure_connectors.sh
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + ((  1  ))
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + export GCS_METADATA_CACHE_TYPE=FILESYSTEM_BACKED
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + GCS_METADATA_CACHE_TYPE=FILESYSTEM_BACKED
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ get_nfs_mount_point
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ echo /hadoop_gcs_connector_metadata_cache
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + export GCS_FILE_CACHE_DIRECTORY=/hadoop_gcs_connector_metadata_cache
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + GCS_FILE_CACHE_DIRECTORY=/hadoop_gcs_connector_metadata_cache
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file gcs-core-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: ++ hostname -s
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + [[ cluster-89fa-m == \c\l\u\s\t\e\r\-\8\9\f\a\-\m ]]
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + [[ 1 -ne 0 ]]
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + setup_cache_cleaner
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + mkdir -p /usr/lib/hadoop/google
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local cleaner=/usr/lib/hadoop/google/clean-caches.sh
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + make_cache_cleaner_script
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local gc_cleaner=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemCacheCleaner
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local etab=/var/lib/nfs/etab
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + chmod 755 /usr/lib/hadoop/google/clean-caches.sh
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + make_cleaner_crontab /usr/lib/hadoop/google/clean-caches.sh
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + local cleaner=/usr/lib/hadoop/google/clean-caches.sh
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file bq-mapred-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + set -e
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + set -o nounset
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + loginfo 'Running configure_spark.sh'
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + echo 'Running configure_spark.sh'
<13>Oct 28 02:17:55 google-dataproc-startup[843]: Running configure_spark.sh
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + SPARK_EVENTLOG_DIR=hdfs://cluster-89fa-m/user/spark/eventlog
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + SPARK_TMPDIR=/hadoop/spark/tmp
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + SPARK_WORKDIR=/hadoop/spark/work
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + SPARK_LOG_DIR=/var/log/spark
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + mkdir -p /hadoop/spark/tmp /hadoop/spark/work /var/log/spark
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + chown spark:spark -R /hadoop/spark /var/log/spark
<13>Oct 28 02:17:55 google-dataproc-startup[843]: + chmod 1777 -R /hadoop/spark /var/log/spark
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + is_version_at_least 2.3.3 2
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local ver1=2.3.3.0.0.0.0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local ver2=2
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local log=/tmp/tmp.eGAqjKUDbc
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + dpkg --compare-versions 2.3.3.0.0.0.0 '>=' 2
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + err_code=0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.eGAqjKUDbc
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + rm -f /tmp/tmp.eGAqjKUDbc
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + return 0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + RPC_SIZE_KEY=spark.rpc.message.maxSize
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + NUM_INITIAL_EXECUTORS_KEY=spark.executor.instances
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + SPARK_YARN_DIR=/usr/lib/spark/yarn
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + SPARKR_LIB_DIR=/usr/lib/spark/R/lib
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + [[ -f /usr/lib/spark/R/lib/sparkr.zip ]]
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ free -m
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ awk '/^Mem:/{print $2}'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + TOTAL_MEM=7483
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ python -c 'print int(7483 * 0.15)'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + SPARK_DAEMON_MEMORY=1122
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ python -c 'print int(7483 / 4)'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + SPARK_DRIVER_MEM_MB=1870
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ python -c 'print int(1870 / 2)'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + SPARK_DRIVER_MAX_RESULT_MB=935
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ find /tmp/mrv2_Mj7_tmp_env.sh
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ head -1
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + YARN_MEMORY_ENV=/tmp/mrv2_Mj7_tmp_env.sh
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + source /tmp/mrv2_Mj7_tmp_env.sh
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export YARN_MIN_MEM_MB=512
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ YARN_MIN_MEM_MB=512
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export YARN_MAX_MEM_MB=5632
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ YARN_MAX_MEM_MB=5632
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export NODEMANAGER_MEM_MB=5632
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ NODEMANAGER_MEM_MB=5632
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export APP_MASTER_MEM_MB=5632
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ APP_MASTER_MEM_MB=5632
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ CORES_PER_APP_MASTER_ROUNDED=2
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export MAP_MEM_MB=2560
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ MAP_MEM_MB=2560
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ CORES_PER_MAP_ROUNDED=1
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ MAP_JAVA_OPTS=-Xmx2048m
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export REDUCE_MEM_MB=5632
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ REDUCE_MEM_MB=5632
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export CORES_PER_REDUCE_ROUNDED=2
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ CORES_PER_REDUCE_ROUNDED=2
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ export REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ REDUCE_JAVA_OPTS=-Xmx4505m
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ python
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ cat
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + SPARK_EXECUTOR_MEMORY=2560
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ python -c 'print max(1,     2 / 2)'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + SPARK_EXECUTOR_CORES=1
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ python -c 'print int(max(     2560 / 11, 384))'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + SPARK_YARN_EXECUTOR_MEMORY_OVERHEAD=384
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + SPARK_EXECUTOR_MEMORY=2176
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + is_version_at_least 1.3 1.4
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local ver1=1.3.0.0.0.0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local ver2=1.4
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local log=/tmp/tmp.X0Owud0WJ3
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.4
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + err_code=1
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.X0Owud0WJ3
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + rm -f /tmp/tmp.X0Owud0WJ3
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + is_version_at_least 1.3 1.3
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local ver1=1.3.0.0.0.0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local ver2=1.3
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local log=/tmp/tmp.26JD3aWLba
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + err_code=0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.26JD3aWLba
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + rm -f /tmp/tmp.26JD3aWLba
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + return 0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + set -e
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + loginfo 'Running configure_tez.sh'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + echo 'Running configure_tez.sh'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: Running configure_tez.sh
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + readonly CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + CONFIG_CHECKSUM=23fbfca8f7b8e142395c6bb4676427ae
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + is_version_at_least 1.3 1.3
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local ver1=1.3.0.0.0.0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local ver2=1.3
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local log=/tmp/tmp.bKHcOvhtw6
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + err_code=0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.bKHcOvhtw6
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + rm -f /tmp/tmp.bKHcOvhtw6
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + return 0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ ls /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + configure_war /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local -r tez_war=/usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ mktemp -d
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local -r tmp_dir=/tmp/tmp.8dR7bDL3w0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + unzip -q /usr/lib/tez/tez-ui-0.9.2.war -d /tmp/tmp.8dR7bDL3w0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local -r tez_configs=/tmp/tmp.8dR7bDL3w0/config/configs.env
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ cut -d ' ' -f 1
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ md5sum /tmp/tmp.8dR7bDL3w0/config/configs.env
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + [[ 23fbfca8f7b8e142395c6bb4676427ae != \2\3\f\b\f\c\a\8\f\7\b\8\e\1\4\2\3\9\5\c\6\b\b\4\6\7\6\4\2\7\a\e ]]
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + [[ -f /tmp/tmp.8dR7bDL3w0/config/configs.env ]]
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + sed -i 's#\(.*\)//timeline: "http://localhost:8188"\(.*\)#\1timeline: "http://cluster-89fa-m:8188"\2#' /tmp/tmp.8dR7bDL3w0/config/configs.env
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + sed -i 's#\(.*\)//rm: "http://localhost:8088"\(.*\)#\1rm: "http://cluster-89fa-m:8088"\2#' /tmp/tmp.8dR7bDL3w0/config/configs.env
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ get_dataproc_property dataproc.components.activate
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ local property_name=dataproc.components.activate
<13>Oct 28 02:17:56 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.components.activate
<13>Oct 28 02:17:56 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:56 google-dataproc-startup[843]: +++ local property_name=dataproc.components.activate
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++++ grep '^dataproc.components.activate=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:56 google-dataproc-startup[843]: +++ local property_value=
<13>Oct 28 02:17:56 google-dataproc-startup[843]: +++ echo ''
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ local property_value=
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ echo ''
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + local -r optional_components_value=
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + [[ '' == *\k\n\o\x* ]]
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + cd /tmp/tmp.8dR7bDL3w0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + zip -q /usr/lib/tez/tez-ui-0.9.2.war -r ./assets ./config ./fonts ./index.html ./META-INF ./WEB-INF
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + cd ..
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + rm -rf /tmp/tmp.8dR7bDL3w0
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ stat /usr/lib/tez/tez-common-0.9.2.jar --format=%Y
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + touch -d @1568819629 /usr/lib/tez/tez-ui-0.9.2.war
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-names --value tez --clobber
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-on-disk-path.tez --value /usr/lib/tez/tez-ui-0.9.2.war --clobber
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-web-path.tez --value /tez-ui --clobber
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.history.logging.service.class --value org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService --clobber
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.tez-ui.history-url.base --value http://cluster-89fa-m:8188/tez-ui/ --clobber
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.am.node-blacklisting.enabled --value false --clobber
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + set -e
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + declare -r ZOOKEEPER_CONFIG=/etc/zookeeper/conf/zoo.cfg
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + declare -r ZOOKEEPER_DATA_DIR=/var/lib/zookeeper/
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + (( i=0 ))
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + (( i<1 ))
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + echo server.0=cluster-89fa-m:2888:3888
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + (( i++  ))
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + (( i<1 ))
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + echo autopurge.purgeInterval=168
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ sed -e 's/.*-m-//'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ uname -n
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + declare -r MY_ID=cluster-89fa-m
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + echo cluster-89fa-m
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + [[ false == \t\r\u\e ]]
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + loginfo 'Populating initial cluster member list'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: + echo 'Populating initial cluster member list'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: Populating initial cluster member list
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ get_dataproc_property dataproc.worker.custom.init.actions.mode
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++ local property_name=dataproc.worker.custom.init.actions.mode
<13>Oct 28 02:17:56 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.worker.custom.init.actions.mode
<13>Oct 28 02:17:56 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:56 google-dataproc-startup[843]: +++ local property_name=dataproc.worker.custom.init.actions.mode
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:56 google-dataproc-startup[843]: ++++ grep '^dataproc.worker.custom.init.actions.mode=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:56 google-dataproc-startup[843]: +++ local property_value=
<13>Oct 28 02:17:56 google-dataproc-startup[843]: +++ echo ''
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local property_value=
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ echo ''
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + WORKER_CUSTOM_INIT_ACTIONS_MODE=
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + WORKER_COUNT=2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ get_dataproc_property simplified.scaling.enable
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local property_name=simplified.scaling.enable
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties simplified.scaling.enable
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ local property_name=simplified.scaling.enable
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++++ grep '^simplified.scaling.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ local property_value=
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ echo ''
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local property_value=
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ echo ''
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + SIMPLIFIED_SCALING_ENABLED=
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ '' != \t\r\u\e ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + ((  2 == 0  ))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ Master == \M\a\s\t\e\r ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ '' == \R\U\N\_\B\E\F\O\R\E\_\S\E\R\V\I\C\E\S ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + MEMBERSHIP_FILE=/etc/hadoop/conf/nodes_include
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + (( i=0 ))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + (( i<2 ))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo cluster-89fa-w-0.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + (( i++  ))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + (( i<2 ))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo cluster-89fa-w-1.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + (( i++  ))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + (( i<2 ))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merging user-specified cluster properties'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merging user-specified cluster properties'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merging user-specified cluster properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_xml_properties /tmp/cluster/properties/capacity-scheduler.xml /etc/hadoop/conf/capacity-scheduler.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/capacity-scheduler.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/hadoop/conf/capacity-scheduler.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/capacity-scheduler.xml ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file /tmp/cluster/properties/capacity-scheduler.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/capacity-scheduler.xml.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_xml_properties /tmp/cluster/properties/core.xml /etc/hadoop/conf/core-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/core.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/hadoop/conf/core-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/core.xml ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file /tmp/cluster/properties/core.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/core.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/core.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/core.xml.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_xml_properties /tmp/cluster/properties/distcp.xml /etc/hadoop/conf/distcp-default.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/distcp.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/hadoop/conf/distcp-default.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/distcp.xml ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file /tmp/cluster/properties/distcp.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/distcp.xml.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_xml_properties /tmp/cluster/properties/hdfs.xml /etc/hadoop/conf/hdfs-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/hdfs.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/hadoop/conf/hdfs-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/hdfs.xml ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /tmp/cluster/properties/hdfs.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/hdfs.xml.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_xml_properties /tmp/cluster/properties/mapred.xml /etc/hadoop/conf/mapred-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/mapred.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/hadoop/conf/mapred-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/mapred.xml ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file /tmp/cluster/properties/mapred.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/mapred.xml.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_xml_properties /tmp/cluster/properties/yarn.xml /etc/hadoop/conf/yarn-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/yarn.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/hadoop/conf/yarn-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/yarn.xml ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /tmp/cluster/properties/yarn.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/yarn.xml.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_xml_properties /tmp/cluster/properties/hive.xml /etc/hive/conf/hive-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/hive.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/hive/conf/hive-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/hive.xml ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/hive/conf/hive-site.xml --source_configuration_file /tmp/cluster/properties/hive.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/hive.xml.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_java_properties /tmp/cluster/properties/pig.properties /etc/pig/conf/pig.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/pig.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/pig/conf/pig.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/pig.properties ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + cat /tmp/cluster/properties/pig.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/pig.properties.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_java_properties /tmp/cluster/properties/spark.properties /etc/spark/conf/spark-defaults.conf
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/spark.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/spark/conf/spark-defaults.conf
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/spark.properties ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + cat /tmp/cluster/properties/spark.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/spark.properties.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_java_properties /tmp/cluster/properties/zookeeper.properties /etc/zookeeper/conf/zoo.cfg
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/zookeeper.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/zookeeper/conf/zoo.cfg
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/zookeeper.properties ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + cat /tmp/cluster/properties/zookeeper.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/zookeeper.properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/zookeeper.properties.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_sh_env_vars /tmp/cluster/properties/hadoop-env.sh /etc/hadoop/conf/hadoop-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/hadoop-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/hadoop/conf/hadoop-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/hadoop-env.sh ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + cat /tmp/cluster/properties/hadoop-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/hadoop-env.sh.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_sh_env_vars /tmp/cluster/properties/mapred-env.sh /etc/hadoop/conf/mapred-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/mapred-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/hadoop/conf/mapred-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/mapred-env.sh ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + cat /tmp/cluster/properties/mapred-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/mapred-env.sh.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_sh_env_vars /tmp/cluster/properties/spark-env.sh /etc/spark/conf/spark-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/spark-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/spark/conf/spark-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/spark-env.sh ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + cat /tmp/cluster/properties/spark-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/spark-env.sh.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_sh_env_vars /tmp/cluster/properties/yarn-env.sh /etc/hadoop/conf/yarn-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/yarn-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/hadoop/conf/yarn-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/yarn-env.sh ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo -e '\n# User-supplied properties.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + cat /tmp/cluster/properties/yarn-env.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/yarn-env.sh.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ -f /etc/hbase/conf/hbase-site.xml ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ -f /etc/tez/conf/tez-site.xml ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + merge_xml_properties /tmp/cluster/properties/tez.xml /etc/tez/conf/tez-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local src=/tmp/cluster/properties/tez.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local dest=/etc/tez/conf/tez-site.xml
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ ! -f /tmp/cluster/properties/tez.xml ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + bdconfig merge_configurations --configuration_file /etc/tez/conf/tez-site.xml --source_configuration_file /tmp/cluster/properties/tez.xml --resolve_environment_variables --create_if_absent --clobber
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Merged /tmp/cluster/properties/tez.xml.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ false == \t\r\u\e ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + ACTIVATABLE_COMPONENTS=(${DATAPROC_OPTIONAL_COMPONENTS})
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + DATAPROC_NON_DEBIAN_COMPONENTS=(${DATAPROC_NON_DEBIAN_COMPONENTS})
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + PACKAGES_TO_KEEP=("${SERVICES[@]}" ${DATAPROC_COMMON_PACKAGES})
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + DATAPROC_START_AFTER_HDFS_SERVICES=(${DATAPROC_START_AFTER_HDFS_SERVICES})
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + COMPONENTS_TO_ACTIVATE=($(intersection COMPONENTS_TO_ACTIVATE PACKAGES_TO_KEEP))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ intersection COMPONENTS_TO_ACTIVATE PACKAGES_TO_KEEP
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local -n values=COMPONENTS_TO_ACTIVATE
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local -n filter=PACKAGES_TO_KEEP
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ sort -u
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ printf '%s\n' hadoop-hdfs-namenode hadoop-yarn-resourcemanager hive-metastore hive-server2 zookeeper-server solr-server hadoop-mapreduce-historyserver spark-history-server hive-webhcat-server jupyter knox proxy-agent zeppelin hadoop-yarn-timelineserver mariadb-server hadoop-hdfs-secondarynamenode openjdk-8-jdk openjdk-8-dbg libjansi-java python-numpy libmysql-java hadoop-client hive pig spark-core spark-python spark-r autofs libhdfs0 libsnappy-dev libzstd-dev libatlas3-base libopenblas-base libapr1 vim git bash-completion spark-yarn-shuffle spark-datanucleus spark-extras hadoop-lzo python-setuptools anaconda druid kafka-server kerberos presto openssl tez hive-hcatalog
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ sort -u
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ printf '%s\n'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + NON_ACTIVATED_COMPONENTS=($(difference ACTIVATABLE_COMPONENTS COMPONENTS_TO_ACTIVATE))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ difference ACTIVATABLE_COMPONENTS COMPONENTS_TO_ACTIVATE
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local -n filter=COMPONENTS_TO_ACTIVATE
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ sort -u
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ printf '%s\n'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ sort -u
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ printf '%s\n' zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + PACKAGES_TO_UNINSTALL+=($(difference NON_ACTIVATED_COMPONENTS PACKAGES_TO_UNINSTALL))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ difference NON_ACTIVATED_COMPONENTS PACKAGES_TO_UNINSTALL
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local -n values=NON_ACTIVATED_COMPONENTS
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local -n filter=PACKAGES_TO_UNINSTALL
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ sort -u
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ sort -u
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ printf '%s\n' anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + NON_DEBIAN_COMPONENTS_TO_UNINSTALL=($(intersection PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ intersection PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local -n values=PACKAGES_TO_UNINSTALL
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local -n filter=DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ sort -u
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ printf '%s\n' anaconda jupyter kerberos presto proxy-agent
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ sort -u
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + DEBIAN_COMPONENTS_TO_UNINSTALL=($(difference PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ difference PACKAGES_TO_UNINSTALL DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local -n values=PACKAGES_TO_UNINSTALL
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local -n filter=DATAPROC_NON_DEBIAN_COMPONENTS
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ sort -u
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ printf '%s\n' anaconda jupyter kerberos presto proxy-agent
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ printf '%s\n' hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-hdfs-datanode hadoop-yarn-nodemanager anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin zookeeper-server
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ sort -u
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ false != \t\r\u\e ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + DEBIAN_COMPONENTS_TO_UNINSTALL+=('krb5-kpropd' 'krb5-kdc' 'krb5-admin-server' 'krb5-user' 'krb5-config' 'xinetd')
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + uninstall_packages
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_with_retries set_selections
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local retry_backoff
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + cmd=("$@")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local -a cmd
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'About to run '\''set_selections'\'' with retries...'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'About to run '\''set_selections'\'' with retries...'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: About to run 'set_selections' with retries...
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local update_succeeded=0
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + (( i = 0 ))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + (( i < 12 ))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + set_selections
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + cat
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + debconf-set-selections
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + update_succeeded=1
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + break
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + ((  1  ))
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Uninstalling un-needed daemons'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Uninstalling un-needed daemons'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Uninstalling un-needed daemons
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_in_background --tag uninstall bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + PID=1443
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Started background process [bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true] as pid 1443'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Started background process [bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true] as pid 1443
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_in_background --tag uninstall-component-anaconda uninstall_component anaconda
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + PID=1444
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Started background process [uninstall_component anaconda] as pid 1444'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Started background process [uninstall_component anaconda] as pid 1444
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_in_background --tag uninstall-component-jupyter uninstall_component jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + PID=1445
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Started background process [uninstall_component jupyter] as pid 1445'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Started background process [uninstall_component jupyter] as pid 1445
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_in_background --tag uninstall-component-kerberos uninstall_component kerberos
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + PID=1446
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Started background process [uninstall_component kerberos] as pid 1446'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Started background process [uninstall_component kerberos] as pid 1446
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_in_background --tag uninstall-component-presto uninstall_component presto
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + PID=1447
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Started background process [uninstall_component presto] as pid 1447'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Started background process [uninstall_component presto] as pid 1447
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + for component in "${NON_DEBIAN_COMPONENTS_TO_UNINSTALL[@]}"
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_in_background --tag uninstall-component-proxy-agent uninstall_component proxy-agent
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + PID=1448
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Started background process [uninstall_component proxy-agent] as pid 1448'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Started background process [uninstall_component proxy-agent] as pid 1448
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + is_version_at_least 1.3 1.3
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local ver1=1.3.0.0.0.0
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local ver2=1.3
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_with_logger --tag uninstall-component-presto uninstall_component presto
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local pid=1447
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + tag=uninstall-component-presto
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ mktemp
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_with_logger --tag uninstall-component-jupyter uninstall_component jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local pid=1445
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + tag=uninstall-component-jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ logger -s -t 'uninstall-component-jupyter[1445]'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_with_logger --tag uninstall bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local pid=1443
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + tag=uninstall
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_with_logger --tag uninstall-component-proxy-agent uninstall_component proxy-agent
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local pid=1448
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + tag=uninstall-component-proxy-agent
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_with_logger --tag uninstall-component-kerberos uninstall_component kerberos
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ logger -s -t 'uninstall-component-presto[1447]'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local pid=1446
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + tag=uninstall-component-kerberos
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + run_with_logger --tag uninstall-component-anaconda uninstall_component anaconda
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local log=/tmp/tmp.a2vn6Ym8Js
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + dpkg --compare-versions 1.3.0.0.0.0 '>=' 1.3
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + local pid=1444
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + tag=uninstall-component-anaconda
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: + uninstall_component jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: + local component=jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: + set -euxo pipefail
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ logger -s -t 'uninstall[1443]'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ logger -s -t 'uninstall-component-kerberos[1446]'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + uninstall_component presto
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + local component=presto
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + set -exo pipefail
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ logger -s -t 'uninstall-component-proxy-agent[1448]'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ logger -s -t 'uninstall-component-anaconda[1444]'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/jupyter.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + err_code=0
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + grep -C 10 -i warning /tmp/tmp.a2vn6Ym8Js
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/jupyter.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/presto.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall[1443]: + bash -c 'DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-kerberos[1446]: + uninstall_component kerberos
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-kerberos[1446]: + local component=kerberos
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-kerberos[1446]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-kerberos[1446]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-kerberos[1446]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-kerberos[1446]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-kerberos[1446]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/kerberos.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-kerberos[1446]: + set -euxo pipefail
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ export WHEEL_DIR=/opt/dataproc/jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ WHEEL_DIR=/opt/dataproc/jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ export JGSCM_WHEEL=/opt/dataproc/jupyter/jgscm
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ JGSCM_WHEEL=/opt/dataproc/jupyter/jgscm
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ export JGSCM_REQUIREMENTS=/opt/dataproc/jupyter/jgscm.requirements
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ JGSCM_REQUIREMENTS=/opt/dataproc/jupyter/jgscm.requirements
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ export JUPYTER_ETC_DIR=/etc/jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ JUPYTER_ETC_DIR=/etc/jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ export JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ JUPYTER_ENV_FILE=/etc/default/jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ export JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ JUPYTER_CONFIG_FILE=/etc/jupyter/jupyter_notebook_config.py
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ export PYSPARK_KERNELSPEC=/opt/conda/anaconda/share/jupyter/kernels/pyspark/kernel.json
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ PYSPARK_KERNELSPEC=/opt/conda/anaconda/share/jupyter/kernels/pyspark/kernel.json
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ export JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ JUPYTER_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyter.service
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ export JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: ++ JUPYTERHUB_SYSTEMD_UNIT=/usr/lib/systemd/system/jupyterhub.service
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: + rm -Rf /etc/jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + rm -f /tmp/tmp.a2vn6Ym8Js
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/presto.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + return 0
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ readonly HTTP_PORT=8060
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ HTTP_PORT=8060
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ readonly PRESTO_BIN_PATH=/usr/bin/presto
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ PRESTO_BIN_PATH=/usr/bin/presto
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ readonly PRESTO_CLI_PATH=/opt/presto-cli
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ PRESTO_CLI_PATH=/opt/presto-cli
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ readonly PRESTO_DATA_DIRECTORY=/var/presto/data
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ PRESTO_DATA_DIRECTORY=/var/presto/data
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ readonly PRESTO_INSTALL_DIRECTORY=/opt/presto-server
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ PRESTO_INSTALL_DIRECTORY=/opt/presto-server
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ readonly INIT_SCRIPT=/usr/lib/systemd/system/presto.service
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ INIT_SCRIPT=/usr/lib/systemd/system/presto.service
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ readonly PRESTO_VERSION=0.215
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: ++ PRESTO_VERSION=0.215
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + rm -Rf /opt/presto-server
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: + uninstall_component proxy-agent
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: + local component=proxy-agent
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: + set -exo pipefail
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/proxy-agent.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/proxy-agent.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: ++ readonly PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: ++ PROXY_AGENT_SERVICE_NAME=google-dataproc-component-gateway
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: ++ readonly PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: ++ PROXY_AGENT_INSTALL_LOCATION=/usr/bin/proxy-forwarding-agent
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: ++ readonly PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: ++ PROXY_AGENT_INIT_SCRIPT=/usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: + rm -f /usr/bin/proxy-forwarding-agent
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-jupyter[1445]: + rm -Rf /opt/dataproc/jupyter
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-proxy-agent[1448]: + rm -f /usr/lib/systemd/system/google-dataproc-component-gateway.service
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ get_dataproc_property dataproc.monitoring.stackdriver.enable
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local property_name=dataproc.monitoring.stackdriver.enable
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: + uninstall_component anaconda
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.monitoring.stackdriver.enable
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ local property_name=dataproc.monitoring.stackdriver.enable
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: + local component=anaconda
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: + local uninstall_script=/usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: + echo 'Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: Running component uninstall script: /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: + set -exo pipefail
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/uninstall/anaconda.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: + source /usr/local/share/google/dataproc/bdutil/components/uninstall/../shared/anaconda.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++++ grep '^dataproc.monitoring.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: ++ export ANACONDA_INSTALL_PATH=/opt/conda/anaconda
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: ++ ANACONDA_INSTALL_PATH=/opt/conda/anaconda
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: ++ export ANACONDA_BIN_DIR=/opt/conda/anaconda/bin
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: ++ ANACONDA_BIN_DIR=/opt/conda/anaconda/bin
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: ++ export PATH=/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: ++ PATH=/opt/conda/anaconda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-anaconda[1444]: + rm -Rf /opt/conda/anaconda
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ local property_value=false
<13>Oct 28 02:17:57 google-dataproc-startup[843]: +++ echo false
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ local property_value=false
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ echo false
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + STACKDRIVER_MONITORING_ENABLED=false
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + [[ false == \t\r\u\e ]]
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Stackdriver monitoring disabled.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Stackdriver monitoring disabled.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Stackdriver monitoring disabled.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Replace dataproc plugin instance_name label with gce instance name.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Replace dataproc plugin instance_name label with gce instance name.'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Replace dataproc plugin instance_name label with gce instance name.
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + for PLUGIN_FILE in /opt/stackdriver/collectd/etc/collectd.d/dataproc*
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ hostname
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + sed -i 's/"label:instance_name".*$/"label:instance_name" "cluster-89fa-m"/g' /opt/stackdriver/collectd/etc/collectd.d/dataproc_collectd_default-20170324-133642.conf
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + chmod +x /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + /usr/local/share/google/dataproc/bdutil/verify_setup.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + loginfo 'Running verify_setup.sh'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: + echo 'Running verify_setup.sh'
<13>Oct 28 02:17:57 google-dataproc-startup[843]: Running verify_setup.sh
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + rm -Rf /var/presto/data
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + rm -f /usr/bin/presto
<13>Oct 28 02:17:57 google-dataproc-startup[843]: ++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.warehouse.dir
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + rm -f /opt/presto-cli
<13>Oct 28 02:17:57 google-dataproc-startup[843]: <13>Oct 28 02:17:57 uninstall-component-presto[1447]: + rm -f /usr/lib/systemd/system/presto.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + hive_warehouse_dir=None
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ None == \g\s\:\/\/* ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + loginfo 'Starting services'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Starting services'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Starting services
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array hadoop-hdfs-namenode ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=hadoop-hdfs-namenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-hdfs-namenode  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + case "${SERVICE}" in
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_in_background --tag setup-hadoop-hdfs-namenode setup_service hadoop-hdfs-namenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + PID=1505
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Started background process [setup_service hadoop-hdfs-namenode] as pid 1505'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Started background process [setup_service hadoop-hdfs-namenode] as pid 1505
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array hadoop-yarn-resourcemanager ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=hadoop-yarn-resourcemanager
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-yarn-resourcemanager  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + case "${SERVICE}" in
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_in_background --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + PID=1506
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Started background process [setup_service hadoop-yarn-resourcemanager] as pid 1506'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Started background process [setup_service hadoop-yarn-resourcemanager] as pid 1506
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array hive-metastore ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=hive-metastore
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-metastore  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + case "${SERVICE}" in
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_in_background --tag setup-hive-metastore setup_service hive-metastore
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + PID=1507
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Started background process [setup_service hive-metastore] as pid 1507'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Started background process [setup_service hive-metastore] as pid 1507
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array hive-server2 ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=hive-server2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-server2  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + case "${SERVICE}" in
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_in_background --tag setup-hive-server2 setup_service hive-server2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + PID=1508
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Started background process [setup_service hive-server2] as pid 1508'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Started background process [setup_service hive-server2] as pid 1508
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array zookeeper-server ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=zookeeper-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  zookeeper-server  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + continue
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array solr-server ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=solr-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  solr-server  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + continue
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array hadoop-mapreduce-historyserver ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=hadoop-mapreduce-historyserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-mapreduce-historyserver  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + case "${SERVICE}" in
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_in_background --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + PID=1509
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_with_logger --tag setup-hive-server2 setup_service hive-server2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_with_logger --tag setup-hadoop-yarn-resourcemanager setup_service hadoop-yarn-resourcemanager
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local pid=1508
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local pid=1506
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + tag=setup-hive-server2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Started background process [setup_service hadoop-mapreduce-historyserver] as pid 1509'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Started background process [setup_service hadoop-mapreduce-historyserver] as pid 1509
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array spark-history-server ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=spark-history-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  spark-history-server  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + case "${SERVICE}" in
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_in_background --tag setup-spark-history-server setup_service spark-history-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + tag=setup-hadoop-yarn-resourcemanager
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + PID=1510
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Started background process [setup_service spark-history-server] as pid 1510'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Started background process [setup_service spark-history-server] as pid 1510
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array hive-webhcat-server ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=hive-webhcat-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hive-webhcat-server  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + continue
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array jupyter ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=jupyter
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  jupyter  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + continue
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array knox ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=knox
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  knox  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + continue
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array proxy-agent ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=proxy-agent
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  proxy-agent  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + continue
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array zeppelin ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=zeppelin
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  zeppelin  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + continue
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array hadoop-yarn-timelineserver ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=hadoop-yarn-timelineserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-yarn-timelineserver  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + case "${SERVICE}" in
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_in_background --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + PID=1511
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Started background process [setup_service hadoop-yarn-timelineserver] as pid 1511'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Started background process [setup_service hadoop-yarn-timelineserver] as pid 1511
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array mariadb-server ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=mariadb-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  mariadb-server  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + case "${SERVICE}" in
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_in_background --tag setup-mariadb setup_service mariadb
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + PID=1512
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Started background process [setup_service mariadb] as pid 1512'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Started background process [setup_service mariadb] as pid 1512
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + for SERVICE in "${SERVICES[@]}"
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + in_array hadoop-hdfs-secondarynamenode ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local value=hadoop-hdfs-secondarynamenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local -n values=ACTIVATABLE_COMPONENTS
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ !  zookeeper-server anaconda druid hive-webhcat-server jupyter kafka-server kerberos knox presto proxy-agent solr-server zeppelin  =~  hadoop-hdfs-secondarynamenode  ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_with_logger --tag setup-hadoop-mapreduce-historyserver setup_service hadoop-mapreduce-historyserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local pid=1509
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + tag=setup-hadoop-mapreduce-historyserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_with_logger --tag setup-hive-metastore setup_service hive-metastore
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local pid=1507
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + tag=setup-hive-metastore
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + return 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + case "${SERVICE}" in
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_in_background --tag setup-hadoop-hdfs-secondarynamenode setup_service hadoop-hdfs-secondarynamenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + PID=1521
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Started background process [setup_service hadoop-hdfs-secondarynamenode] as pid 1521'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Started background process [setup_service hadoop-hdfs-secondarynamenode] as pid 1521
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + loginfo 'Configuring optional components'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Configuring optional components'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Configuring optional components
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_with_logger --tag setup-mariadb setup_service mariadb
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_with_logger --tag setup-hadoop-yarn-timelineserver setup_service hadoop-yarn-timelineserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local pid=1511
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + tag=setup-hadoop-yarn-timelineserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local pid=1512
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + tag=setup-mariadb
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_with_logger --tag setup-hadoop-hdfs-namenode setup_service hadoop-hdfs-namenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local pid=1505
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + tag=setup-hadoop-hdfs-namenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ logger -s -t 'setup-hadoop-mapreduce-historyserver[1509]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ logger -s -t 'setup-hive-metastore[1507]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_with_logger --tag setup-spark-history-server setup_service spark-history-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local pid=1510
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + tag=setup-spark-history-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ logger -s -t 'setup-hive-server2[1508]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ logger -s -t 'setup-hadoop-yarn-timelineserver[1511]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ logger -s -t 'setup-hadoop-hdfs-namenode[1505]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ logger -s -t 'setup-hadoop-yarn-resourcemanager[1506]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + setup_service hive-metastore
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + export KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + export -f login_through_keytab_if_necessary
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + export MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + local service=hive-metastore
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + [[ hive-metastore == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + wait_for_port cluster-89fa-m 3306
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + local -r host=cluster-89fa-m
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + local -r port=3306
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + loginfo 'Waiting for service to come up on host=cluster-89fa-m port=3306.'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + echo 'Waiting for service to come up on host=cluster-89fa-m port=3306.'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: Waiting for service to come up on host=cluster-89fa-m port=3306.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + retry_with_constant_backoff nc -v -z -w 0 cluster-89fa-m 3306
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + local max_retry=300
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + cmd=("$@")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + local -a cmd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + local update_succeeded=0
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: ++ seq 1 300
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ logger -s -t 'setup-spark-history-server[1510]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_with_logger --tag setup-hadoop-hdfs-secondarynamenode setup_service hadoop-hdfs-secondarynamenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local pid=1521
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + tag=setup-hadoop-hdfs-secondarynamenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + setup_service hive-server2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + export KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + export -f login_through_keytab_if_necessary
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + export MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + local service=hive-server2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + [[ hive-server2 == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + enable_service hive-server2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + local service=hive-server2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + local unit=hive-server2.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + run_with_retries systemctl enable hive-server2.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + local retry_backoff
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + cmd=("$@")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + local -a cmd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + loginfo 'About to run '\''systemctl enable hive-server2.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + echo 'About to run '\''systemctl enable hive-server2.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: About to run 'systemctl enable hive-server2.service' with retries...
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + local update_succeeded=0
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + (( i = 0 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + (( i < 12 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: + systemctl enable hive-server2.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: hive-server2.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-server2[1508]: Executing: /lib/systemd/systemd-sysv-install enable hive-server2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ logger -s -t 'setup-mariadb[1512]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + setup_service hadoop-yarn-timelineserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + export KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + export -f login_through_keytab_if_necessary
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + export MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + local service=hadoop-yarn-timelineserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + enable_service hadoop-yarn-timelineserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + local service=hadoop-yarn-timelineserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + local unit=hadoop-yarn-timelineserver.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + run_with_retries systemctl enable hadoop-yarn-timelineserver.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + local retry_backoff
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + cmd=("$@")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + local -a cmd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + loginfo 'About to run '\''systemctl enable hadoop-yarn-timelineserver.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + echo 'About to run '\''systemctl enable hadoop-yarn-timelineserver.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: About to run 'systemctl enable hadoop-yarn-timelineserver.service' with retries...
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + local update_succeeded=0
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + (( i = 0 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + (( i < 12 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: + systemctl enable hadoop-yarn-timelineserver.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: hadoop-yarn-timelineserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-timelineserver[1511]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-timelineserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ get_dataproc_property dataproc.conscrypt.provider.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ local property_name=dataproc.conscrypt.provider.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + setup_service hadoop-hdfs-namenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + export KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + export -f login_through_keytab_if_necessary
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + export MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + local service=hadoop-hdfs-namenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + case "${MASTER_INDEX?}" in
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + loginfo 'Formatting NameNode'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + echo 'Formatting NameNode'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: Formatting NameNode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + run_with_retries su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + local retry_backoff
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + cmd=("$@")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + local -a cmd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + loginfo 'About to run '\''su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + echo 'About to run '\''su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: About to run 'su -s /bin/bash hdfs -c source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive' with retries...
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + setup_service spark-history-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + local update_succeeded=0
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + export KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + (( i = 0 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + (( i < 12 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-namenode[1505]: + su -s /bin/bash hdfs -c 'source /etc/default/hadoop-hdfs-namenode &&             login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal &&             hdfs namenode -format -nonInteractive'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + export -f login_through_keytab_if_necessary
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + export MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + local service=spark-history-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + [[ spark-history-server == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + enable_service spark-history-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + local service=spark-history-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + local unit=spark-history-server.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + run_with_retries systemctl enable spark-history-server.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + local retry_backoff
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + cmd=("$@")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + local -a cmd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + loginfo 'About to run '\''systemctl enable spark-history-server.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + echo 'About to run '\''systemctl enable spark-history-server.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: About to run 'systemctl enable spark-history-server.service' with retries...
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + local update_succeeded=0
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + (( i = 0 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + (( i < 12 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: + systemctl enable spark-history-server.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + setup_service hadoop-mapreduce-historyserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + export KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + export -f login_through_keytab_if_necessary
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + export MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + local service=hadoop-mapreduce-historyserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + [[ hadoop-mapreduce-historyserver == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + [[ hadoop-mapreduce-historyserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + enable_service hadoop-mapreduce-historyserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + local service=hadoop-mapreduce-historyserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + local unit=hadoop-mapreduce-historyserver.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + run_with_retries systemctl enable hadoop-mapreduce-historyserver.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + local retry_backoff
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + cmd=("$@")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + local -a cmd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + loginfo 'About to run '\''systemctl enable hadoop-mapreduce-historyserver.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + echo 'About to run '\''systemctl enable hadoop-mapreduce-historyserver.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: About to run 'systemctl enable hadoop-mapreduce-historyserver.service' with retries...
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + local update_succeeded=0
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + (( i = 0 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + (( i < 12 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: + systemctl enable hadoop-mapreduce-historyserver.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + setup_service hadoop-yarn-resourcemanager
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + export KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + export -f login_through_keytab_if_necessary
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + export MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + local service=hadoop-yarn-resourcemanager
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + enable_service hadoop-yarn-resourcemanager
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + local service=hadoop-yarn-resourcemanager
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + local unit=hadoop-yarn-resourcemanager.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + run_with_retries systemctl enable hadoop-yarn-resourcemanager.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + local retry_backoff
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + cmd=("$@")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + local -a cmd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + loginfo 'About to run '\''systemctl enable hadoop-yarn-resourcemanager.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + echo 'About to run '\''systemctl enable hadoop-yarn-resourcemanager.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: About to run 'systemctl enable hadoop-yarn-resourcemanager.service' with retries...
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + local update_succeeded=0
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + (( i = 0 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + (( i < 12 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: + systemctl enable hadoop-yarn-resourcemanager.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: hadoop-yarn-resourcemanager.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-yarn-resourcemanager[1506]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-resourcemanager
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + setup_service mariadb
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + export KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + export -f login_through_keytab_if_necessary
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + export MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + local service=mariadb
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + [[ mariadb == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + enable_service mariadb
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + local service=mariadb
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 3306
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + local unit=mariadb.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + run_with_retries systemctl enable mariadb.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + local retry_backoff
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + cmd=("$@")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + local -a cmd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + loginfo 'About to run '\''systemctl enable mariadb.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + echo 'About to run '\''systemctl enable mariadb.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: About to run 'systemctl enable mariadb.service' with retries...
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + local update_succeeded=0
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + (( i = 0 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + (( i < 12 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + systemctl enable mariadb.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ logger -s -t 'setup-hadoop-hdfs-secondarynamenode[1521]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.conscrypt.provider.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ local property_name=dataproc.conscrypt.provider.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: hadoop-mapreduce-historyserver.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-mapreduce-historyserver[1509]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-mapreduce-historyserver
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 1.'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 1.'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 1.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + setup_service hadoop-hdfs-secondarynamenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + export KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + export -f login_through_keytab_if_necessary
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + export MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + local service=hadoop-hdfs-secondarynamenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + enable_service hadoop-hdfs-secondarynamenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + local service=hadoop-hdfs-secondarynamenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + local unit=hadoop-hdfs-secondarynamenode.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + run_with_retries systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + local retry_backoff
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + cmd=("$@")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + local -a cmd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + loginfo 'About to run '\''systemctl enable hadoop-hdfs-secondarynamenode.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + echo 'About to run '\''systemctl enable hadoop-hdfs-secondarynamenode.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: About to run 'systemctl enable hadoop-hdfs-secondarynamenode.service' with retries...
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + local update_succeeded=0
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + (( i = 0 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + (( i < 12 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: spark-history-server.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-spark-history-server[1510]: Executing: /lib/systemd/systemd-sysv-install enable spark-history-server
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: Created symlink /etc/systemd/system/mysql.service → /lib/systemd/system/mariadb.service.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: Created symlink /etc/systemd/system/mysqld.service → /lib/systemd/system/mariadb.service.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: Created symlink /etc/systemd/system/multi-user.target.wants/mariadb.service → /lib/systemd/system/mariadb.service.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ grep '^dataproc.conscrypt.provider.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ local property_value=true
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ echo true
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ local property_value=true
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ echo true
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + CONSCRYPT_ENABLED=true
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ true == \t\r\u\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt.jar /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext/libconscrypt.jar
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: + systemctl enable hadoop-hdfs-secondarynamenode.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: hadoop-hdfs-secondarynamenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-hadoop-hdfs-secondarynamenode[1521]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-secondarynamenode
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni.so /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/libconscrypt_openjdk_jni.so
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + cp /usr/local/share/google/dataproc/java.security.conscrypt /etc/java-8-openjdk/security/java.security
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ get_dataproc_property dataproc.logging.stackdriver.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ local property_name=dataproc.logging.stackdriver.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ local property_name=dataproc.logging.stackdriver.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ cut -d = -f 2-
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ grep '^dataproc.logging.stackdriver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ tail -n 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ local property_value=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ echo ''
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ local property_value=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ echo ''
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + STACKDRIVER_LOGGING_ENABLED=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ '' == \f\a\l\s\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + loginfo 'Stackdriver enabled; enabling google-fluentd.'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Stackdriver enabled; enabling google-fluentd.'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Stackdriver enabled; enabling google-fluentd.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + source /usr/local/share/google/dataproc/bdutil/configure_fluentd.sh
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ set -e
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ set -u
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ loginfo 'Running configure_fluentd.sh'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ echo 'Running configure_fluentd.sh'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Running configure_fluentd.sh
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ FLUENTD_BASE_DIR=/etc/google-fluentd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ FLUENTD_CONF_DIR=/etc/google-fluentd/config.d
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ FLUENTD_PLUGIN_DIR=/etc/google-fluentd/plugin
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ cp /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/monitoring.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/out_google_cloud.rb /etc/google-fluentd/plugin
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + update_succeeded=1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + break
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: + ((  1  ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-mariadb[1512]: ++ systemctl show mariadb.service -p Restart,RemainAfterExit
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ get_dataproc_property dataproc.logging.stackdriver.job.driver.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ local property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.driver.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ local property_name=dataproc.logging.stackdriver.job.driver.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++++ cut -d = -f 2-
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++++ grep '^dataproc.logging.stackdriver.job.driver.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++++ tail -n 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ local property_value=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ echo ''
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ local property_value=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ echo ''
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ JOB_DRIVER_LOGGING_ENABLED=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ [[ '' == \t\r\u\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ get_dataproc_property dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ local property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ get_java_property /etc/google-dataproc/dataproc.properties dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ local property_name=dataproc.logging.stackdriver.job.yarn.container.enable
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++++ cut -d = -f 2-
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++++ tail -n 1
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++++ grep '^dataproc.logging.stackdriver.job.yarn.container.enable=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ local property_value=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++++ echo ''
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ local property_value=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: +++ echo ''
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ CONTAINER_LOGGING_ENABLED=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ [[ '' == \t\r\u\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_in_background --tag setup-google-fluentd setup_service google-fluentd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + PID=1623
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=("${PID}" "${BACKGROUND_PROCESSES[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Started background process [setup_service google-fluentd] as pid 1623'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Started background process [setup_service google-fluentd] as pid 1623
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + wait_on_async_processes
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + loginfo 'Waiting on async proccesses'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Waiting on async proccesses'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Waiting on async proccesses
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + (( i = 0 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + pid=1623
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + cmd='setup_service google-fluentd'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1623 cmd=[setup_service google-fluentd]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + echo 'Waiting on pid=1623 cmd=[setup_service google-fluentd]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: Waiting on pid=1623 cmd=[setup_service google-fluentd]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + wait 1623
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + run_with_logger --tag setup-google-fluentd setup_service google-fluentd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local tag=
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + local pid=1623
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + [[ --tag == \-\-\t\a\g ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + tag=setup-google-fluentd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + shift 2
<13>Oct 28 02:17:58 google-dataproc-startup[843]: + exec
<13>Oct 28 02:17:58 google-dataproc-startup[843]: ++ logger -s -t 'setup-google-fluentd[1623]'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + setup_service google-fluentd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + export KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + KERBEROS_ENABLED=false
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + export KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + KEYTAB_DIR=/etc/security/keytab
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + export -f login_through_keytab_if_necessary
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + export MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + MY_FULL_HOSTNAME=cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + local service=google-fluentd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + [[ google-fluentd == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + enable_service google-fluentd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + local service=google-fluentd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + local unit=google-fluentd.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + run_with_retries systemctl enable google-fluentd.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + local retry_backoff
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + cmd=("$@")
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + local -a cmd
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + loginfo 'About to run '\''systemctl enable google-fluentd.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + echo 'About to run '\''systemctl enable google-fluentd.service'\'' with retries...'
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: About to run 'systemctl enable google-fluentd.service' with retries...
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + local update_succeeded=0
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + (( i = 0 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + (( i < 12 ))
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: + systemctl enable google-fluentd.service
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: google-fluentd.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 02:17:58 google-dataproc-startup[843]: <13>Oct 28 02:17:58 setup-google-fluentd[1623]: Executing: /lib/systemd/systemd-sysv-install enable google-fluentd
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 3306
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 2.'
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 2.'
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 2.
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + local 'props=Restart=on-abort
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: RemainAfterExit=no'
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + [[ Restart=on-abort
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + in_array mariadb DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + local value=mariadb
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  mariadb  ]]
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + return 1
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + [[ mariadb == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + run_with_retries systemctl start mariadb
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + local retry_backoff
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + cmd=("$@")
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + local -a cmd
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + loginfo 'About to run '\''systemctl start mariadb'\'' with retries...'
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + echo 'About to run '\''systemctl start mariadb'\'' with retries...'
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: About to run 'systemctl start mariadb' with retries...
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + local update_succeeded=0
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + (( i = 0 ))
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + (( i < 12 ))
<13>Oct 28 02:17:59 google-dataproc-startup[843]: <13>Oct 28 02:17:59 setup-mariadb[1512]: + systemctl start mariadb
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 3306
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 3.'
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 3.'
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 3.
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hadoop-yarn-timelineserver[1511]: + update_succeeded=1
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hadoop-yarn-timelineserver[1511]: + break
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hadoop-yarn-timelineserver[1511]: + ((  1  ))
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hadoop-yarn-timelineserver[1511]: ++ systemctl show hadoop-yarn-timelineserver.service -p Restart,RemainAfterExit
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 uninstall[1443]: Reading package lists...
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hive-server2[1508]: + update_succeeded=1
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hive-server2[1508]: + break
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hive-server2[1508]: + ((  1  ))
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hive-server2[1508]: ++ systemctl show hive-server2.service -p Restart,RemainAfterExit
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hadoop-hdfs-secondarynamenode[1521]: + update_succeeded=1
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hadoop-hdfs-secondarynamenode[1521]: + break
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hadoop-hdfs-secondarynamenode[1521]: + ((  1  ))
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 setup-hadoop-hdfs-secondarynamenode[1521]: ++ systemctl show hadoop-hdfs-secondarynamenode.service -p Restart,RemainAfterExit
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 uninstall[1443]: Building dependency tree...
<13>Oct 28 02:18:00 google-dataproc-startup[843]: <13>Oct 28 02:18:00 uninstall[1443]: Reading state information...
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + update_succeeded=1
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + break
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + ((  1  ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: ++ systemctl show spark-history-server.service -p Restart,RemainAfterExit
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + update_succeeded=1
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + break
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + ((  1  ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: ++ systemctl show hadoop-yarn-resourcemanager.service -p Restart,RemainAfterExit
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 3306
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 4.'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 4.'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 4.
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]: The following packages will be REMOVED:
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   bind9-host* druid* fonts-font-awesome* fonts-mathjax* geoip-database*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   hadoop-hdfs-datanode* hadoop-hdfs-journalnode* hadoop-hdfs-zkfc*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   hadoop-yarn-nodemanager* hive-webhcat* hive-webhcat-server*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   javascript-common* kafka* kafka-server* knox* krb5-admin-server*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   krb5-config* krb5-kdc* krb5-kpropd* krb5-user* libbind9-140* libc-ares2*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   libdns162* libev4* libfile-copy-recursive-perl* libgeoip1* libgssrpc4*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   libhttp-parser2.8* libisc160* libisccc140* libisccfg140* libjs-bootstrap*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   libjs-d3* libjs-es5-shim* libjs-highlight.js* libjs-jquery*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   libjs-jquery-datatables* libjs-jquery-metadata* libjs-jquery-selectize.js*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   libjs-jquery-tablesorter* libjs-jquery-ui* libjs-json* libjs-mathjax*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   libjs-microplugin.js* libjs-modernizr* libjs-prettify* libjs-sifter.js*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   libjs-twitter-bootstrap* libjs-twitter-bootstrap-datepicker*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   libkadm5clnt-mit11* libkadm5srv-mit11* libkdb5-8* liblua5.1-0*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   libluajit-5.1-2* libluajit-5.1-common* liblwres141* libuv1* libverto-libev1*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   libverto1* libyaml-0-2* littler* node-highlight.js* node-normalize.css*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   nodejs* nodejs-doc* pandoc* pandoc-data* r-cran-assertthat*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-base64enc* r-cran-bindr* r-cran-bindrcpp* r-cran-bit* r-cran-bit64*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-cli* r-cran-colorspace* r-cran-crayon* r-cran-data.table* r-cran-dbi*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-digest* r-cran-dplyr* r-cran-evaluate* r-cran-fansi* r-cran-filehash*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-ggplot2* r-cran-glue* r-cran-googlevis* r-cran-gtable* r-cran-hexbin*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-highr* r-cran-hms* r-cran-htmltools* r-cran-htmlwidgets*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-httpuv* r-cran-jsonlite* r-cran-knitr* r-cran-labeling* r-cran-later*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-lazyeval* r-cran-littler* r-cran-magrittr* r-cran-mapproj*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-maps* r-cran-markdown* r-cran-memoise* r-cran-mime* r-cran-munsell*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-pillar* r-cran-pkgconfig* r-cran-pkgkitten* r-cran-plyr* r-cran-png*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-praise* r-cran-promises* r-cran-purrr* r-cran-r6*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-rcolorbrewer* r-cran-rcpp* r-cran-reshape2* r-cran-rlang*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-rmarkdown* r-cran-rsqlite* r-cran-scales* r-cran-shiny*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-sourcetools* r-cran-sp* r-cran-stringi* r-cran-stringr*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-testit* r-cran-testthat* r-cran-tibble* r-cran-tidyselect*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-tikzdevice* r-cran-tinytex* r-cran-utf8* r-cran-viridislite*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   r-cran-withr* r-cran-xfun* r-cran-xml2* r-cran-xtable* r-cran-yaml* solr*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 uninstall[1443]:   solr-server* update-inetd* xinetd* zeppelin* zookeeper-server*
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + update_succeeded=1
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + break
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + ((  1  ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: ++ systemctl show hadoop-mapreduce-historyserver.service -p Restart,RemainAfterExit
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + update_succeeded=1
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + break
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + ((  1  ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + local 'props=Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: RemainAfterExit=no'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + local drop_in_dir=/etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + mkdir /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: ++ systemctl show google-fluentd.service -p Restart,RemainAfterExit
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-timelineserver.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + local 'props=Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: RemainAfterExit=no'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + local drop_in_dir=/etc/systemd/system/hive-server2.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + mkdir /etc/systemd/system/hive-server2.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + in_array hadoop-yarn-timelineserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + local value=hadoop-yarn-timelineserver
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-yarn-timelineserver  ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + return 1
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + run_with_retries systemctl start hadoop-yarn-timelineserver
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + local retry_backoff
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + cmd=("$@")
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + local -a cmd
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + loginfo 'About to run '\''systemctl start hadoop-yarn-timelineserver'\'' with retries...'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + echo 'About to run '\''systemctl start hadoop-yarn-timelineserver'\'' with retries...'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: About to run 'systemctl start hadoop-yarn-timelineserver' with retries...
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + local update_succeeded=0
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + (( i = 0 ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + (( i < 12 ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: + systemctl start hadoop-yarn-timelineserver
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-server2.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + local 'props=Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: RemainAfterExit=no'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + local drop_in_dir=/etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + mkdir /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + in_array hive-server2 DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + local value=hive-server2
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hive-server2  ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + return 1
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + [[ hive-server2 == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hive-server2[1508]: + return
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-secondarynamenode.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + local 'props=Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: RemainAfterExit=no'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + local drop_in_dir=/etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + mkdir /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + in_array hadoop-hdfs-secondarynamenode DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + local value=hadoop-hdfs-secondarynamenode
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-hdfs-secondarynamenode  ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + return 1
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + run_with_retries systemctl start hadoop-hdfs-secondarynamenode
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + local retry_backoff
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + cmd=("$@")
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + local -a cmd
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + loginfo 'About to run '\''systemctl start hadoop-hdfs-secondarynamenode'\'' with retries...'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + echo 'About to run '\''systemctl start hadoop-hdfs-secondarynamenode'\'' with retries...'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: About to run 'systemctl start hadoop-hdfs-secondarynamenode' with retries...
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + local update_succeeded=0
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + (( i = 0 ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + (( i < 12 ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: + systemctl start hadoop-hdfs-secondarynamenode
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + local 'props=Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: RemainAfterExit=no'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + local drop_in_dir=/etc/systemd/system/spark-history-server.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + mkdir /etc/systemd/system/spark-history-server.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + local 'props=Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: RemainAfterExit=no'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + local drop_in_dir=/etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + mkdir /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + local 'props=Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: RemainAfterExit=yes'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + [[ Restart=no
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + in_array google-fluentd DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + local value=google-fluentd
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  google-fluentd  ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + return 1
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + [[ google-fluentd == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + run_with_retries systemctl start google-fluentd
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + local retry_backoff
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + cmd=("$@")
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + local -a cmd
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + loginfo 'About to run '\''systemctl start google-fluentd'\'' with retries...'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + echo 'About to run '\''systemctl start google-fluentd'\'' with retries...'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: About to run 'systemctl start google-fluentd' with retries...
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + local update_succeeded=0
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + (( i = 0 ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + (( i < 12 ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-google-fluentd[1623]: + systemctl start google-fluentd
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-yarn-resourcemanager.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-mapreduce-historyserver.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/spark-history-server.service.d
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + in_array hadoop-yarn-resourcemanager DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + local value=hadoop-yarn-resourcemanager
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-yarn-resourcemanager  ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + return 1
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + run_with_retries systemctl start hadoop-yarn-resourcemanager
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + local retry_backoff
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + cmd=("$@")
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + local -a cmd
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + loginfo 'About to run '\''systemctl start hadoop-yarn-resourcemanager'\'' with retries...'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + echo 'About to run '\''systemctl start hadoop-yarn-resourcemanager'\'' with retries...'
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: About to run 'systemctl start hadoop-yarn-resourcemanager' with retries...
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + local update_succeeded=0
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + (( i = 0 ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + (( i < 12 ))
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: + systemctl start hadoop-yarn-resourcemanager
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + in_array spark-history-server DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + local value=spark-history-server
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  spark-history-server  ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-spark-history-server[1510]: + return
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + in_array hadoop-mapreduce-historyserver DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + local value=hadoop-mapreduce-historyserver
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-mapreduce-historyserver  ]]
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-mapreduce-historyserver[1509]: + return
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-timelineserver[1511]: Warning: hadoop-yarn-timelineserver.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-hdfs-secondarynamenode[1521]: Warning: hadoop-hdfs-secondarynamenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 28 02:18:01 google-dataproc-startup[843]: <13>Oct 28 02:18:01 setup-hadoop-yarn-resourcemanager[1506]: Warning: hadoop-yarn-resourcemanager.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 28 02:18:02 google-dataproc-startup[843]: <13>Oct 28 02:18:02 uninstall[1443]: 0 upgraded, 0 newly installed, 146 to remove and 1 not upgraded.
<13>Oct 28 02:18:02 google-dataproc-startup[843]: <13>Oct 28 02:18:02 uninstall[1443]: After this operation, 2,006 MB disk space will be freed.
<13>Oct 28 02:18:02 google-dataproc-startup[843]: <13>Oct 28 02:18:02 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:02 google-dataproc-startup[843]: <13>Oct 28 02:18:02 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 3306
<13>Oct 28 02:18:02 google-dataproc-startup[843]: <13>Oct 28 02:18:02 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 02:18:02 google-dataproc-startup[843]: <13>Oct 28 02:18:02 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 5.'
<13>Oct 28 02:18:02 google-dataproc-startup[843]: <13>Oct 28 02:18:02 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 5.'
<13>Oct 28 02:18:02 google-dataproc-startup[843]: <13>Oct 28 02:18:02 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 5.
<13>Oct 28 02:18:02 google-dataproc-startup[843]: <13>Oct 28 02:18:02 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:03 google-dataproc-startup[843]: <13>Oct 28 02:18:03 uninstall[1443]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 119619 files and directories currently installed.)
<13>Oct 28 02:18:03 google-dataproc-startup[843]: <13>Oct 28 02:18:03 uninstall[1443]: Removing krb5-admin-server (1.15-1+deb9u1) ...
<13>Oct 28 02:18:03 google-dataproc-startup[843]: <13>Oct 28 02:18:03 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:03 google-dataproc-startup[843]: <13>Oct 28 02:18:03 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 3306
<13>Oct 28 02:18:03 google-dataproc-startup[843]: <13>Oct 28 02:18:03 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 3306 (tcp) failed: Connection refused
<13>Oct 28 02:18:03 google-dataproc-startup[843]: <13>Oct 28 02:18:03 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 6.'
<13>Oct 28 02:18:03 google-dataproc-startup[843]: <13>Oct 28 02:18:03 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 6.'
<13>Oct 28 02:18:03 google-dataproc-startup[843]: <13>Oct 28 02:18:03 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 3306 failed. Retry attempt: 6.
<13>Oct 28 02:18:03 google-dataproc-startup[843]: <13>Oct 28 02:18:03 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 3306
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: Connection to cluster-89fa-m 3306 port [tcp/mysql] succeeded!
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + update_succeeded=1
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 3306 succeeded.'
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 3306 succeeded.'
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 3306 succeeded.
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + break
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + ((  1  ))
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + loginfo 'Service up on host=cluster-89fa-m port=3306.'
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + echo 'Service up on host=cluster-89fa-m port=3306.'
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: Service up on host=cluster-89fa-m port=3306.
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + enable_service hive-metastore
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + local service=hive-metastore
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + local unit=hive-metastore.service
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + run_with_retries systemctl enable hive-metastore.service
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + local retry_backoff
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + cmd=("$@")
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + local -a cmd
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + loginfo 'About to run '\''systemctl enable hive-metastore.service'\'' with retries...'
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + echo 'About to run '\''systemctl enable hive-metastore.service'\'' with retries...'
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: About to run 'systemctl enable hive-metastore.service' with retries...
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + local update_succeeded=0
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + (( i = 0 ))
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + (( i < 12 ))
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: + systemctl enable hive-metastore.service
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: hive-metastore.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-hive-metastore[1507]: Executing: /lib/systemd/systemd-sysv-install enable hive-metastore
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: + update_succeeded=1
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: + break
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: + ((  1  ))
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: + [[ mariadb == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: + [[ mariadb == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: ++ get_dataproc_property am.primary_only
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: ++ local property_name=am.primary_only
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: +++ local property_name=am.primary_only
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: ++++ cut -d = -f 2-
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: ++++ tail -n 1
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: +++ local property_value=false
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: +++ echo false
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: ++ local property_value=false
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: ++ echo false
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 02:18:04 google-dataproc-startup[843]: <13>Oct 28 02:18:04 setup-mariadb[1512]: + [[ mariadb == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:06 INFO namenode.NameNode: STARTUP_MSG: 
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: /************************************************************
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: STARTUP_MSG: Starting NameNode
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: STARTUP_MSG:   host = cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal/10.128.0.32
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: STARTUP_MSG:   args = [-format, -nonInteractive]
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: STARTUP_MSG:   version = 2.9.2
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/hadoop-lzo-0.4.20.jar:/usr/lib/hadoop/lib/httpclient-4.5.2.jar:/usr/lib/hadoop/lib/gcs-connector-hadoop2-1.9.17.jar:/usr/lib/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/gcs-connector.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.1
<13>Oct 28 02:18:06 google-dataproc-startup[843]: 3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jetty-6.1.26.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/xz-1.0.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0
<13>Oct 28 02:18:06 google-dataproc-startup[843]: .4.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/u
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: sr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/httpcore-4.4.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/li
<13>Oct 28 02:18:06 google-dataproc-startup[843]: b/jetty-sslengine-6.1.26.jar:/usr/lib/hadoop/lib/m
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: ockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.9.2.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-common-2.9.2-tests.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-common-2.9.2.jar:/usr/lib/hadoop/.//hadoop-nfs-2.9.2.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-auth-2.9.2.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-core-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.7.8.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib
<13>Oct 28 02:18:06 google-dataproc-startup[843]: /hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoo
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: p-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.3.04.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2
<13>Oct 28 02:18:06 google-dataproc-startup[843]: .7.8.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.9.1
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: .jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.9.2.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.9.2-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.9.2-tests.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/l
<13>Oct 28 02:18:06 google-dataproc-startup[843]: ib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: /hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.7.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/api-as
<13>Oct 28 02:18:06 google-dataproc-startup[843]: n1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/comm
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: ons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/xz-1.0.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hado
<13>Oct 28 02:18:06 google-dataproc-startup[843]: op-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/ha
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: doop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/ht
<13>Oct 28 02:18:06 google-dataproc-startup[843]: tpcore-4.4.4.jar:/usr/lib/hadoop-yarn/lib/jetty-ut
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: il-6.1.26.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.13.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.9.2.jar:/usr
<13>Oct 28 02:18:06 google-dataproc-startup[843]: /lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/li
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: b/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-y
<13>Oct 28 02:18:06 google-dataproc-startup[843]: arn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-ya
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: rn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.9.2.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.9.2.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/lib/jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce
<13>Oct 28 02:18:06 google-dataproc-startup[843]: /lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapredu
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: ce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/xz-1.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.0.5.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//l
<13>Oct 28 02:18:06 google-dataproc-startup[843]: eveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/./
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: /jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-core-1.8.0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//netty-3.6.2.Final.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.7.0.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ar
<13>Oct 28 02:18:06 google-dataproc-startup[843]: chive-logs.jar:/usr/lib/hadoop-mapreduce/.//log4j-
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: 1.2.17.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//h
<13>Oct 28 02:18:06 google-dataproc-startup[843]: adoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hado
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: op-archives-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.199.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2
<13>Oct 28 02:18:06 google-dataproc-startup[843]: .0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//mssql-jd
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: bc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//xz-1.0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client
<13>Oct 28 02:18:06 google-dataproc-startup[843]: -hs-plugins-2.9.2.jar:/usr/lib/hadoop-mapreduce/./
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: /hadoop-yarn-server-applicationhistoryservice-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.9.2-tests.jar:/usr
<13>Oct 28 02:18:06 google-dataproc-startup[843]: /lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: /lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.4.1.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.9.2.jar:/usr/lib/hadoop-ma
<13>Oct 28 02:18:06 google-dataproc-startup[843]: preduce/.//json-20170516.jar:/usr/lib/hadoop-mapre
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: duce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.0.5.j
<13>Oct 28 02:18:06 google-dataproc-startup[843]: ar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: .jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.7.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sd
<13>Oct 28 02:18:06 google-dataproc-startup[843]: k-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ar
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: chive-logs-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.13.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.9.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: STARTUP_MSG:   build = https://bigdataoss-internal.googlesource.com/third_party/apache/hadoop -r 849ee9eda72c7e8b1eb9fc5a830432c887914111; compiled by 'bigtop' on 2019-09-18T10:58Z
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: STARTUP_MSG:   java = 1.8.0_222
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: ************************************************************/
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:06 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
<13>Oct 28 02:18:06 google-dataproc-startup[843]: <13>Oct 28 02:18:06 uninstall[1443]: Removing krb5-kpropd (1.15-1+deb9u1) ...
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:07 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + update_succeeded=1
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + break
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + ((  1  ))
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: ++ systemctl show hive-metastore.service -p Restart,RemainAfterExit
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + local 'props=Restart=no
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: RemainAfterExit=no'
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + [[ Restart=no
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + [[ Restart=no
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + local drop_in_dir=/etc/systemd/system/hive-metastore.service.d
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + mkdir /etc/systemd/system/hive-metastore.service.d
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-metastore.service.d
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + in_array hive-metastore DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + local value=hive-metastore
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hive-metastore  ]]
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + return 1
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + [[ hive-metastore == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + run_with_retries systemctl start hive-metastore
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + local retry_backoff
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + cmd=("$@")
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + local -a cmd
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + loginfo 'About to run '\''systemctl start hive-metastore'\'' with retries...'
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + echo 'About to run '\''systemctl start hive-metastore'\'' with retries...'
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: About to run 'systemctl start hive-metastore' with retries...
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + local update_succeeded=0
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + (( i = 0 ))
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + (( i < 12 ))
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: + systemctl start hive-metastore
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hive-metastore[1507]: Warning: hive-metastore.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 28 02:18:07 google-dataproc-startup[843]: <13>Oct 28 02:18:07 setup-hadoop-hdfs-namenode[1505]: 2019-10-28T02:18:07.781+0000: 5.359: [GC (Allocation Failure) 2019-10-28T02:18:07.781+0000: 5.359: [ParNew: 32320K->3968K(36288K), 0.0346473 secs] 32320K->5327K(116864K), 0.0347441 secs] [Times: user=0.01 sys=0.01, real=0.04 secs] 
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: + update_succeeded=1
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: + break
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: + ((  1  ))
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: + [[ hadoop-yarn-resourcemanager == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: ++ get_dataproc_property am.primary_only
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: ++ local property_name=am.primary_only
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: +++ local property_name=am.primary_only
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: ++++ cut -d = -f 2-
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: ++++ tail -n 1
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: +++ local property_value=false
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: +++ echo false
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: ++ local property_value=false
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: ++ echo false
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: + [[ hadoop-yarn-resourcemanager == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: + [[ 0 -eq 0 ]]
<13>Oct 28 02:18:08 google-dataproc-startup[843]: <13>Oct 28 02:18:08 setup-hadoop-yarn-resourcemanager[1506]: + [[ false == \t\r\u\e ]]
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: + update_succeeded=1
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: + break
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: + ((  1  ))
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: + [[ hadoop-yarn-timelineserver == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: ++ get_dataproc_property am.primary_only
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: ++ local property_name=am.primary_only
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: +++ local property_name=am.primary_only
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: ++++ cut -d = -f 2-
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: ++++ tail -n 1
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: +++ local property_value=false
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: +++ echo false
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: ++ local property_value=false
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: ++ echo false
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-yarn-timelineserver[1511]: + [[ hadoop-yarn-timelineserver == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:09 WARN common.Util: Path /hadoop/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
<13>Oct 28 02:18:09 google-dataproc-startup[843]: <13>Oct 28 02:18:09 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:09 WARN common.Util: Path /hadoop/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
<13>Oct 28 02:18:10 google-dataproc-startup[843]: <13>Oct 28 02:18:10 setup-hadoop-hdfs-namenode[1505]: Formatting using clusterid: CID-ae0ca626-e543-43e1-a5e4-a4ef1882212e
<13>Oct 28 02:18:10 google-dataproc-startup[843]: <13>Oct 28 02:18:10 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:10 INFO namenode.FSEditLog: Edit logging is async:true
<13>Oct 28 02:18:10 google-dataproc-startup[843]: <13>Oct 28 02:18:10 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:10 INFO namenode.FSNamesystem: KeyProvider: null
<13>Oct 28 02:18:10 google-dataproc-startup[843]: <13>Oct 28 02:18:10 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:10 INFO namenode.FSNamesystem: fsLock is fair: true
<13>Oct 28 02:18:10 google-dataproc-startup[843]: <13>Oct 28 02:18:10 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:10 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
<13>Oct 28 02:18:10 google-dataproc-startup[843]: <13>Oct 28 02:18:10 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:10 INFO namenode.FSNamesystem: fsOwner             = hdfs (auth:SIMPLE)
<13>Oct 28 02:18:10 google-dataproc-startup[843]: <13>Oct 28 02:18:10 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:10 INFO namenode.FSNamesystem: supergroup          = hadoop
<13>Oct 28 02:18:10 google-dataproc-startup[843]: <13>Oct 28 02:18:10 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:10 INFO namenode.FSNamesystem: isPermissionEnabled = false
<13>Oct 28 02:18:10 google-dataproc-startup[843]: <13>Oct 28 02:18:10 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:10 INFO namenode.FSNamesystem: HA Enabled: false
<13>Oct 28 02:18:10 google-dataproc-startup[843]: <13>Oct 28 02:18:10 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:10 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
<13>Oct 28 02:18:10 google-dataproc-startup[843]: <13>Oct 28 02:18:10 setup-hadoop-hdfs-namenode[1505]: 2019-10-28T02:18:10.831+0000: 8.409: [GC (Allocation Failure) 2019-10-28T02:18:10.831+0000: 8.409: [ParNew: 36288K->3968K(36288K), 0.1516547 secs] 37647K->8275K(116864K), 0.1517320 secs] [Times: user=0.03 sys=0.01, real=0.15 secs] 
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + update_succeeded=1
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + break
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + ((  1  ))
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + [[ hive-metastore == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + wait_for_port cluster-89fa-m 9083
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + local -r host=cluster-89fa-m
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + local -r port=9083
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + loginfo 'Waiting for service to come up on host=cluster-89fa-m port=9083.'
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + echo 'Waiting for service to come up on host=cluster-89fa-m port=9083.'
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: Waiting for service to come up on host=cluster-89fa-m port=9083.
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + retry_with_constant_backoff nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + local max_retry=300
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + cmd=("$@")
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + local -a cmd
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + local update_succeeded=0
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: ++ seq 1 300
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 1.'
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 1.'
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 1.
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:11 google-dataproc-startup[843]: <13>Oct 28 02:18:11 uninstall[1443]: Removing krb5-kdc (1.15-1+deb9u1) ...
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 2.'
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 2.'
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 2.
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: + update_succeeded=1
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: + break
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: + ((  1  ))
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: + [[ hadoop-hdfs-secondarynamenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: ++ get_dataproc_property am.primary_only
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: ++ local property_name=am.primary_only
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: +++ local property_name=am.primary_only
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: ++++ cut -d = -f 2-
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: ++++ tail -n 1
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: +++ local property_value=false
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: +++ echo false
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: ++ local property_value=false
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: ++ echo false
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 02:18:12 google-dataproc-startup[843]: <13>Oct 28 02:18:12 setup-hadoop-hdfs-secondarynamenode[1521]: + [[ hadoop-hdfs-secondarynamenode == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO util.HostsFileReader: Adding a node "cluster-89fa-w-0.us-central1-c.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO util.HostsFileReader: Adding a node "cluster-89fa-w-1.us-central1-c.c.lustrous-drake-255300.internal" to the list of included hosts from /etc/hadoop/conf/nodes_include
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 3.'
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 3.'
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 3.
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.retry-hostname-dns-lookup=true
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Oct 28 02:18:13
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO util.GSet: Computing capacity for map BlocksMap
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO util.GSet: VM type       = 64-bit
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO util.GSet: 2.0% max memory 1.4 GB = 29.6 MB
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO util.GSet: capacity      = 2^22 = 4194304 entries
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 2019-10-28T02:18:13.736+0000: 11.314: [GC (Allocation Failure) 2019-10-28T02:18:13.736+0000: 11.314: [ParNew: 35324K->3968K(36288K), 0.0330152 secs] 39631K->9698K(116864K), 0.0330907 secs] [Times: user=0.01 sys=0.00, real=0.03 secs] 
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManager: defaultReplication         = 2
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManager: maxReplication             = 512
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManager: minReplication             = 1
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
<13>Oct 28 02:18:13 google-dataproc-startup[843]: <13>Oct 28 02:18:13 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:13 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO namenode.FSNamesystem: Append Enabled: true
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: Computing capacity for map INodeMap
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: VM type       = 64-bit
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: 1.0% max memory 1.4 GB = 14.8 MB
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: capacity      = 2^21 = 2097152 entries
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO namenode.FSDirectory: ACLs enabled? false
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO namenode.FSDirectory: XAttrs enabled? true
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO namenode.NameNode: Caching file names occurring more than 10 times
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 4.'
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 4.'
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 4.
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 2019-10-28T02:18:14.453+0000: 12.031: [GC (Allocation Failure) 2019-10-28T02:18:14.453+0000: 12.031: [ParNew: 36288K->1253K(36288K), 0.2761407 secs] 42018K->32124K(116864K), 0.2762054 secs] [Times: user=0.09 sys=0.03, real=0.27 secs] 
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: Computing capacity for map cachedBlocks
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: VM type       = 64-bit
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: 0.25% max memory 1.4 GB = 3.7 MB
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: capacity      = 2^19 = 524288 entries
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: Computing capacity for map NameNodeRetryCache
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: VM type       = 64-bit
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: 0.029999999329447746% max memory 1.4 GB = 454.5 KB
<13>Oct 28 02:18:14 google-dataproc-startup[843]: <13>Oct 28 02:18:14 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:14 INFO util.GSet: capacity      = 2^16 = 65536 entries
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:15 INFO namenode.FSImage: Allocated new BlockPoolId: BP-306136595-10.128.0.32-1572229095011
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:15 INFO common.Storage: Storage directory /hadoop/dfs/name has been successfully formatted.
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:15 INFO namenode.FSImageFormatProtobuf: Saving image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 uninstall[1443]: Removing krb5-user (1.15-1+deb9u1) ...
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 uninstall[1443]: Removing krb5-config (2.6) ...
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 5.'
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 5.'
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 5.
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 uninstall[1443]: Removing bind9-host (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:15 INFO namenode.FSImageFormatProtobuf: Image file /hadoop/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 318 bytes saved in 0 seconds .
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 uninstall[1443]: Removing druid (0.13.0-incubating-1) ...
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:15 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hadoop-hdfs-namenode[1505]: 19/10/28 02:18:15 INFO namenode.NameNode: SHUTDOWN_MSG: 
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hadoop-hdfs-namenode[1505]: /************************************************************
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hadoop-hdfs-namenode[1505]: SHUTDOWN_MSG: Shutting down NameNode at cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal/10.128.0.32
<13>Oct 28 02:18:15 google-dataproc-startup[843]: <13>Oct 28 02:18:15 setup-hadoop-hdfs-namenode[1505]: ************************************************************/
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 uninstall[1443]: dpkg: warning: while removing druid, directory '/usr/lib/druid/extensions/mysql-metadata-storage' not empty so not removed
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 uninstall[1443]: dpkg: warning: while removing druid, directory '/usr/lib/druid/conf/druid/_common' not empty so not removed
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: Heap
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]:  par new generation   total 36288K, used 23723K [0x00000000a2800000, 0x00000000a4f50000, 0x00000000ace60000)
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]:   eden space 32320K,  69% used [0x00000000a2800000, 0x00000000a3df1a08, 0x00000000a4790000)
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]:   from space 3968K,  31% used [0x00000000a4790000, 0x00000000a48c95c0, 0x00000000a4b70000)
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]:   to   space 3968K,   0% used [0x00000000a4b70000, 0x00000000a4b70000, 0x00000000a4f50000)
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]:  concurrent mark-sweep generation total 80576K, used 30871K [0x00000000ace60000, 0x00000000b1d10000, 0x0000000100000000)
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]:  Metaspace       used 19011K, capacity 19216K, committed 19456K, reserved 1067008K
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]:   class space    used 2135K, capacity 2192K, committed 2304K, reserved 1048576K
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 uninstall[1443]: Removing r-cran-rmarkdown (1.11+dfsg-1~bpo9+1) ...
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + update_succeeded=1
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + break
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + ((  1  ))
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\z\k\f\c ]]
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + enable_service hadoop-hdfs-namenode
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + local service=hadoop-hdfs-namenode
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + local unit=hadoop-hdfs-namenode.service
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + run_with_retries systemctl enable hadoop-hdfs-namenode.service
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + local retry_backoff
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + cmd=("$@")
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + local -a cmd
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + loginfo 'About to run '\''systemctl enable hadoop-hdfs-namenode.service'\'' with retries...'
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + echo 'About to run '\''systemctl enable hadoop-hdfs-namenode.service'\'' with retries...'
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: About to run 'systemctl enable hadoop-hdfs-namenode.service' with retries...
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + local update_succeeded=0
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + (( i = 0 ))
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + (( i < 12 ))
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: + systemctl enable hadoop-hdfs-namenode.service
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: hadoop-hdfs-namenode.service is not a native service, redirecting to systemd-sysv-install.
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hadoop-hdfs-namenode[1505]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-namenode
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 uninstall[1443]: Removing r-cran-shiny (1.2.0+dfsg-1~bpo9+1) ...
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 uninstall[1443]: Removing fonts-font-awesome (4.7.0~dfsg-1) ...
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 uninstall[1443]: Removing r-cran-knitr (1.21+dfsg-2~bpo9+1) ...
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 6.'
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 6.'
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 6.
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 uninstall[1443]: Removing r-cran-markdown (0.9+dfsg-1~bpo9+1) ...
<13>Oct 28 02:18:16 google-dataproc-startup[843]: <13>Oct 28 02:18:16 uninstall[1443]: Removing libjs-mathjax (2.7.0-2) ...
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + update_succeeded=1
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + break
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + ((  1  ))
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: ++ systemctl show hadoop-hdfs-namenode.service -p Restart,RemainAfterExit
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + local 'props=Restart=no
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: RemainAfterExit=no'
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + [[ Restart=no
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + [[ Restart=no
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + local drop_in_dir=/etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + mkdir /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + ln -s /etc/systemd/system/common/restart.conf /etc/systemd/system/hadoop-hdfs-namenode.service.d
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + in_array hadoop-hdfs-namenode DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + local value=hadoop-hdfs-namenode
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + local -n values=DATAPROC_START_AFTER_HDFS_SERVICES
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + [[ !  hadoop-mapreduce-historyserver spark-history-server  =~  hadoop-hdfs-namenode  ]]
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + return 1
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\s\e\r\v\e\r\2 ]]
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + run_with_retries systemctl start hadoop-hdfs-namenode
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + local retry_backoff
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + cmd=("$@")
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + local -a cmd
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + loginfo 'About to run '\''systemctl start hadoop-hdfs-namenode'\'' with retries...'
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + echo 'About to run '\''systemctl start hadoop-hdfs-namenode'\'' with retries...'
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: About to run 'systemctl start hadoop-hdfs-namenode' with retries...
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + local update_succeeded=0
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + (( i = 0 ))
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + (( i < 12 ))
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: + systemctl start hadoop-hdfs-namenode
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hadoop-hdfs-namenode[1505]: Warning: hadoop-hdfs-namenode.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 7.'
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 7.'
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 7.
<13>Oct 28 02:18:17 google-dataproc-startup[843]: <13>Oct 28 02:18:17 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:18 google-dataproc-startup[843]: <13>Oct 28 02:18:18 uninstall[1443]: Removing fonts-mathjax (2.7.0-2) ...
<13>Oct 28 02:18:18 google-dataproc-startup[843]: <13>Oct 28 02:18:18 uninstall[1443]: Removing geoip-database (20170512-1) ...
<13>Oct 28 02:18:18 google-dataproc-startup[843]: <13>Oct 28 02:18:18 uninstall[1443]: Removing hadoop-hdfs-datanode (2.9.2-1) ...
<13>Oct 28 02:18:18 google-dataproc-startup[843]: <13>Oct 28 02:18:18 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:18 google-dataproc-startup[843]: <13>Oct 28 02:18:18 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:18 google-dataproc-startup[843]: <13>Oct 28 02:18:18 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:18 google-dataproc-startup[843]: <13>Oct 28 02:18:18 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 8.'
<13>Oct 28 02:18:18 google-dataproc-startup[843]: <13>Oct 28 02:18:18 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 8.'
<13>Oct 28 02:18:18 google-dataproc-startup[843]: <13>Oct 28 02:18:18 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 8.
<13>Oct 28 02:18:18 google-dataproc-startup[843]: <13>Oct 28 02:18:18 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:19 google-dataproc-startup[843]: <13>Oct 28 02:18:19 uninstall[1443]: Removing hadoop-hdfs-journalnode (2.9.2-1) ...
<13>Oct 28 02:18:19 google-dataproc-startup[843]: <13>Oct 28 02:18:19 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:19 google-dataproc-startup[843]: <13>Oct 28 02:18:19 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:19 google-dataproc-startup[843]: <13>Oct 28 02:18:19 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:19 google-dataproc-startup[843]: <13>Oct 28 02:18:19 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 9.'
<13>Oct 28 02:18:19 google-dataproc-startup[843]: <13>Oct 28 02:18:19 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 9.'
<13>Oct 28 02:18:19 google-dataproc-startup[843]: <13>Oct 28 02:18:19 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 9.
<13>Oct 28 02:18:19 google-dataproc-startup[843]: <13>Oct 28 02:18:19 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:20 google-dataproc-startup[843]: <13>Oct 28 02:18:20 uninstall[1443]: Removing hadoop-hdfs-zkfc (2.9.2-1) ...
<13>Oct 28 02:18:20 google-dataproc-startup[843]: <13>Oct 28 02:18:20 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:20 google-dataproc-startup[843]: <13>Oct 28 02:18:20 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:20 google-dataproc-startup[843]: <13>Oct 28 02:18:20 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:20 google-dataproc-startup[843]: <13>Oct 28 02:18:20 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 10.'
<13>Oct 28 02:18:20 google-dataproc-startup[843]: <13>Oct 28 02:18:20 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 10.'
<13>Oct 28 02:18:20 google-dataproc-startup[843]: <13>Oct 28 02:18:20 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 10.
<13>Oct 28 02:18:20 google-dataproc-startup[843]: <13>Oct 28 02:18:20 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:21 google-dataproc-startup[843]: <13>Oct 28 02:18:21 uninstall[1443]: Removing hadoop-yarn-nodemanager (2.9.2-1) ...
<13>Oct 28 02:18:21 google-dataproc-startup[843]: <13>Oct 28 02:18:21 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:21 google-dataproc-startup[843]: <13>Oct 28 02:18:21 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:21 google-dataproc-startup[843]: <13>Oct 28 02:18:21 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:21 google-dataproc-startup[843]: <13>Oct 28 02:18:21 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 11.'
<13>Oct 28 02:18:21 google-dataproc-startup[843]: <13>Oct 28 02:18:21 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 11.'
<13>Oct 28 02:18:21 google-dataproc-startup[843]: <13>Oct 28 02:18:21 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 11.
<13>Oct 28 02:18:21 google-dataproc-startup[843]: <13>Oct 28 02:18:21 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:22 google-dataproc-startup[843]: <13>Oct 28 02:18:22 uninstall[1443]: Removing hive-webhcat-server (2.3.5-1) ...
<13>Oct 28 02:18:22 google-dataproc-startup[843]: <13>Oct 28 02:18:22 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:22 google-dataproc-startup[843]: <13>Oct 28 02:18:22 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:22 google-dataproc-startup[843]: <13>Oct 28 02:18:22 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:22 google-dataproc-startup[843]: <13>Oct 28 02:18:22 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 12.'
<13>Oct 28 02:18:22 google-dataproc-startup[843]: <13>Oct 28 02:18:22 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 12.'
<13>Oct 28 02:18:22 google-dataproc-startup[843]: <13>Oct 28 02:18:22 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 12.
<13>Oct 28 02:18:22 google-dataproc-startup[843]: <13>Oct 28 02:18:22 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:23 google-dataproc-startup[843]: <13>Oct 28 02:18:23 uninstall[1443]: Removing hive-webhcat (2.3.5-1) ...
<13>Oct 28 02:18:23 google-dataproc-startup[843]: <13>Oct 28 02:18:23 uninstall[1443]: Removing javascript-common (11) ...
<13>Oct 28 02:18:23 google-dataproc-startup[843]: <13>Oct 28 02:18:23 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:23 google-dataproc-startup[843]: <13>Oct 28 02:18:23 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:23 google-dataproc-startup[843]: <13>Oct 28 02:18:23 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:23 google-dataproc-startup[843]: <13>Oct 28 02:18:23 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 13.'
<13>Oct 28 02:18:23 google-dataproc-startup[843]: <13>Oct 28 02:18:23 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 13.'
<13>Oct 28 02:18:23 google-dataproc-startup[843]: <13>Oct 28 02:18:23 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 13.
<13>Oct 28 02:18:23 google-dataproc-startup[843]: <13>Oct 28 02:18:23 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:24 google-dataproc-startup[843]: <13>Oct 28 02:18:24 uninstall[1443]: Removing kafka-server (1.1.1-1) ...
<13>Oct 28 02:18:24 google-dataproc-startup[843]: <13>Oct 28 02:18:24 uninstall[1443]: Removing kafka (1.1.1-1) ...
<13>Oct 28 02:18:24 google-dataproc-startup[843]: <13>Oct 28 02:18:24 uninstall[1443]: Removing knox (1.1.0-1) ...
<13>Oct 28 02:18:24 google-dataproc-startup[843]: <13>Oct 28 02:18:24 uninstall[1443]: Removing libbind9-140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 02:18:24 google-dataproc-startup[843]: <13>Oct 28 02:18:24 uninstall[1443]: Removing node-highlight.js (8.2+ds-5) ...
<13>Oct 28 02:18:24 google-dataproc-startup[843]: <13>Oct 28 02:18:24 uninstall[1443]: Removing nodejs (8.11.1~dfsg-2~bpo9+1) ...
<13>Oct 28 02:18:24 google-dataproc-startup[843]: <13>Oct 28 02:18:24 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:24 google-dataproc-startup[843]: <13>Oct 28 02:18:24 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:24 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 14.'
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 14.'
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 14.
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing libc-ares2:amd64 (1.14.0-1~bpo9+1) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing libisccfg140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing libdns162:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing update-inetd (4.44) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing libfile-copy-recursive-perl (0.38-1) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing liblwres141:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing libisccc140:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing libkadm5srv-mit11:amd64 (1.15-1+deb9u1) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing libkdb5-8:amd64 (1.15-1+deb9u1) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing libkadm5clnt-mit11:amd64 (1.15-1+deb9u1) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing libgssrpc4:amd64 (1.15-1+deb9u1) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing libhttp-parser2.8:amd64 (2.8.1-1~bpo9+1) ...
<13>Oct 28 02:18:25 google-dataproc-startup[843]: <13>Oct 28 02:18:25 uninstall[1443]: Removing libisc160:amd64 (1:9.10.3.dfsg.P4-12.3+deb9u5) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing libjs-bootstrap (3.3.7+dfsg-2+deb9u2) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 15.'
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 15.'
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 15.
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing libjs-d3 (3.5.17-2) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing libjs-es5-shim (4.5.9-1) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing r-cran-highr (0.6-1) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing libjs-highlight.js (8.2+ds-5) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing libjs-jquery-selectize.js (0.12.4+dfsg-1~bpo9+1) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing libjs-twitter-bootstrap-datepicker (1.3.1+dfsg1-1) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing libjs-twitter-bootstrap (2.0.2+dfsg-10) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing libjs-jquery-tablesorter (1:2.31.1+dfsg1-1~bpo9+1) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing libjs-jquery-datatables (1.10.13+dfsg-2) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing libjs-jquery-metadata (11-3) ...
<13>Oct 28 02:18:26 google-dataproc-startup[843]: <13>Oct 28 02:18:26 uninstall[1443]: Removing libjs-jquery-ui (1.12.1+dfsg-4) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing libjs-json (0~20160510-1) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 16.'
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 16.'
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 16.
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing libjs-microplugin.js (0.0.3+dfsg-1) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing libjs-modernizr (2.6.2+ds1-1) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing libjs-prettify (2013.03.04+dfsg-4) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing libjs-sifter.js (0.5.1+dfsg-2) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing pandoc (1.17.2~dfsg-3) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing liblua5.1-0:amd64 (5.1.5-8.1+b2) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + update_succeeded=1
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + break
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + ((  1  ))
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + [[ hadoop-hdfs-namenode == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + [[ 0 -eq 0 ]]
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + loginfo 'Waiting for namenode to listen on rpc port'
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + echo 'Waiting for namenode to listen on rpc port'
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: Waiting for namenode to listen on rpc port
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + wait_for_port cluster-89fa-m 8020
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + local -r host=cluster-89fa-m
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + local -r port=8020
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + loginfo 'Waiting for service to come up on host=cluster-89fa-m port=8020.'
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + echo 'Waiting for service to come up on host=cluster-89fa-m port=8020.'
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: Waiting for service to come up on host=cluster-89fa-m port=8020.
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + retry_with_constant_backoff nc -v -z -w 0 cluster-89fa-m 8020
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + local max_retry=300
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + cmd=("$@")
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + local -a cmd
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + local update_succeeded=0
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing libluajit-5.1-2:amd64 (2.1.0~beta3+dfsg-5.1~bpo9+1) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: ++ seq 1 300
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + nc -v -z -w 0 cluster-89fa-m 8020
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: nc: connect to cluster-89fa-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 1.'
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + echo 'nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 1.'
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 1.
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 setup-hadoop-hdfs-namenode[1505]: + sleep 1
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing libluajit-5.1-common (2.1.0~beta3+dfsg-5.1~bpo9+1) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing r-cran-httpuv (1.4.5.1+dfsg-1~bpo9+1) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing libuv1:amd64 (1.18.0-3~bpo9+1) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing libyaml-0-2:amd64 (0.2.1-1~bpo9+1) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing r-cran-dplyr (0.7.8-1~bpo9+1) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing r-cran-tidyselect (0.2.5-1~bpo9+1) ...
<13>Oct 28 02:18:27 google-dataproc-startup[843]: <13>Oct 28 02:18:27 uninstall[1443]: Removing r-cran-ggplot2 (3.1.0-1~bpo9+1) ...
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 uninstall[1443]: Removing r-cran-scales (1.0.0-1~bpo9+1) ...
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 17.'
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 17.'
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 17.
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 uninstall[1443]: Removing littler (0.3.1-1) ...
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 uninstall[1443]: Removing node-normalize.css (8.0.0-3~bpo9+1) ...
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 uninstall[1443]: Removing nodejs-doc (8.11.1~dfsg-2~bpo9+1) ...
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 uninstall[1443]: Removing pandoc-data (1.17.2~dfsg-3) ...
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 uninstall[1443]: Removing r-cran-tibble (2.0.1-1~bpo9+1) ...
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 uninstall[1443]: Removing r-cran-pillar (1.3.1-1~bpo9+1) ...
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hadoop-hdfs-namenode[1505]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hadoop-hdfs-namenode[1505]: + nc -v -z -w 0 cluster-89fa-m 8020
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hadoop-hdfs-namenode[1505]: nc: connect to cluster-89fa-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hadoop-hdfs-namenode[1505]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 2.'
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hadoop-hdfs-namenode[1505]: + echo 'nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 2.'
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hadoop-hdfs-namenode[1505]: nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 2.
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 setup-hadoop-hdfs-namenode[1505]: + sleep 1
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 uninstall[1443]: Removing r-cran-base64enc (0.1-3-1) ...
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 uninstall[1443]: Removing r-cran-bindrcpp (0.2.2-2~bpo9+1) ...
<13>Oct 28 02:18:28 google-dataproc-startup[843]: <13>Oct 28 02:18:28 uninstall[1443]: Removing r-cran-bindr (0.1.1-2~bpo9+1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-bit64 (0.9-7-2~bpo9+1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-bit (1.1-14-1~bpo9+1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-munsell (0.5.0-1~bpo9+1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 18.'
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 18.'
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 18.
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-colorspace (1.3-2-1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-testthat (2.0.1-1~bpo9+1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-data.table (1.10.0-1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-rsqlite (1.1-2-1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-dbi (1.0.0-1~bpo9+2) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hadoop-hdfs-namenode[1505]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hadoop-hdfs-namenode[1505]: + nc -v -z -w 0 cluster-89fa-m 8020
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hadoop-hdfs-namenode[1505]: nc: connect to cluster-89fa-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hadoop-hdfs-namenode[1505]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 3.'
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hadoop-hdfs-namenode[1505]: + echo 'nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 3.'
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hadoop-hdfs-namenode[1505]: nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 3.
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 setup-hadoop-hdfs-namenode[1505]: + sleep 1
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-memoise (1.1.0-1~bpo9+1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-htmlwidgets (1.3+dfsg-1~bpo9+1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-htmltools (0.3.6-2~bpo9+1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-digest (0.6.11-1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-evaluate (0.10-1) ...
<13>Oct 28 02:18:29 google-dataproc-startup[843]: <13>Oct 28 02:18:29 uninstall[1443]: Removing r-cran-fansi (0.4.0-1~bpo9+1) ...
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 uninstall[1443]: Removing r-cran-tikzdevice (0.10-1-1) ...
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 uninstall[1443]: Removing r-cran-filehash (2.3-1) ...
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 19.'
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 19.'
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 19.
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 uninstall[1443]: Removing r-cran-reshape2 (1.4.2-1) ...
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 uninstall[1443]: Removing r-cran-stringr (1.4.0-1~bpo9+1) ...
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 uninstall[1443]: Removing r-cran-glue (1.3.0-1~bpo9+1) ...
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 uninstall[1443]: Removing r-cran-googlevis (0.6.2-1) ...
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 uninstall[1443]: Removing r-cran-gtable (0.2.0-1) ...
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hadoop-hdfs-namenode[1505]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hadoop-hdfs-namenode[1505]: + nc -v -z -w 0 cluster-89fa-m 8020
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hadoop-hdfs-namenode[1505]: nc: connect to cluster-89fa-m port 8020 (tcp) failed: Connection refused
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hadoop-hdfs-namenode[1505]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 4.'
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hadoop-hdfs-namenode[1505]: + echo 'nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 4.'
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hadoop-hdfs-namenode[1505]: nc -v -z -w 0 cluster-89fa-m 8020 failed. Retry attempt: 4.
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 setup-hadoop-hdfs-namenode[1505]: + sleep 1
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 uninstall[1443]: Removing r-cran-hexbin (1.27.1-1) ...
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 uninstall[1443]: Removing r-cran-hms (0.4.2-1~bpo9+1) ...
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 uninstall[1443]: Removing r-cran-jsonlite (1.6+dfsg-1~bpo9+1) ...
<13>Oct 28 02:18:30 google-dataproc-startup[843]: <13>Oct 28 02:18:30 uninstall[1443]: Removing r-cran-labeling (0.3-1) ...
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 uninstall[1443]: Removing r-cran-promises (1.0.1-2~bpo9+1) ...
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 uninstall[1443]: Removing r-cran-later (0.7.5+dfsg-2~bpo9+1) ...
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 20.'
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 20.'
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 20.
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 uninstall[1443]: Removing r-cran-lazyeval (0.2.0-1) ...
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 uninstall[1443]: Removing r-cran-purrr (0.3.0-1~bpo9+1) ...
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 uninstall[1443]: Removing r-cran-magrittr (1.5-3) ...
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 uninstall[1443]: Removing r-cran-mapproj (1.2-4-1) ...
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 uninstall[1443]: Removing r-cran-maps (3.1.1-1) ...
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + nc -v -z -w 0 cluster-89fa-m 8020
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: Connection to cluster-89fa-m 8020 port [tcp/*] succeeded!
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + update_succeeded=1
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 8020 succeeded.'
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + echo 'nc -v -z -w 0 cluster-89fa-m 8020 succeeded.'
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: nc -v -z -w 0 cluster-89fa-m 8020 succeeded.
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + break
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + ((  1  ))
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + loginfo 'Service up on host=cluster-89fa-m port=8020.'
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + echo 'Service up on host=cluster-89fa-m port=8020.'
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: Service up on host=cluster-89fa-m port=8020.
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + loginfo 'Initializing HDFS directories'
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + echo 'Initializing HDFS directories'
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: Initializing HDFS directories
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + HADOOP_USERS=(hdfs mapred yarn spark pig hive hbase zookeeper)
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + REAL_USERS=($(getent passwd | awk -F: '1000 < $3 && $3 < 6000 { print $1}'))
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: ++ getent passwd
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: ++ awk -F: '1000 < $3 && $3 < 6000 { print $1}'
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + HDFS_USERS=("${HADOOP_USERS[@]}" "${REAL_USERS[@]}")
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + HDFS_USER_DIRS=("${HDFS_USERS[@]/#//user/}")
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 uninstall[1443]: Removing r-cran-mime (0.5-1) ...
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.eventLog.dir
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: ++ local property_file=/etc/spark/conf/spark-defaults.conf
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: ++ local property_name=spark.eventLog.dir
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: +++ tail -n 1
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: +++ grep '^spark.eventLog.dir=' /etc/spark/conf/spark-defaults.conf
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: +++ cut -d = -f 2-
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: +++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: ++ local property_value=hdfs://cluster-89fa-m/user/spark/eventlog
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: ++ echo hdfs://cluster-89fa-m/user/spark/eventlog
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + SPARK_EVENTLOG_DIR=hdfs://cluster-89fa-m/user/spark/eventlog
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 setup-hadoop-hdfs-namenode[1505]: + su -s /bin/bash hdfs -c 'login_through_keytab_if_necessary /etc/security/keytab/hdfs.service.keytab hdfs/cluster-89fa-m.us-central1-c.c.lustrous-drake-255300.internal &&              hadoop fs -mkdir -p              /tmp/hadoop-yarn/staging/history /user/hdfs /user/mapred /user/yarn /user/spark /user/pig /user/hive /user/hbase /user/zookeeper hdfs://cluster-89fa-m/user/spark/eventlog'
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 uninstall[1443]: Removing r-cran-pkgconfig (2.0.2-1~bpo9+1) ...
<13>Oct 28 02:18:31 google-dataproc-startup[843]: <13>Oct 28 02:18:31 uninstall[1443]: Removing r-cran-plyr (1.8.4-1) ...
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 uninstall[1443]: Removing r-cran-png (0.1-7-1) ...
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 uninstall[1443]: Removing r-cran-praise (1.0.0-1) ...
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: + update_succeeded=1
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: + break
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: + ((  1  ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: + [[ google-fluentd == \h\i\v\e\-\m\e\t\a\s\t\o\r\e ]]
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: ++ get_dataproc_property am.primary_only
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: ++ local property_name=am.primary_only
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: +++ local property_name=am.primary_only
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 uninstall[1443]: Removing r-cran-r6 (2.4.0-1~bpo9+1) ...
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 21.'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 21.'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 21.
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: ++++ cut -d = -f 2-
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: ++++ tail -n 1
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: +++ local property_value=false
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: +++ echo false
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: ++ local property_value=false
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: ++ echo false
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 setup-google-fluentd[1623]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + pid=1521
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + cmd='setup_service hadoop-hdfs-secondarynamenode'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1521 cmd=[setup_service hadoop-hdfs-secondarynamenode]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + echo 'Waiting on pid=1521 cmd=[setup_service hadoop-hdfs-secondarynamenode]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: Waiting on pid=1521 cmd=[setup_service hadoop-hdfs-secondarynamenode]
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + wait 1521
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + pid=1512
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + cmd='setup_service mariadb'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1512 cmd=[setup_service mariadb]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + echo 'Waiting on pid=1512 cmd=[setup_service mariadb]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: Waiting on pid=1512 cmd=[setup_service mariadb]
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + wait 1512
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + pid=1511
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + cmd='setup_service hadoop-yarn-timelineserver'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1511 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + echo 'Waiting on pid=1511 cmd=[setup_service hadoop-yarn-timelineserver]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: Waiting on pid=1511 cmd=[setup_service hadoop-yarn-timelineserver]
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + wait 1511
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + pid=1510
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + cmd='setup_service spark-history-server'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1510 cmd=[setup_service spark-history-server]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + echo 'Waiting on pid=1510 cmd=[setup_service spark-history-server]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: Waiting on pid=1510 cmd=[setup_service spark-history-server]
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + wait 1510
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + pid=1509
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + cmd='setup_service hadoop-mapreduce-historyserver'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1509 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + echo 'Waiting on pid=1509 cmd=[setup_service hadoop-mapreduce-historyserver]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: Waiting on pid=1509 cmd=[setup_service hadoop-mapreduce-historyserver]
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + wait 1509
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + pid=1508
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + cmd='setup_service hive-server2'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1508 cmd=[setup_service hive-server2]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + echo 'Waiting on pid=1508 cmd=[setup_service hive-server2]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: Waiting on pid=1508 cmd=[setup_service hive-server2]
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + wait 1508
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + pid=1507
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + cmd='setup_service hive-metastore'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1507 cmd=[setup_service hive-metastore]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + echo 'Waiting on pid=1507 cmd=[setup_service hive-metastore]'
<13>Oct 28 02:18:32 google-dataproc-startup[843]: Waiting on pid=1507 cmd=[setup_service hive-metastore]
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:18:32 google-dataproc-startup[843]: + wait 1507
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 uninstall[1443]: Removing r-cran-rcolorbrewer (1.1-2-1) ...
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 uninstall[1443]: Removing r-cran-rlang (0.3.1-2~bpo9+1) ...
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 uninstall[1443]: Removing r-cran-sourcetools (0.1.5-1) ...
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 uninstall[1443]: Removing r-cran-sp (1:1.2-4-1) ...
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 uninstall[1443]: Removing r-cran-stringi (1.2.4-2~bpo9+1) ...
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 uninstall[1443]: Removing r-cran-testit (0.6-1) ...
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 uninstall[1443]: Removing r-cran-tinytex (0.10-1~bpo9+1) ...
<13>Oct 28 02:18:32 google-dataproc-startup[843]: <13>Oct 28 02:18:32 uninstall[1443]: Removing r-cran-utf8 (1.1.4-1~bpo9+1) ...
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 uninstall[1443]: Removing r-cran-viridislite (0.3.0-3~bpo9+1) ...
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 uninstall[1443]: Removing r-cran-withr (2.1.2-1~bpo9+1) ...
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 uninstall[1443]: Removing r-cran-xfun (0.4-1~bpo9+1) ...
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 22.'
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 22.'
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 22.
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 uninstall[1443]: Removing r-cran-xml2 (1.1.0-1) ...
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 uninstall[1443]: Removing r-cran-xtable (1:1.8-2-1) ...
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 uninstall[1443]: Removing r-cran-yaml (2.2.0-1~bpo9+1) ...
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 uninstall[1443]: Removing solr-server (6.6.5-1) ...
<13>Oct 28 02:18:33 google-dataproc-startup[843]: <13>Oct 28 02:18:33 uninstall[1443]: Removing solr (6.6.5-1) ...
<13>Oct 28 02:18:34 google-dataproc-startup[843]: <13>Oct 28 02:18:34 uninstall[1443]: Removing xinetd (1:2.3.15-7) ...
<13>Oct 28 02:18:34 google-dataproc-startup[843]: <13>Oct 28 02:18:34 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:34 google-dataproc-startup[843]: <13>Oct 28 02:18:34 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:34 google-dataproc-startup[843]: <13>Oct 28 02:18:34 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:34 google-dataproc-startup[843]: <13>Oct 28 02:18:34 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 23.'
<13>Oct 28 02:18:34 google-dataproc-startup[843]: <13>Oct 28 02:18:34 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 23.'
<13>Oct 28 02:18:34 google-dataproc-startup[843]: <13>Oct 28 02:18:34 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 23.
<13>Oct 28 02:18:34 google-dataproc-startup[843]: <13>Oct 28 02:18:34 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:34 google-dataproc-startup[843]: <13>Oct 28 02:18:34 uninstall[1443]: Removing zeppelin (0.8.0-1) ...
<13>Oct 28 02:18:35 google-dataproc-startup[843]: <13>Oct 28 02:18:35 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:35 google-dataproc-startup[843]: <13>Oct 28 02:18:35 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:35 google-dataproc-startup[843]: <13>Oct 28 02:18:35 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:35 google-dataproc-startup[843]: <13>Oct 28 02:18:35 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 24.'
<13>Oct 28 02:18:35 google-dataproc-startup[843]: <13>Oct 28 02:18:35 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 24.'
<13>Oct 28 02:18:35 google-dataproc-startup[843]: <13>Oct 28 02:18:35 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 24.
<13>Oct 28 02:18:35 google-dataproc-startup[843]: <13>Oct 28 02:18:35 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:35 google-dataproc-startup[843]: <13>Oct 28 02:18:35 uninstall[1443]: Removing zookeeper-server (3.4.13-1) ...
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 uninstall[1443]: Removing libgeoip1:amd64 (1.6.9-4) ...
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 uninstall[1443]: Removing libjs-jquery (3.1.1-2+deb9u1) ...
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 25.'
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 25.'
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 25.
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 uninstall[1443]: Removing r-cran-rcpp (1.0.0-1~bpo9+1) ...
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 uninstall[1443]: Removing r-cran-cli (1.0.1-1~bpo9+1) ...
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 uninstall[1443]: Removing r-cran-assertthat (0.2.0-1~bpo9+1) ...
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 uninstall[1443]: Removing r-cran-crayon (1.3.4-2~bpo9+1) ...
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 uninstall[1443]: Removing r-cran-littler (0.3.1-1) ...
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 uninstall[1443]: Removing r-cran-pkgkitten (0.1.4-1) ...
<13>Oct 28 02:18:36 google-dataproc-startup[843]: <13>Oct 28 02:18:36 uninstall[1443]: Removing libverto1:amd64 (0.2.4-2.1) ...
<13>Oct 28 02:18:37 google-dataproc-startup[843]: <13>Oct 28 02:18:37 uninstall[1443]: Removing libverto-libev1:amd64 (0.2.4-2.1) ...
<13>Oct 28 02:18:37 google-dataproc-startup[843]: <13>Oct 28 02:18:37 uninstall[1443]: Removing libev4 (1:4.22-1+b1) ...
<13>Oct 28 02:18:37 google-dataproc-startup[843]: <13>Oct 28 02:18:37 uninstall[1443]: Processing triggers for libc-bin (2.24-11+deb9u4) ...
<13>Oct 28 02:18:37 google-dataproc-startup[843]: <13>Oct 28 02:18:37 uninstall[1443]: Processing triggers for man-db (2.7.6.1-2) ...
<13>Oct 28 02:18:37 google-dataproc-startup[843]: <13>Oct 28 02:18:37 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:37 google-dataproc-startup[843]: <13>Oct 28 02:18:37 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:37 google-dataproc-startup[843]: <13>Oct 28 02:18:37 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:37 google-dataproc-startup[843]: <13>Oct 28 02:18:37 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 26.'
<13>Oct 28 02:18:37 google-dataproc-startup[843]: <13>Oct 28 02:18:37 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 26.'
<13>Oct 28 02:18:37 google-dataproc-startup[843]: <13>Oct 28 02:18:37 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 26.
<13>Oct 28 02:18:37 google-dataproc-startup[843]: <13>Oct 28 02:18:37 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:38 google-dataproc-startup[843]: <13>Oct 28 02:18:38 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:38 google-dataproc-startup[843]: <13>Oct 28 02:18:38 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:38 google-dataproc-startup[843]: <13>Oct 28 02:18:38 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:38 google-dataproc-startup[843]: <13>Oct 28 02:18:38 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 27.'
<13>Oct 28 02:18:38 google-dataproc-startup[843]: <13>Oct 28 02:18:38 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 27.'
<13>Oct 28 02:18:38 google-dataproc-startup[843]: <13>Oct 28 02:18:38 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 27.
<13>Oct 28 02:18:38 google-dataproc-startup[843]: <13>Oct 28 02:18:38 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 28.'
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 28.'
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 28.
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: + run_with_retries sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: + local retry_backoff
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: + cmd=("$@")
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: + local -a cmd
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: + loginfo 'About to run '\''sudo -u hdfs hadoop fs -chmod -R 1777 /'\'' with retries...'
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: + echo 'About to run '\''sudo -u hdfs hadoop fs -chmod -R 1777 /'\'' with retries...'
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: About to run 'sudo -u hdfs hadoop fs -chmod -R 1777 /' with retries...
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: + local update_succeeded=0
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: + (( i = 0 ))
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: + (( i < 12 ))
<13>Oct 28 02:18:39 google-dataproc-startup[843]: <13>Oct 28 02:18:39 setup-hadoop-hdfs-namenode[1505]: + sudo -u hdfs hadoop fs -chmod -R 1777 /
<13>Oct 28 02:18:40 google-dataproc-startup[843]: <13>Oct 28 02:18:40 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:40 google-dataproc-startup[843]: <13>Oct 28 02:18:40 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:40 google-dataproc-startup[843]: <13>Oct 28 02:18:40 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:40 google-dataproc-startup[843]: <13>Oct 28 02:18:40 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 29.'
<13>Oct 28 02:18:40 google-dataproc-startup[843]: <13>Oct 28 02:18:40 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 29.'
<13>Oct 28 02:18:40 google-dataproc-startup[843]: <13>Oct 28 02:18:40 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 29.
<13>Oct 28 02:18:40 google-dataproc-startup[843]: <13>Oct 28 02:18:40 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:41 google-dataproc-startup[843]: <13>Oct 28 02:18:41 uninstall[1443]: Processing triggers for fontconfig (2.11.0-6.7+b1) ...
<13>Oct 28 02:18:41 google-dataproc-startup[843]: <13>Oct 28 02:18:41 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:41 google-dataproc-startup[843]: <13>Oct 28 02:18:41 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:41 google-dataproc-startup[843]: <13>Oct 28 02:18:41 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:41 google-dataproc-startup[843]: <13>Oct 28 02:18:41 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 30.'
<13>Oct 28 02:18:41 google-dataproc-startup[843]: <13>Oct 28 02:18:41 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 30.'
<13>Oct 28 02:18:41 google-dataproc-startup[843]: <13>Oct 28 02:18:41 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 30.
<13>Oct 28 02:18:41 google-dataproc-startup[843]: <13>Oct 28 02:18:41 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:42 google-dataproc-startup[843]: <13>Oct 28 02:18:42 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:42 google-dataproc-startup[843]: <13>Oct 28 02:18:42 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:42 google-dataproc-startup[843]: <13>Oct 28 02:18:42 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:42 google-dataproc-startup[843]: <13>Oct 28 02:18:42 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 31.'
<13>Oct 28 02:18:42 google-dataproc-startup[843]: <13>Oct 28 02:18:42 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 31.'
<13>Oct 28 02:18:42 google-dataproc-startup[843]: <13>Oct 28 02:18:42 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 31.
<13>Oct 28 02:18:42 google-dataproc-startup[843]: <13>Oct 28 02:18:42 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:43 google-dataproc-startup[843]: <13>Oct 28 02:18:43 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:43 google-dataproc-startup[843]: <13>Oct 28 02:18:43 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:43 google-dataproc-startup[843]: <13>Oct 28 02:18:43 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:43 google-dataproc-startup[843]: <13>Oct 28 02:18:43 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 32.'
<13>Oct 28 02:18:43 google-dataproc-startup[843]: <13>Oct 28 02:18:43 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 32.'
<13>Oct 28 02:18:43 google-dataproc-startup[843]: <13>Oct 28 02:18:43 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 32.
<13>Oct 28 02:18:43 google-dataproc-startup[843]: <13>Oct 28 02:18:43 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:43 google-dataproc-startup[843]: <13>Oct 28 02:18:43 uninstall[1443]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 102578 files and directories currently installed.)
<13>Oct 28 02:18:43 google-dataproc-startup[843]: <13>Oct 28 02:18:43 uninstall[1443]: Purging configuration files for update-inetd (4.44) ...
<13>Oct 28 02:18:44 google-dataproc-startup[843]: <13>Oct 28 02:18:44 uninstall[1443]: Purging configuration files for hive-webhcat-server (2.3.5-1) ...
<13>Oct 28 02:18:44 google-dataproc-startup[843]: <13>Oct 28 02:18:44 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:44 google-dataproc-startup[843]: <13>Oct 28 02:18:44 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:44 google-dataproc-startup[843]: <13>Oct 28 02:18:44 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:44 google-dataproc-startup[843]: <13>Oct 28 02:18:44 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 33.'
<13>Oct 28 02:18:44 google-dataproc-startup[843]: <13>Oct 28 02:18:44 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 33.'
<13>Oct 28 02:18:44 google-dataproc-startup[843]: <13>Oct 28 02:18:44 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 33.
<13>Oct 28 02:18:44 google-dataproc-startup[843]: <13>Oct 28 02:18:44 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:44 google-dataproc-startup[843]: <13>Oct 28 02:18:44 uninstall[1443]: Purging configuration files for xinetd (1:2.3.15-7) ...
<13>Oct 28 02:18:45 google-dataproc-startup[843]: <13>Oct 28 02:18:45 uninstall[1443]: Purging configuration files for zookeeper-server (3.4.13-1) ...
<13>Oct 28 02:18:45 google-dataproc-startup[843]: <13>Oct 28 02:18:45 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:45 google-dataproc-startup[843]: <13>Oct 28 02:18:45 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:45 google-dataproc-startup[843]: <13>Oct 28 02:18:45 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:45 google-dataproc-startup[843]: <13>Oct 28 02:18:45 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 34.'
<13>Oct 28 02:18:45 google-dataproc-startup[843]: <13>Oct 28 02:18:45 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 34.'
<13>Oct 28 02:18:45 google-dataproc-startup[843]: <13>Oct 28 02:18:45 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 34.
<13>Oct 28 02:18:45 google-dataproc-startup[843]: <13>Oct 28 02:18:45 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:45 google-dataproc-startup[843]: <13>Oct 28 02:18:45 uninstall[1443]: Purging configuration files for hadoop-hdfs-journalnode (2.9.2-1) ...
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 uninstall[1443]: Purging configuration files for solr (6.6.5-1) ...
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 uninstall[1443]: Purging configuration files for krb5-admin-server (1.15-1+deb9u1) ...
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + update_succeeded=1
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + break
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + ((  1  ))
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + run_with_retries sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-89fa-m/user/spark/eventlog
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + local retry_backoff
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + cmd=("$@")
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + local -a cmd
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + loginfo 'About to run '\''sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-89fa-m/user/spark/eventlog'\'' with retries...'
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + echo 'About to run '\''sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-89fa-m/user/spark/eventlog'\'' with retries...'
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: About to run 'sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-89fa-m/user/spark/eventlog' with retries...
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + local update_succeeded=0
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + (( i = 0 ))
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + (( i < 12 ))
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hadoop-hdfs-namenode[1505]: + sudo -u hdfs hadoop fs -chgrp spark hdfs://cluster-89fa-m/user/spark/eventlog
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 35.'
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 35.'
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 35.
<13>Oct 28 02:18:46 google-dataproc-startup[843]: <13>Oct 28 02:18:46 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:47 google-dataproc-startup[843]: <13>Oct 28 02:18:47 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:47 google-dataproc-startup[843]: <13>Oct 28 02:18:47 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:47 google-dataproc-startup[843]: <13>Oct 28 02:18:47 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:47 google-dataproc-startup[843]: <13>Oct 28 02:18:47 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 36.'
<13>Oct 28 02:18:47 google-dataproc-startup[843]: <13>Oct 28 02:18:47 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 36.'
<13>Oct 28 02:18:47 google-dataproc-startup[843]: <13>Oct 28 02:18:47 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 36.
<13>Oct 28 02:18:47 google-dataproc-startup[843]: <13>Oct 28 02:18:47 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:48 google-dataproc-startup[843]: <13>Oct 28 02:18:48 uninstall[1443]: Purging configuration files for kafka (1.1.1-1) ...
<13>Oct 28 02:18:48 google-dataproc-startup[843]: <13>Oct 28 02:18:48 uninstall[1443]: Purging configuration files for hadoop-hdfs-zkfc (2.9.2-1) ...
<13>Oct 28 02:18:48 google-dataproc-startup[843]: <13>Oct 28 02:18:48 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:48 google-dataproc-startup[843]: <13>Oct 28 02:18:48 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:48 google-dataproc-startup[843]: <13>Oct 28 02:18:48 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:48 google-dataproc-startup[843]: <13>Oct 28 02:18:48 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 37.'
<13>Oct 28 02:18:48 google-dataproc-startup[843]: <13>Oct 28 02:18:48 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 37.'
<13>Oct 28 02:18:48 google-dataproc-startup[843]: <13>Oct 28 02:18:48 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 37.
<13>Oct 28 02:18:48 google-dataproc-startup[843]: <13>Oct 28 02:18:48 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:48 google-dataproc-startup[843]: <13>Oct 28 02:18:48 uninstall[1443]: Purging configuration files for krb5-kdc (1.15-1+deb9u1) ...
<13>Oct 28 02:18:49 google-dataproc-startup[843]: <13>Oct 28 02:18:49 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:49 google-dataproc-startup[843]: <13>Oct 28 02:18:49 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:49 google-dataproc-startup[843]: <13>Oct 28 02:18:49 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:49 google-dataproc-startup[843]: <13>Oct 28 02:18:49 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 38.'
<13>Oct 28 02:18:49 google-dataproc-startup[843]: <13>Oct 28 02:18:49 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 38.'
<13>Oct 28 02:18:49 google-dataproc-startup[843]: <13>Oct 28 02:18:49 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 38.
<13>Oct 28 02:18:49 google-dataproc-startup[843]: <13>Oct 28 02:18:49 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:50 google-dataproc-startup[843]: <13>Oct 28 02:18:50 uninstall[1443]: Purging configuration files for libjs-jquery-ui (1.12.1+dfsg-4) ...
<13>Oct 28 02:18:50 google-dataproc-startup[843]: <13>Oct 28 02:18:50 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:50 google-dataproc-startup[843]: <13>Oct 28 02:18:50 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:50 google-dataproc-startup[843]: <13>Oct 28 02:18:50 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:50 google-dataproc-startup[843]: <13>Oct 28 02:18:50 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 39.'
<13>Oct 28 02:18:50 google-dataproc-startup[843]: <13>Oct 28 02:18:50 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 39.'
<13>Oct 28 02:18:50 google-dataproc-startup[843]: <13>Oct 28 02:18:50 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 39.
<13>Oct 28 02:18:50 google-dataproc-startup[843]: <13>Oct 28 02:18:50 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:50 google-dataproc-startup[843]: <13>Oct 28 02:18:50 uninstall[1443]: Purging configuration files for krb5-kpropd (1.15-1+deb9u1) ...
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + update_succeeded=1
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + break
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + ((  1  ))
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + for SERVICE in "${DATAPROC_START_AFTER_HDFS_SERVICES[@]}"
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + run_with_retries systemctl start hadoop-mapreduce-historyserver
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + local retry_backoff
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + cmd=("$@")
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + local -a cmd
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + loginfo 'About to run '\''systemctl start hadoop-mapreduce-historyserver'\'' with retries...'
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + echo 'About to run '\''systemctl start hadoop-mapreduce-historyserver'\'' with retries...'
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: About to run 'systemctl start hadoop-mapreduce-historyserver' with retries...
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + local update_succeeded=0
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + (( i = 0 ))
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + (( i < 12 ))
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hadoop-hdfs-namenode[1505]: + systemctl start hadoop-mapreduce-historyserver
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 uninstall[1443]: Purging configuration files for hadoop-hdfs-datanode (2.9.2-1) ...
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 uninstall[1443]: Purging configuration files for javascript-common (11) ...
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 40.'
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 40.'
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 40.
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:51 google-dataproc-startup[843]: <13>Oct 28 02:18:51 uninstall[1443]: Purging configuration files for krb5-config (2.6) ...
<13>Oct 28 02:18:52 google-dataproc-startup[843]: <13>Oct 28 02:18:52 uninstall[1443]: Purging configuration files for knox (1.1.0-1) ...
<13>Oct 28 02:18:52 google-dataproc-startup[843]: <13>Oct 28 02:18:52 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:52 google-dataproc-startup[843]: <13>Oct 28 02:18:52 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:52 google-dataproc-startup[843]: <13>Oct 28 02:18:52 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:52 google-dataproc-startup[843]: <13>Oct 28 02:18:52 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 41.'
<13>Oct 28 02:18:52 google-dataproc-startup[843]: <13>Oct 28 02:18:52 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 41.'
<13>Oct 28 02:18:52 google-dataproc-startup[843]: <13>Oct 28 02:18:52 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 41.
<13>Oct 28 02:18:52 google-dataproc-startup[843]: <13>Oct 28 02:18:52 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:52 google-dataproc-startup[843]: <13>Oct 28 02:18:52 uninstall[1443]: Purging configuration files for r-cran-rmarkdown (1.11+dfsg-1~bpo9+1) ...
<13>Oct 28 02:18:53 google-dataproc-startup[843]: <13>Oct 28 02:18:53 uninstall[1443]: Purging configuration files for hive-webhcat (2.3.5-1) ...
<13>Oct 28 02:18:53 google-dataproc-startup[843]: <13>Oct 28 02:18:53 uninstall[1443]: Purging configuration files for solr-server (6.6.5-1) ...
<13>Oct 28 02:18:53 google-dataproc-startup[843]: <13>Oct 28 02:18:53 uninstall[1443]: Purging configuration files for kafka-server (1.1.1-1) ...
<13>Oct 28 02:18:53 google-dataproc-startup[843]: <13>Oct 28 02:18:53 uninstall[1443]: Purging configuration files for zeppelin (0.8.0-1) ...
<13>Oct 28 02:18:53 google-dataproc-startup[843]: <13>Oct 28 02:18:53 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:53 google-dataproc-startup[843]: <13>Oct 28 02:18:53 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:53 google-dataproc-startup[843]: <13>Oct 28 02:18:53 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:53 google-dataproc-startup[843]: <13>Oct 28 02:18:53 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 42.'
<13>Oct 28 02:18:53 google-dataproc-startup[843]: <13>Oct 28 02:18:53 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 42.'
<13>Oct 28 02:18:53 google-dataproc-startup[843]: <13>Oct 28 02:18:53 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 42.
<13>Oct 28 02:18:53 google-dataproc-startup[843]: <13>Oct 28 02:18:53 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:54 google-dataproc-startup[843]: <13>Oct 28 02:18:54 uninstall[1443]: Purging configuration files for hadoop-yarn-nodemanager (2.9.2-1) ...
<13>Oct 28 02:18:54 google-dataproc-startup[843]: <13>Oct 28 02:18:54 uninstall[1443]: Processing triggers for systemd (232-25+deb9u12) ...
<13>Oct 28 02:18:54 google-dataproc-startup[843]: <13>Oct 28 02:18:54 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:54 google-dataproc-startup[843]: <13>Oct 28 02:18:54 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:54 google-dataproc-startup[843]: <13>Oct 28 02:18:54 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:54 google-dataproc-startup[843]: <13>Oct 28 02:18:54 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 43.'
<13>Oct 28 02:18:54 google-dataproc-startup[843]: <13>Oct 28 02:18:54 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 43.'
<13>Oct 28 02:18:54 google-dataproc-startup[843]: <13>Oct 28 02:18:54 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 43.
<13>Oct 28 02:18:54 google-dataproc-startup[843]: <13>Oct 28 02:18:54 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:56 google-dataproc-startup[843]: <13>Oct 28 02:18:56 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:56 google-dataproc-startup[843]: <13>Oct 28 02:18:56 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:56 google-dataproc-startup[843]: <13>Oct 28 02:18:56 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 9083 (tcp) failed: Connection refused
<13>Oct 28 02:18:56 google-dataproc-startup[843]: <13>Oct 28 02:18:56 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 44.'
<13>Oct 28 02:18:56 google-dataproc-startup[843]: <13>Oct 28 02:18:56 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 44.'
<13>Oct 28 02:18:56 google-dataproc-startup[843]: <13>Oct 28 02:18:56 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 failed. Retry attempt: 44.
<13>Oct 28 02:18:56 google-dataproc-startup[843]: <13>Oct 28 02:18:56 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 9083
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: Connection to cluster-89fa-m 9083 port [tcp/*] succeeded!
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + update_succeeded=1
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 9083 succeeded.'
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 9083 succeeded.'
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 9083 succeeded.
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + break
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + ((  1  ))
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + loginfo 'Service up on host=cluster-89fa-m port=9083.'
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + echo 'Service up on host=cluster-89fa-m port=9083.'
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: Service up on host=cluster-89fa-m port=9083.
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + run_with_retries systemctl start hive-server2
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + local retry_backoff
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + cmd=("$@")
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + local -a cmd
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + loginfo 'About to run '\''systemctl start hive-server2'\'' with retries...'
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + echo 'About to run '\''systemctl start hive-server2'\'' with retries...'
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: About to run 'systemctl start hive-server2' with retries...
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + local update_succeeded=0
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + (( i = 0 ))
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + (( i < 12 ))
<13>Oct 28 02:18:57 google-dataproc-startup[843]: <13>Oct 28 02:18:57 setup-hive-metastore[1507]: + systemctl start hive-server2
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + update_succeeded=1
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + break
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + ((  1  ))
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + for SERVICE in "${DATAPROC_START_AFTER_HDFS_SERVICES[@]}"
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + run_with_retries systemctl start spark-history-server
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + retry_backoff=(1 1 2 3 5 8 13 21 34 55 89 144)
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + local retry_backoff
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + cmd=("$@")
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + local -a cmd
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + loginfo 'About to run '\''systemctl start spark-history-server'\'' with retries...'
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + echo 'About to run '\''systemctl start spark-history-server'\'' with retries...'
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: About to run 'systemctl start spark-history-server' with retries...
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + local update_succeeded=0
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + (( i = 0 ))
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + (( i < 12 ))
<13>Oct 28 02:18:58 google-dataproc-startup[843]: <13>Oct 28 02:18:58 setup-hadoop-hdfs-namenode[1505]: + systemctl start spark-history-server
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + update_succeeded=1
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + break
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + ((  1  ))
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: ++ get_xml_property_or_default /etc/hive/conf/hive-site.xml hive.server2.thrift.port 10000
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: ++ file=/etc/hive/conf/hive-site.xml
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: ++ property=hive.server2.thrift.port
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: ++ default_value=10000
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: +++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.server2.thrift.port
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: ++ val=None
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: ++ [[ None = \N\o\n\e ]]
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: ++ val=10000
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: ++ echo 10000
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + thrift_port=10000
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + wait_for_port cluster-89fa-m 10000
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + local -r host=cluster-89fa-m
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + local -r port=10000
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + loginfo 'Waiting for service to come up on host=cluster-89fa-m port=10000.'
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + echo 'Waiting for service to come up on host=cluster-89fa-m port=10000.'
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: Waiting for service to come up on host=cluster-89fa-m port=10000.
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + retry_with_constant_backoff nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + local max_retry=300
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + cmd=("$@")
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + local -a cmd
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + local update_succeeded=0
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: ++ seq 1 300
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 1.'
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 1.'
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 1.
<13>Oct 28 02:19:00 google-dataproc-startup[843]: <13>Oct 28 02:19:00 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: + update_succeeded=1
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: + break
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: + ((  1  ))
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: ++ get_dataproc_property am.primary_only
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: ++ local property_name=am.primary_only
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: +++ local property_name=am.primary_only
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: ++++ cut -d = -f 2-
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: ++++ tail -n 1
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: +++ local property_value=false
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: +++ echo false
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: ++ local property_value=false
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: ++ echo false
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hadoop-hdfs-namenode[1505]: + [[ hadoop-hdfs-namenode == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 2.'
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 2.'
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 2.
<13>Oct 28 02:19:01 google-dataproc-startup[843]: <13>Oct 28 02:19:01 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:02 google-dataproc-startup[843]: <13>Oct 28 02:19:02 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:02 google-dataproc-startup[843]: <13>Oct 28 02:19:02 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:02 google-dataproc-startup[843]: <13>Oct 28 02:19:02 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:02 google-dataproc-startup[843]: <13>Oct 28 02:19:02 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 3.'
<13>Oct 28 02:19:02 google-dataproc-startup[843]: <13>Oct 28 02:19:02 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 3.'
<13>Oct 28 02:19:02 google-dataproc-startup[843]: <13>Oct 28 02:19:02 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 3.
<13>Oct 28 02:19:02 google-dataproc-startup[843]: <13>Oct 28 02:19:02 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:03 google-dataproc-startup[843]: <13>Oct 28 02:19:03 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:03 google-dataproc-startup[843]: <13>Oct 28 02:19:03 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:03 google-dataproc-startup[843]: <13>Oct 28 02:19:03 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:03 google-dataproc-startup[843]: <13>Oct 28 02:19:03 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 4.'
<13>Oct 28 02:19:03 google-dataproc-startup[843]: <13>Oct 28 02:19:03 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 4.'
<13>Oct 28 02:19:03 google-dataproc-startup[843]: <13>Oct 28 02:19:03 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 4.
<13>Oct 28 02:19:03 google-dataproc-startup[843]: <13>Oct 28 02:19:03 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:04 google-dataproc-startup[843]: <13>Oct 28 02:19:04 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:04 google-dataproc-startup[843]: <13>Oct 28 02:19:04 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:05 google-dataproc-startup[843]: <13>Oct 28 02:19:05 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:05 google-dataproc-startup[843]: <13>Oct 28 02:19:05 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 5.'
<13>Oct 28 02:19:05 google-dataproc-startup[843]: <13>Oct 28 02:19:05 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 5.'
<13>Oct 28 02:19:05 google-dataproc-startup[843]: <13>Oct 28 02:19:05 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 5.
<13>Oct 28 02:19:05 google-dataproc-startup[843]: <13>Oct 28 02:19:05 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:06 google-dataproc-startup[843]: <13>Oct 28 02:19:06 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:06 google-dataproc-startup[843]: <13>Oct 28 02:19:06 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:06 google-dataproc-startup[843]: <13>Oct 28 02:19:06 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:06 google-dataproc-startup[843]: <13>Oct 28 02:19:06 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 6.'
<13>Oct 28 02:19:06 google-dataproc-startup[843]: <13>Oct 28 02:19:06 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 6.'
<13>Oct 28 02:19:06 google-dataproc-startup[843]: <13>Oct 28 02:19:06 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 6.
<13>Oct 28 02:19:06 google-dataproc-startup[843]: <13>Oct 28 02:19:06 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:07 google-dataproc-startup[843]: <13>Oct 28 02:19:07 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:07 google-dataproc-startup[843]: <13>Oct 28 02:19:07 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:07 google-dataproc-startup[843]: <13>Oct 28 02:19:07 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:07 google-dataproc-startup[843]: <13>Oct 28 02:19:07 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 7.'
<13>Oct 28 02:19:07 google-dataproc-startup[843]: <13>Oct 28 02:19:07 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 7.'
<13>Oct 28 02:19:07 google-dataproc-startup[843]: <13>Oct 28 02:19:07 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 7.
<13>Oct 28 02:19:07 google-dataproc-startup[843]: <13>Oct 28 02:19:07 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:08 google-dataproc-startup[843]: <13>Oct 28 02:19:08 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:08 google-dataproc-startup[843]: <13>Oct 28 02:19:08 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:08 google-dataproc-startup[843]: <13>Oct 28 02:19:08 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:08 google-dataproc-startup[843]: <13>Oct 28 02:19:08 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 8.'
<13>Oct 28 02:19:08 google-dataproc-startup[843]: <13>Oct 28 02:19:08 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 8.'
<13>Oct 28 02:19:08 google-dataproc-startup[843]: <13>Oct 28 02:19:08 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 8.
<13>Oct 28 02:19:08 google-dataproc-startup[843]: <13>Oct 28 02:19:08 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:09 google-dataproc-startup[843]: <13>Oct 28 02:19:09 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:09 google-dataproc-startup[843]: <13>Oct 28 02:19:09 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:09 google-dataproc-startup[843]: <13>Oct 28 02:19:09 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:09 google-dataproc-startup[843]: <13>Oct 28 02:19:09 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 9.'
<13>Oct 28 02:19:09 google-dataproc-startup[843]: <13>Oct 28 02:19:09 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 9.'
<13>Oct 28 02:19:09 google-dataproc-startup[843]: <13>Oct 28 02:19:09 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 9.
<13>Oct 28 02:19:09 google-dataproc-startup[843]: <13>Oct 28 02:19:09 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:10 google-dataproc-startup[843]: <13>Oct 28 02:19:10 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:10 google-dataproc-startup[843]: <13>Oct 28 02:19:10 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:10 google-dataproc-startup[843]: <13>Oct 28 02:19:10 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:10 google-dataproc-startup[843]: <13>Oct 28 02:19:10 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 10.'
<13>Oct 28 02:19:10 google-dataproc-startup[843]: <13>Oct 28 02:19:10 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 10.'
<13>Oct 28 02:19:10 google-dataproc-startup[843]: <13>Oct 28 02:19:10 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 10.
<13>Oct 28 02:19:10 google-dataproc-startup[843]: <13>Oct 28 02:19:10 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:11 google-dataproc-startup[843]: <13>Oct 28 02:19:11 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:11 google-dataproc-startup[843]: <13>Oct 28 02:19:11 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:11 google-dataproc-startup[843]: <13>Oct 28 02:19:11 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:11 google-dataproc-startup[843]: <13>Oct 28 02:19:11 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 11.'
<13>Oct 28 02:19:11 google-dataproc-startup[843]: <13>Oct 28 02:19:11 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 11.'
<13>Oct 28 02:19:11 google-dataproc-startup[843]: <13>Oct 28 02:19:11 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 11.
<13>Oct 28 02:19:11 google-dataproc-startup[843]: <13>Oct 28 02:19:11 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:12 google-dataproc-startup[843]: <13>Oct 28 02:19:12 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:12 google-dataproc-startup[843]: <13>Oct 28 02:19:12 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:12 google-dataproc-startup[843]: <13>Oct 28 02:19:12 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:12 google-dataproc-startup[843]: <13>Oct 28 02:19:12 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 12.'
<13>Oct 28 02:19:12 google-dataproc-startup[843]: <13>Oct 28 02:19:12 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 12.'
<13>Oct 28 02:19:12 google-dataproc-startup[843]: <13>Oct 28 02:19:12 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 12.
<13>Oct 28 02:19:12 google-dataproc-startup[843]: <13>Oct 28 02:19:12 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:13 google-dataproc-startup[843]: <13>Oct 28 02:19:13 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:13 google-dataproc-startup[843]: <13>Oct 28 02:19:13 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:13 google-dataproc-startup[843]: <13>Oct 28 02:19:13 setup-hive-metastore[1507]: nc: connect to cluster-89fa-m port 10000 (tcp) failed: Connection refused
<13>Oct 28 02:19:13 google-dataproc-startup[843]: <13>Oct 28 02:19:13 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 13.'
<13>Oct 28 02:19:13 google-dataproc-startup[843]: <13>Oct 28 02:19:13 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 13.'
<13>Oct 28 02:19:13 google-dataproc-startup[843]: <13>Oct 28 02:19:13 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 failed. Retry attempt: 13.
<13>Oct 28 02:19:13 google-dataproc-startup[843]: <13>Oct 28 02:19:13 setup-hive-metastore[1507]: + sleep 1
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + for i in $(seq 1 ${max_retry})
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + nc -v -z -w 0 cluster-89fa-m 10000
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: Connection to cluster-89fa-m 10000 port [tcp/webmin] succeeded!
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + update_succeeded=1
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + loginfo 'nc -v -z -w 0 cluster-89fa-m 10000 succeeded.'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + echo 'nc -v -z -w 0 cluster-89fa-m 10000 succeeded.'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: nc -v -z -w 0 cluster-89fa-m 10000 succeeded.
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + break
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + ((  1  ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + loginfo 'Service up on host=cluster-89fa-m port=10000.'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + echo 'Service up on host=cluster-89fa-m port=10000.'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: Service up on host=cluster-89fa-m port=10000.
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + [[ hive-metastore == \h\a\d\o\o\p\-\h\d\f\s\-\n\a\m\e\n\o\d\e ]]
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: ++ get_dataproc_property am.primary_only
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: ++ local property_name=am.primary_only
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: +++ get_java_property /etc/google-dataproc/dataproc.properties am.primary_only
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: +++ local property_file=/etc/google-dataproc/dataproc.properties
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: +++ local property_name=am.primary_only
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: ++++ grep '^am.primary_only=' /etc/google-dataproc/dataproc.properties
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: ++++ sed -r 's/\\([#!=:])/\1/g'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: ++++ tail -n 1
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: ++++ cut -d = -f 2-
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: +++ local property_value=false
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: +++ echo false
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: ++ local property_value=false
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: ++ echo false
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Oct 28 02:19:14 google-dataproc-startup[843]: <13>Oct 28 02:19:14 setup-hive-metastore[1507]: + [[ hive-metastore == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + pid=1506
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + cmd='setup_service hadoop-yarn-resourcemanager'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1506 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + echo 'Waiting on pid=1506 cmd=[setup_service hadoop-yarn-resourcemanager]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: Waiting on pid=1506 cmd=[setup_service hadoop-yarn-resourcemanager]
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + wait 1506
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + pid=1505
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + cmd='setup_service hadoop-hdfs-namenode'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1505 cmd=[setup_service hadoop-hdfs-namenode]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + echo 'Waiting on pid=1505 cmd=[setup_service hadoop-hdfs-namenode]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: Waiting on pid=1505 cmd=[setup_service hadoop-hdfs-namenode]
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + wait 1505
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + pid=1448
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + cmd='uninstall_component proxy-agent'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1448 cmd=[uninstall_component proxy-agent]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + echo 'Waiting on pid=1448 cmd=[uninstall_component proxy-agent]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: Waiting on pid=1448 cmd=[uninstall_component proxy-agent]
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + wait 1448
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + pid=1447
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + cmd='uninstall_component presto'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1447 cmd=[uninstall_component presto]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + echo 'Waiting on pid=1447 cmd=[uninstall_component presto]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: Waiting on pid=1447 cmd=[uninstall_component presto]
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + wait 1447
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + pid=1446
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + cmd='uninstall_component kerberos'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1446 cmd=[uninstall_component kerberos]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + echo 'Waiting on pid=1446 cmd=[uninstall_component kerberos]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: Waiting on pid=1446 cmd=[uninstall_component kerberos]
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + wait 1446
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + pid=1445
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + cmd='uninstall_component jupyter'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1445 cmd=[uninstall_component jupyter]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + echo 'Waiting on pid=1445 cmd=[uninstall_component jupyter]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: Waiting on pid=1445 cmd=[uninstall_component jupyter]
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + wait 1445
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + pid=1444
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + cmd='uninstall_component anaconda'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1444 cmd=[uninstall_component anaconda]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + echo 'Waiting on pid=1444 cmd=[uninstall_component anaconda]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: Waiting on pid=1444 cmd=[uninstall_component anaconda]
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + wait 1444
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + pid=1443
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + cmd='bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + loginfo 'Waiting on pid=1443 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + echo 'Waiting on pid=1443 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: Waiting on pid=1443 cmd=[bash -c DEBIAN_FRONTEND=noninteractive apt-get autoremove -y --purge        druid hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-zkfc hadoop-yarn-nodemanager hive-webhcat-server kafka-server knox solr-server zeppelin zookeeper-server krb5-kpropd krb5-kdc krb5-admin-server krb5-user krb5-config xinetd || true]
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + status=0
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + wait 1443
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( status != 0 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( ++i  ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + (( i < 16 ))
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + BACKGROUND_PROCESSES=()
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + BACKGROUND_COMMANDS=()
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + systemctl daemon-reload
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + loginfo 'All done'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: + echo 'All done'
<13>Oct 28 02:19:14 google-dataproc-startup[843]: All done
