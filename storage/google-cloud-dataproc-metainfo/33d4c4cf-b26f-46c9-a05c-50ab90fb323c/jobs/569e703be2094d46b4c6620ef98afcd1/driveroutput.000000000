19/10/28 03:11:30 INFO org.spark_project.jetty.util.log: Logging initialized @3521ms
19/10/28 03:11:31 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
19/10/28 03:11:31 INFO org.spark_project.jetty.server.Server: Started @3643ms
19/10/28 03:11:31 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@225bb478{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
19/10/28 03:11:31 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
19/10/28 03:11:32 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at cluster-89fa-m/10.128.0.32:8032
19/10/28 03:11:32 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at cluster-89fa-m/10.128.0.32:10200
19/10/28 03:11:34 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1572229097244_0005
19/10/28 03:11:42 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1
19/10/28 03:32:54 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 1 on cluster-89fa-w-0.us-central1-c.c.lustrous-drake-255300.internal: Container killed by YARN for exceeding memory limits. 3.0 GB of 3 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
19/10/28 03:32:54 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container killed by YARN for exceeding memory limits. 3.0 GB of 3 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
19/10/28 03:32:54 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 1.0 (TID 8, cluster-89fa-w-0.us-central1-c.c.lustrous-drake-255300.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 3.0 GB of 3 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
